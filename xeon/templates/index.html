{% extends "base.html" %} 
 {% block sitenav %}
<nav id="site-nav">
 <div class="category" state="0">
  <a href="http://{{host}}:{{port}}/index">
   Optimization Guide
  </a>
 </div>
 <div class="category" state="0">
  <a href="http://{{host}}:{{port}}/summary">
   Optimization Guide Summary
  </a>
 </div>
 <ul>
  <li>
   <div class="section-link" state="1">
    <a href="#introduction.section">
     1. Introduction
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.3">
     2.  Intel MIC architecture &amp; system overview
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#section-native">
     3.  Native compilation
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#section-offload">
     4. Intel compiler’s offload pragmas
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.6">
     5.  OpenMP and hybrid
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.7">
     6. MPI
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.8">
     7. Intel MKL (Math Kernel Library)
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.9">
     8. TBB: Intel Threading Building Blocks
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.10">
     9. IPP: The Intel Integrated Performance Primitives
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.11">
     10. Further programming models
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.12">
     11. Debugging
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.13">
     12. Tuning
    </a>
   </div>
  </li>
  <li>
   <div class="section-link" state="1">
    <a href="#id-1.14">
     13.  Performance analysis tools
    </a>
   </div>
  </li>
 </ul>
</nav>
 {% endblock %} 
{% block content %}
<div id="contents-container">
 <article id="contents">
  <div class="topic concept nested1" id="introduction.section" xml:lang="en-US">
   <a name="introduction.section" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#introduction.section" name="introduction.section" shape="rect">
     1. Introduction
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      The guide covers a wide range of topics from the
description of the hardware of the Intel Xeon Phi coprocessor through information about the
basic programming models
as well as information about porting programs
up to tools and strategies how to analyze and
improve the performance of applications.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.3" xml:lang="en-US">
   <a name="id-1.3" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.3" name="id-1.3" shape="rect">
     2.  Intel MIC architecture &amp; system overview
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      It has a load-to-use latency of 1 cycle, which means that an integer value loaded from the L1 cache can be used in the next clock by an integer instruction.
     </li>
     <li class="li">
      To start the Intel MPSS stack and initialize the Xeon Phi coprocessor the following command has to be executed as root or during host system start-up:

During start-up details are logged to  /var/log/messages.
     </li>
     <li class="li">
      If MPSS with OFED support is needed, further the following commands have to be executed as root:

Per default IP addresses 172.31.1.254 , 172.31.2.254 , 172.31.3.254 etc.
     </li>
     <li class="li">
      To get also PCIe related details the command has to be run with root privileges.
     </li>
     <li class="li">
      Here is an example output for a C0 stepping Intel Xeon Phi prototype:

Users can log in directly onto the Xeon Phi coprocessor via ssh.
     </li>
     <li class="li">
      Since the access to the coprocessor is ssh-key based users have to generate a private/public key pair via  ssh-keygen before accessing the coprocessor for the first time.
     </li>
     <li class="li">
      After the keys have been generated, the following commands have to be executed as root to populate the filesystem image for the coprocessor on the host (/opt/intel/mic/filesystem/mic0/home) with the new keys.
     </li>
     <li class="li">
      Since the coprocessor has to be restarted to copy the new image to the coprocessor, the following commands have to be used (preferrably only by the system administrator) with care.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="section-native" xml:lang="en-US">
   <a name="section-native" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-native" name="section-native" shape="rect">
     3.  Native compilation
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      In native mode an application is compiled on the host using the compiler switch  -mmic  to generate code for the MIC architecture.
     </li>
     <li class="li">
      The binary can then be copied to the coprocessor and has to be started there.
     </li>
     <li class="li">
      To achieve good performance one should mind the following items.
     </li>
     <li class="li">
      Data should be for the MIC architecture, in contrast to 32 Bytes (256 Bits) for AVX and 16 Bytes (128 Bits) for SSE.
     </li>
     <li class="li">
      Use pragmas like  #pragma ivdep, #pragma vector always, #pragma vector aligned, #pragma simd etc.
     </li>
     <li class="li">
      to achieve autovectorization.
     </li>
     <li class="li">
      One can use intrinsics to have full control over the vector registers and the instruction set.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="section-offload" xml:lang="en-US">
   <a name="section-offload" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-offload" name="section-offload" shape="rect">
     4. Intel compiler’s offload pragmas
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      One can simply add OpenMP-like pragmas to C/C++ or Fortran code to mark regions of code that should be offloaded to the Intel Xeon Phi Coprocessor and be run there.
     </li>
     <li class="li">
      Code to transfer the data to the coprocessor is automatically created by the compiler, however the programmer can influence the data transfer by adding data clauses to the offload pragma.
     </li>
     <li class="li">
      One could also specify the specific coprocessor  num  in a system with multiple coprocessors by using  #pragma offload target(mic:num).
     </li>
     <li class="li">
      Using in, out and  inout  one can specify which data has to be copied in which direction.
     </li>
     <li class="li">
      It is recommended that for Intel Xeon Phi data is 64-byte aligned.
     </li>
     <li class="li">
      Use  -no-offload  to disable the generation of offload code.
     </li>
     <li class="li">
      Using the compiler option  -vec-report2  one can see which loops have been vectorized on the host and the MIC coprocessor:

By setting the environment variable OFFLOAD_REPORT  one can obtain information about performance and data transfers at runtime:

If a function is called within the offloaded code block, this function has to be declared with  __attribute__((target(mic))) .
     </li>
     <li class="li">
      For example one could put the matrix-matrix multiplication of the previous example into a subroutine and call that routine within an offloaded block region:

Mind the C99 restrict keyword that specifies that the vectors do not overlap.
     </li>
     <li class="li">
      (Compile with -std=c99)









The following offload pragmas are available (from [11]):



The following clauses can be used to control data transfers:



The following (optional) modifiers are specified:




















To explicitly share work between the coprocessor and the host one can use OpenMP sections to manually distribute the work.
     </li>
     <li class="li">
      To increase the performance one should minimize data transfers as much as possible and keep the data on the coprocessor between computations using the same data.
     </li>
     <li class="li">
      Using intrinsics with manual data prefetching and register blocking can still considerably increase the performance.
     </li>
     <li class="li">
      Generally speaking, the programmer should try to get a suitable vectorization and write cache and register efficient code, i.e.
     </li>
     <li class="li">
      values stored in registers should be reused as often as possible in order to avoid cache and memory access.
     </li>
     <li class="li">
      The tuning techniques for native implementations discussed in Section 3 also apply for offloaded code, of course.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.6" xml:lang="en-US">
   <a name="id-1.6" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6" name="id-1.6" shape="rect">
     5.  OpenMP and hybrid
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      For offload schemes the maximal amount of threads that can be used on the device is 4 times the number of cores minus one, because one core is reserved for the OS and its services.
     </li>
     <li class="li">
      Unlike some CPU-intensive HPC applications that are run on Xeon architecture, which do not benefit from hyperthreading, applications run on Xeon Phi coprocessors do and using more than one thread per core is recommended.
     </li>
     <li class="li">
      The most important considerations for OpenMP threading and affinity are the total number of threads that should be utilized and the scheme for binding threads to processor cores.
     </li>
     <li class="li">
      Another useful option is the verbosity modifier:

With compiler version 13.1.0 and newer one can use the KMP_PLACE_THREADS variable to point out the topology of the system to the OpenMP runtime, for example:

meaning that 60 cores and 3 threads per core should be used.
     </li>
     <li class="li">
      Still one should use the KMP_AFFINITY variable to bind the threads to the cores.
     </li>
     <li class="li">
      Environment variables for controlling OpenMP behavior are to be set for both runtimes, for example the KMP_AFFINITY variable which can be used to assign a particular thread to a particular physical node.
     </li>
     <li class="li">
      One can also use special API calls to set the environment for the coprocessor only, e.g.
     </li>
     <li class="li">
      The schedule clause can be used to set the loop scheduling at compile time.
     </li>
     <li class="li">
      If the amount of work that should be done by each thread is non-trivial and consists of nested for-loops, one might use the  collapse()  directive to specify how many for-loops are associated with the OpenMP loop construct.
     </li>
     <li class="li">
      Another way to improve scalability is to reduce barrier synchronization overheads by using the nowait directive.
     </li>
     <li class="li">
      When assigning MPI ranks, one should take into account that there is a data transfer overhead over the PCIe, so minimizing the communication from and to the Xeon Phi is a good idea.
     </li>
     <li class="li">
      For hybrid OpenMP/MPI applications use the thread safe version of the Intel MPI Library by using the -mt_mpi compiler driver option.
     </li>
     <li class="li">
      It is recommended to use the following setting:

By setting this to omp, one sets the process pinning domain size to be to OMP_NUM_THREADS.
     </li>
     <li class="li">
      Further, to pin OpenMP threads within a particular domain, one could use the KMP_AFFINITY environment variable.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.7" xml:lang="en-US">
   <a name="id-1.7" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7" name="id-1.7" shape="rect">
     6. MPI
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      The following commands have to be executed to set up the MPI environment:

The following network fabrics are available for the Intel Xeon Phi coprocessor:



The Intel MPI library tries to automatically use the best available network fabric detected (usually shm for intra-node communication and InfiniBand (dapl, ofa) for inter-node communication).
     </li>
     <li class="li">
      Intel MPI for the Xeon Phi coprocessors offers various MPI programming models:









To build and run an application in coprocessor-only mode, the following commands have to be executed:










To build and run an application in symmetric mode, the following commands have to be executed:










To build and run an application in host-only mode, the following commands have to be executed:











Instead of specifying the hosts and coprocessors via  -n hostname one can also put the names into a hostfile and launch the jobs via

Mind that the executable must have the same name on the hosts and the coprocessors in this case.
     </li>
     <li class="li">
      If one sets

the .mix postfix is automatically added to the executable name by mpirun, so in the case of the example above  test  is launched on the host and  test.mic on the coprocessors.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.8" xml:lang="en-US">
   <a name="id-1.8" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8" name="id-1.8" shape="rect">
     7. Intel MKL (Math Kernel Library)
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      All functions can be used on the Xeon Phi, however the optimization level for wider 512-bit SIMD instructions differs.
     </li>
     <li class="li">
      The following 3 usage models of MKL are available for the Xeon Phi:


Automatic Offload
Compiler Assisted Offload
Native Execution










In the case of automatic offload the user does not have to change the code at all.
     </li>
     <li class="li">
      To enable automatic offload either the function

mkl_mic_enable()

has to be called within the source code or the environment variable

MKL_MIC_ENABLE=1

has to be set.
     </li>
     <li class="li">
      If no Xeon Phi coprocessor is detected the application runs on the host without penalty.
     </li>
     <li class="li">
      In case of the BLAS routines the user can specify the work division between the host and the coprocessor by calling the routine

or by setting the environment variable

Both examples specify to offload 50% of computation only to the 1st card (card #0).
     </li>
     <li class="li">
      It is also recommended to exploit data persistence with CAO.
     </li>
     <li class="li">
      To build a program for native mode, the following compiler settings should be used:

The binary must then be manually copied to the coprocessor via ssh and directly started on the coprocessor.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.9" xml:lang="en-US">
   <a name="id-1.9" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9" name="id-1.9" shape="rect">
     8. TBB: Intel Threading Building Blocks
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      You should be able to use it with any compiler supporting ISO C++.
     </li>
     <li class="li">
      Typically as a rule of thumb an application must scale well past one hundred threads on Intel Xeon processors to profit from the possible higher parallel performance offered with e.g.
     </li>
     <li class="li">
      To check if the scaling would profit from utilising the highly parallel capabilities of the MIC architecture, you should start to create a simple performance graph with a varying number of threads (from one up to the number of cores).
     </li>
     <li class="li">
      Unlike hyper-threading these hardware threads cannot be switched off and should never be ignored.
     </li>
     <li class="li">
      Generally it should be impossible for a single thread per core to approach the memory or floating point capability limit.
     </li>
     <li class="li">
      This is one of the reasons why the number of threads per core should be parameterized as well as the number of cores.
     </li>
     <li class="li">
      So shared memory programmers have Intel TBB and Intel Cilk Plus to assist them with built-in tasking models.
     </li>
     <li class="li">
      Intel TBB specifies the requirements on the types instead and in this way keeps the algorithms themselves generic and easily adaptable to different data representations.
     </li>
     <li class="li">
      It is easy to start: You don’t have to be a threading expert to leverage multi-core performance with the help of TBB.
     </li>
     <li class="li">
      It obeys to logical parallelism: Since with TBB you specify tasks instead of threads, you automatically produce more portable code which emphasizes scalable, data parallel programming.
     </li>
     <li class="li">
      In TBB every template solves a computationally intensive problem in a generic, simple way instead.
     </li>
     <li class="li">
      In the simplest form scalable parallelism can be achieved by parallelizing a loop of iterations that can each run independently from each other.
     </li>
     <li class="li">
      There might be situations though where controlling the chunk size more precisely might yield better performance.
     </li>
     <li class="li">
      To make them available on the coprocessor the header files have to be wrapped with #pragma offload directives as demonstrated in the example below:

Functions called from within the offloaded construct and global data required on the Intel Xeon Phi coprocessor should be appended by the special function attribute  __attribute__((target(mic))).
     </li>
     <li class="li">
      Codes using Intel TBB with an offload should be compiled with -tbb flag instead of -ltbb.
     </li>
     <li class="li">
      Amongst it, the templates cover well-known
parallel-programming patterns like parallel loops or reduction
operations, task-based constructs or pipelines.
     </li>
     <li class="li">
      However, some estimation can be made what degree
the needed parallelism should have in order to achieve efficient
execution on the coprocessor.
     </li>
     <li class="li">
      A rule of thumb is that one should
have about 10 computational tasks per thread in order to
compensate delays from load imbalances.
     </li>
     <li class="li">
      Considerations also have to be made if it would
really increase the performance to use all 4 hyperthreads of a
core.
     </li>
     <li class="li">
      Parallelisation with the task construct



This example shows how one can implement a multi-threaded,
task-oriented algorithm.
     </li>
     <li class="li">
      Loop parallelisation



TBB provides several algorithms that can be used for the
parallelisation of loops, again without forcing the
implementer do deal with the details of low-level threads.
     </li>
     <li class="li">
      The implemented algorithm shall calculate the sum of
the vector elements.
     </li>
     <li class="li">
      The serial implementation could be written as follows:

The parallel algorithm shall compute the partial sums of
partial vectors.
     </li>
     <li class="li">
      The TBB template parallel_reduce provides
the basic implementation of subdividing a range to iterate
over into several subranges.
     </li>
     <li class="li">
      The transformation
function working on the vector elements has to be provided as
operator() method.
     </li>
     <li class="li">
      The function that
reduces the results of the parallel work on different
subvectors has to be implemented as method
join(), which takes a reference to another
instance of the adder class as argument.
     </li>
     <li class="li">
      The reduction of two partial
sums to one value is implemented in join().
     </li>
     <li class="li">
      The summation of all array elements can be performed with the
following code piece:











Vectorization and cache-locality should be used in order to
increase the program efficiency.
     </li>
     <li class="li">
      Using work-stealing for task scheduling means that idle threads
will take over tasks from other busy threads.
     </li>
     <li class="li">
      This approach has
been mentioned already above when it was said that there should
be a certain amount of tasks per thread available in order to
compensate load imbalances.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.10" xml:lang="en-US">
   <a name="id-1.10" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10" name="id-1.10" shape="rect">
     9. IPP: The Intel Integrated Performance Primitives
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      Intel’s default solution
is a shellscript compilervars.sh in the bin directory of the
installation that should be executed.
     </li>
     <li class="li">
      The program, saved in the file ipptest.cpp can be compiled and
linked with the following command line:

The executable can be started with the following command:










IPP contains different implementations of each function that
provide the best performance on different processor
architectures.
     </li>
     <li class="li">
      They will be selected by means of dispatching
while the programmer uses always the same API.
     </li>
     <li class="li">
      The use of treads within the IPP should also be taken into
consideration by the application developer.
     </li>
     <li class="li">
      Some functions (for example FFT) are designed to use two
threads that should be mapped onto the same die in order to
use a shared L2 cache if available.
     </li>
     <li class="li">
      The user should set the
following environment variable when using processors with more
than two cores per die in order to ensure best performance:
KMP_AFFINITY=compact.
     </li>
     <li class="li">
      There is a risk of thread oversubscription and performance
degradation in the case that an application uses OpenMP
additionally.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.11" xml:lang="en-US">
   <a name="id-1.11" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.11" name="id-1.11" shape="rect">
     10. Further programming models
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.12" xml:lang="en-US">
   <a name="id-1.12" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.12" name="id-1.12" shape="rect">
     11. Debugging
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      The debugger is now part of the recent MPSS release and does not have to be downloaded separately any more.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.13" xml:lang="en-US">
   <a name="id-1.13" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13" name="id-1.13" shape="rect">
     12. Tuning
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      The following sections describe a few basic methodologies for
improving the performance of a code on a Xeon Phi.
     </li>
     <li class="li">
      As a
first step, we consider methods for improving the performance on a
single core.
     </li>
     <li class="li">
      Let the compiler know the loop should be
vectorized, but only affect compiler heuristics.
     </li>
     <li class="li">
      In order to extract maximum performance, consider binding
OpenMP threads to logical and physical cores across different
sockets on a single Xeon Phi card.
     </li>
     <li class="li">
      Using affinity none is
generally not recommended and is the slowest when all
available threads are being used.
     </li>
     <li class="li">
      Using
affinity balanced is generally a good
compromise, especially if one wishes to use less than the
number of threads available at maximum.
     </li>
     <li class="li">
      For a given
number of cores and threads, the user has control where
(relative to the first physical core, i.e., core 0) the OpenMP
threads are placed.
     </li>
     <li class="li">
      In conjunction with offloading from several
MPI processes to a single Xeon Phi card, such control can be
very useful to avoid oversubscription of cores.
     </li>
     <li class="li">
      The second one initializes the
parallel region once, but contains two implicit barriers
from OpenMP constructs.
     </li>
     <li class="li">
      If a variable is updated
often, it may be worthwhile to use a private variable in
stead of a shared one and use reduction at the end of the
work sharing loop.
     </li>
     <li class="li">
      Intel VTune Performance Analyzer
can be used to locate false sharing.
     </li>
     <li class="li">
      Implementation 2: Private array used to avoid false sharing.
     </li>
     <li class="li">
      We note that a better implementation for this
particular problem will be given in the next section.
     </li>
     <li class="li">
      When an application is run using all the available
threads, approximately 30Mb of memory is available per
thread assuming none of the data is shared.
     </li>
     <li class="li">
      Care
should be also taken when assigning private variables in
order to avoid unnecessary data duplication among
threads.
     </li>
     <li class="li">
      What is important to note is that
in doing so, each thread now implicitly allocates a vector
of length nthreads, i.e., the memory consumption is
quadratic in terms of the number of threads.
     </li>
     <li class="li">
      A better
alternative is to let each thread to store the local result
in a temporary variable and use a reduction to count the
number of elements smaller than zero.
     </li>
     <li class="li">
      Implementation 3: Temporary variable with reduction used
to store local results.
     </li>
     <li class="li">
      In total 57600 elements have to be stored in
Implementation 2 with 240 threads, whereas just 240
elements suffice in Implementation 3 for the same amount
of threads.
     </li>
     <li class="li">
      We note that if an array of results is to be
computed, one should prefer using an implementation where
the size of the work arrays does grow with the number of
threads used.
     </li>
     <li class="li">
      Due to the limited amount of memory available, sometimes
using all available threads on an Intel Xeon Phi to
parallelize the outer loop of some computation is not
possible.
     </li>
     <li class="li">
      Consider a case where several independent matrix-matrix
multiplications have to be computed.
     </li>
     <li class="li">
      Intuitively one would expect using 240 threads to be
the most efficient in this case.
     </li>
    </ul>
   </div>
  </div>
  <div class="topic concept nested1" id="id-1.14" xml:lang="en-US">
   <a name="id-1.14" shape="rect">
   </a>
   <h2 class="title topictitle1">
    <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.14" name="id-1.14" shape="rect">
     13.  Performance analysis tools
    </a>
   </h2>
   <div class="body conbody">
    <ul class="ul">
     <li class="li">
      On the host Scalasca can be normally compiled, while on the device one must perform a cross-compilation and add -mmic compiler option.
     </li>
    </ul>
   </div>
  </div>
 </article>
</div>
 {% endblock %} 
