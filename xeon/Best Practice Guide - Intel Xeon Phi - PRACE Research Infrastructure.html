<!DOCTYPE html>
<!-- saved from url=(0063)http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/ -->
<html lang="en-US" prefix="og: http://ogp.me/ns#" class="gr__prace-ri_eu"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        
        <title>Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure</title>
        
<!-- This site is optimized with the Yoast SEO plugin v4.2.1 - https://yoast.com/wordpress/plugins/seo/ -->
<link rel="canonical" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/">
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure">
<meta property="og:description" content="Best Practice Guide Intel Xeon Phi">
<meta property="og:url" content="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/">
<meta property="og:site_name" content="PRACE Research Infrastructure">
<meta property="article:section" content="Best Practice Guides">
<meta property="article:published_time" content="2014-02-18T10:45:07+02:00">
<meta property="article:modified_time" content="2016-05-09T10:14:24+02:00">
<meta property="og:updated_time" content="2016-05-09T10:14:24+02:00">
<meta property="og:image" content="http://www.prace-ri.eu/IMG/jpg/Xeon_Phi_PCIe_Card_M.jpg">
<meta property="og:image" content="http://www.prace-ri.eu/IMG/png/building-blocks.png">
<meta property="og:image" content="http://www.prace-ri.eu/IMG/png/results_affinity.png">
<meta property="og:image" content="http://www.prace-ri.eu/IMG/png/results_parallel.png">
<meta property="og:image" content="http://www.prace-ri.eu/IMG/png/results_falsesharing.png">
<meta property="og:image" content="http://www.prace-ri.eu/IMG/png/results_memsave.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:description" content="Best Practice Guide Intel Xeon Phi">
<meta name="twitter:title" content="Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure">
<meta name="twitter:image" content="http://www.prace-ri.eu/IMG/jpg/Xeon_Phi_PCIe_Card_M.jpg">
<!-- / Yoast SEO plugin. -->

<link rel="dns-prefetch" href="http://cdnjs.cloudflare.com/">
<link rel="dns-prefetch" href="http://w.sharethis.com/">
<link rel="dns-prefetch" href="http://ajax.googleapis.com/">
<link rel="dns-prefetch" href="http://s.w.org/">
<link rel="alternate" type="application/rss+xml" title="PRACE Research Infrastructure » Feed" href="http://www.prace-ri.eu/feed/">
<link rel="alternate" type="application/rss+xml" title="PRACE Research Infrastructure » Comments Feed" href="http://www.prace-ri.eu/comments/feed/">
<link rel="alternate" type="application/rss+xml" title="PRACE Research Infrastructure » Best Practice Guide – Intel Xeon Phi Comments Feed" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/feed/">
		<script id="facebook-jssdk" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/sdk.js"></script><script type="text/javascript" async="" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/ga.js"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.2.1\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.prace-ri.eu\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.7.5"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),!(j.toDataURL().length<3e3)&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,65039,8205,55356,57096),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57331,55356,57096),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55357,56425,55356,57341,8205,55357,56507),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55357,56425,55356,57341,55357,56507),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/wp-emoji-release.min.js" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="ai1ec_style-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/60c9db71_ai1ec_parsed_css.css" type="text/css" media="all">
<link rel="stylesheet" id="pracecall_css-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/pracecalls.css" type="text/css" media="all">
<link rel="stylesheet" id="search-pracecall_css-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/search-projects.css" type="text/css" media="all">
<link rel="stylesheet" id="cookie-notice-front-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/front.css" type="text/css" media="all">
<link rel="stylesheet" id="ecp1_fullcalendar_style_all-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/fullcalendar.css" type="text/css" media="all">
<link rel="stylesheet" id="ecp1_fullcalendar_style_print-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/fullcalendar.print.css" type="text/css" media="print">
<link rel="stylesheet" id="ecp1_client_style-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/ecp1-client.css" type="text/css" media="all">
<link rel="stylesheet" id="collapseomatic-css-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/light_style.css" type="text/css" media="all">
<link rel="stylesheet" id="jquery-ui-standard-css-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/jquery-ui.css" type="text/css" media="all">
<link rel="stylesheet" id="wpba_front_end_styles-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/wpba-frontend.css" type="text/css" media="all">
<link rel="stylesheet" id="raindrops_reset_fonts_grids-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/reset-fonts-grids.css" type="text/css" media="all">
<link rel="stylesheet" id="raindrops_grids-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/grids.css" type="text/css" media="all">
<link rel="stylesheet" id="raindrops_fonts-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/fonts.css" type="text/css" media="all">
<link rel="stylesheet" id="lang_style-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/en_US.css" type="text/css" media="all">
<link rel="stylesheet" id="raindrops_css3-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/css3.css" type="text/css" media="all">
<link rel="stylesheet" id="style-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/style.css" type="text/css" media="all">
<link rel="stylesheet" id="child-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/style(1).css" type="text/css" media="all">
<link rel="stylesheet" id="msl-main-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/masterslider.main.css" type="text/css" media="all">
<link rel="stylesheet" id="msl-custom-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/custom.css" type="text/css" media="all">
<link rel="stylesheet" id="fancybox-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/jquery.fancybox-1.3.8.min.css" type="text/css" media="screen">
<link rel="stylesheet" id="amr-ical-events-list-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/icallist.css" type="text/css" media="all">
<link rel="stylesheet" id="amr-ical-events-list_print-css" href="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/icalprint.css" type="text/css" media="print">
<!-- This site uses the Google Analytics by Yoast plugin v5.4.2 - Universal disabled - https://yoast.com/wordpress/plugins/google-analytics/ -->
<script type="text/javascript">

	var _gaq = _gaq || [];
	_gaq.push(['_setAccount', 'UA-39487143-1']);
	_gaq.push(['_gat._forceSSL']);
	_gaq.push(['_trackPageview']);

	(function () {
		var ga = document.createElement('script');
		ga.type = 'text/javascript';
		ga.async = true;
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		var s = document.getElementsByTagName('script')[0];
		s.parentNode.insertBefore(ga, s);
	})();

</script>
<!-- / Google Analytics by Yoast -->
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/jquery.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/jquery-migrate.min.js"></script>
<script type="text/javascript">
/* <![CDATA[ */
var cnArgs = {"ajaxurl":"http:\/\/www.prace-ri.eu\/wp-admin\/admin-ajax.php","hideEffect":"fade","onScroll":"","onScrollOffset":"100","cookieName":"cookie_notice_accepted","cookieValue":"TRUE","cookieTime":"2592000","cookiePath":"\/","cookieDomain":""};
/* ]]> */
</script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/front.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/fullcalendar.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/popup.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/gcal.js"></script>
<script id="st_insights_js" type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/st_insights.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/blocs.js"></script><style type="text/css">div.blocs_invisible{display:none;}</style>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/raindrops.js"></script>
<link rel="https://api.w.org/" href="http://www.prace-ri.eu/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.prace-ri.eu/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.prace-ri.eu/wp-includes/wlwmanifest.xml"> 
<meta name="generator" content="WordPress 4.7.5">
<link rel="shortlink" href="http://www.prace-ri.eu/?p=609">
<link rel="alternate" type="application/json+oembed" href="http://www.prace-ri.eu/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.prace-ri.eu%2Fbest-practice-guide-intel-xeon-phi-html%2F">
<link rel="alternate" type="text/xml+oembed" href="http://www.prace-ri.eu/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fwww.prace-ri.eu%2Fbest-practice-guide-intel-xeon-phi-html%2F&amp;format=xml">
<script type="text/javascript">
jQuery(document).ready(function($) {

$( "#menu-principal" ).accordion({
active: false,
autoHeight: false,
navigation: false,
collapsible: true,
heightStyle: "content" 
});

setSocialMediaPosition(10);

window.onresize = function() {
  setSocialMediaPosition(10);
};

function setSocialMediaPosition(padding) {
  header = $("#top");
  posx = ($(window).width() - header.width()) / 2;
  $("#floating-social-media").css("right", posx - padding - 32 + "px");
}
});

</script>
<style type="text/css">
.ui-widget {
font-family: inherit;
font-size: inherit;
}

.ui-accordion-header {
  width: 100% !important;
  box-sizing: border-box;
  margin: 0;
}
.ui-accordion-content {
  padding: 0 !important;
  margin: 0;
}
.ui-accordion .ui-accordion-content {
  overflow: visible;
}

.lsidebar ul, .lsidebar li, .lsidebar li a {
  box-sizing: border-box;
}
.lsidebar ul, .sub-menu {
  width: 150px !important;
}
.lsidebar li, .lsidebar li a {  
  width: 100% !important;

}
</style>
<script>var ms_grabbing_curosr = 'http://www.prace-ri.eu/wp-content/plugins/master-slider/public/assets/css/common/grabbing.cur', ms_grab_curosr = 'http://www.prace-ri.eu/wp-content/plugins/master-slider/public/assets/css/common/grab.cur';</script>
<meta name="generator" content="MasterSlider 2.9.8 - Responsive Touch Image Slider | www.avt.li/msf">
<script>(function(d, s, id){
                 var js, fjs = d.getElementsByTagName(s)[0];
                 if (d.getElementById(id)) {return;}
                 js = d.createElement(s); js.id = id;
                 js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.6";
                 fjs.parentNode.insertBefore(js, fjs);
               }(document, 'script', 'facebook-jssdk'));</script><style type="text/css">	.ssba {
									
									
									
									
								}
								.ssba img
								{
									width: 28px !important;
									padding: 6px;
									border:  0;
									box-shadow: none !important;
									display: inline !important;
									vertical-align: middle;
								}
								.ssba, .ssba a
								{
									text-decoration:none;
									border:0;
									background: none;
									
									font-size: 	12px;
									
									
								}
								</style><style type="text/css" id="raindrops-embed-css">
<!--/*<! [CDATA[*/
.gallery { margin: auto; overflow: hidden; width: 100%; }

            .gallery dl { margin: 0px; }

            .gallery .gallery-item { float: left; margin-top: 10px; text-align: center; }

            .gallery img { border: 2px solid #cfcfcf;max-width:100%; }

            .gallery .gallery-caption { margin-left: 0; }

            .gallery br { clear: both }

            .gallery-columns-1 dl{ width: 100% }

            .gallery-columns-2 dl{ width: 50% }

            .gallery-columns-3 dl{ width: 33.3% }

            .gallery-columns-4 dl{ width: 25% }

            .gallery-columns-5 dl{ width: 20% }

            .gallery-columns-6 dl{ width: 16.6% }

            .gallery-columns-7 dl{ width: 14.28% }

            .gallery-columns-8 dl{ width: 12.5% }

            .gallery-columns-9 dl{ width: 11.1% }

            .gallery-columns-10 dl{ width: 9.9% }
#header-image{background-image:url( http://www.prace-ri.eu/IMG/png/PRACE_new_header.png );width:950px;height:142px;background-repeat:no-repeat;background-position:center;background-color:#000;background-size:auto;  background-origin:content-box;}#header-image p { display:none;}/* raindrops is fixed start*/                #doc2{margin:auto;text-align:left;width:73.1em;max-width:950px;}#container{width:58em;}/* raindrops is fixed end */#hd{}#ft{}/*hide*/ .rsidebar{display:none;} .raindrops-accessible-mode .raindrops-comment-link:focus em,.enable-keyboard .raindrops-comment-link:focus em,.ie11.enable-keyboard #access .sub-menu a:focus,.ie11.enable-keyboard #access .children a:focus,.enable-keyboard .hfeed a:focus,.ie11.raindrops-accessible-mode #access .sub-menu a:focus,.ie11.raindrops-accessible-mode #access .children a:focus,.raindrops-accessible-mode .hfeed a:focus{      font-weight:bold!important;      border-bottom:1px solid #000;}body{    border-top:6px solid #444444;}a{    color:#444444;}a:hover{    color:#777;}#yui-main{    color:#444444;}#ft a{     color:#000;           }.footer-widget h2,.rsidebar h2,.lsidebar h2,.widgettitle h2,h2.footer-content {    text-indent:0;}/*comment bubble*/.raindrops-comment-link em {    color:#222222;background-color:#d0d0d0;    -moz-border-radius: 0.25em;    -webkit-border-radius: 0.25em;    border-radius: 0.25em;    position: relative;}.raindrops-comment-link .point {    border-left: 0.45em solid rgba(68, 68, 68,0.5);    border-bottom: 0.45em solid #FFF; /* IE fix */    border-bottom: 0.45em solid rgba(0,0,0,0);    overflow: hidden; /* IE fix */}a.raindrops-comment-link:hover em {    color:#ffffff;background-color:#3a3a3a;}a.raindrops-comment-link:hover .point {    border-left:1px solid rgba(68, 68, 68,0.5);}input[type="text"],textarea#comment{    border:1px solid #ddd;    border-top-color:rgba(68, 68, 68,0.5);    border-left-color:rgba(68, 68, 68,0.5);    padding:3px;    -moz-border-radius: 3px;    -khtml-border-radius: 3px;    -webkit-border-radius: 3px;}#access .children,#access .children li{    border-top:none;}#access .children li:nth-child(1){border-top:1px solid #ccc;}#access .sub-menu a,#access .children a,#access .children .current_page_item a{    text-align:left;    padding:10px;    background:#fff;    border-left-color:rgba(68, 68, 68,0.5);}#access .sub-menu a:hover,#access .children a:hover,#access .children .current_page_item a:hover{color:#222222;background-color:#d0d0d0;}blockquote{    border-left:6px solid;    border-left-color:rgba(68, 68, 68,0.5);    padding:10px;    color:#222222;background-color:#d0d0d0;}.color-1 a,.color-1{  background:#3a3a3a;  color:#ffffff;}.color-2 a,.color-2 {  background:#333333;  color:#ffffff;}.color-3 a,.color-3 {  background:#222222;  color:#d0d0d0;}.color-4 a,.color-4 {  /** Use the base color, two shades darker */  background:#111111;  /** Use the corresponding foreground color */  color:#d0d0d0;}.color-5 a,.color-5 {  background:#070707;  color:#d0d0d0;}.color1 a,.color1{  background:#606060;  color:#ffffff;}.color2 a,.color2 {  background:#737373;  color:#ffffff;}.color3 a,.color3 {  background:#a2a2a2;  color:#000000;}.color4 a,.color4 {  /** Use the base color, two shades darker */  background:#d0d0d0;  /** Use the corresponding foreground color */  color:#222222;}.color5 a,.color5 {  background:#ededed;  color:#444444;}.face-1{  color:#ffffff;}.face-2 {  color:#ffffff;}.face-3 {  color:#d0d0d0;}.face-4 {  color:#d0d0d0;}.face-5 {  color:#d0d0d0;}.face1{  color:#ffffff;}.face2 {  color:#ffffff;}.face3 {  color:#000000;}.face4 {  color:#222222;}.face5 {  color:#444444;}body{font-size:13px;}body{background-repeat: repeat;}body{background-position:top left;}body{background-attachment: fixed;}
/*]]>*/-->
</style>
<!-- Easy FancyBox 1.5.8.2 using FancyBox 1.3.8 - RavanH (http://status301.net/wordpress-plugins/easy-fancybox/) -->
<script type="text/javascript">
/* <![CDATA[ */
var fb_timeout = null;
var fb_opts = { 'overlayShow' : true, 'hideOnOverlayClick' : true, 'showCloseButton' : true, 'margin' : 20, 'centerOnScroll' : true, 'enableEscapeButton' : true, 'autoScale' : true };
var easy_fancybox_handler = function(){
	/* IMG */
	var fb_IMG_select = 'a[href*=".jpg"]:not(.nolightbox,li.nolightbox>a), area[href*=".jpg"]:not(.nolightbox), a[href*=".jpeg"]:not(.nolightbox,li.nolightbox>a), area[href*=".jpeg"]:not(.nolightbox), a[href*=".png"]:not(.nolightbox,li.nolightbox>a), area[href*=".png"]:not(.nolightbox)';
	jQuery(fb_IMG_select).addClass('fancybox image');
	var fb_IMG_sections = jQuery('div.gallery');
	fb_IMG_sections.each(function() { jQuery(this).find(fb_IMG_select).attr('rel', 'gallery-' + fb_IMG_sections.index(this)); });
	jQuery('a.fancybox, area.fancybox, li.fancybox a').fancybox( jQuery.extend({}, fb_opts, { 'transitionIn' : 'elastic', 'easingIn' : 'easeOutBack', 'transitionOut' : 'elastic', 'easingOut' : 'easeInBack', 'opacity' : false, 'hideOnContentClick' : false, 'titleShow' : true, 'titlePosition' : 'over', 'titleFromAlt' : true, 'showNavArrows' : true, 'enableKeyboardNav' : true, 'cyclic' : false }) );
}
var easy_fancybox_auto = function(){
	/* Auto-click */
	setTimeout(function(){jQuery('#fancybox-auto').trigger('click')},1000);
}
/* ]]> */
</script>
<style type="text/css">
#fancybox-content{border-color:#fff}#fancybox-outer{background-color:#fff}
</style>
        <style type="text/css">
        body {
        border-top: none!important;
        }
        </style>
    <style type="text/css">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}.fb_link img{border:none}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_reset .fb_dialog_legacy{overflow:visible}.fb_dialog_advanced{padding:10px;-moz-border-radius:8px;-webkit-border-radius:8px;border-radius:8px}.fb_dialog_content{background:#fff;color:#333}.fb_dialog_close_icon{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{top:5px;left:5px;right:auto}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_loader{background-color:#f6f7f9;border:1px solid #606060;font-size:24px;padding:20px}.fb_dialog_top_left,.fb_dialog_top_right,.fb_dialog_bottom_left,.fb_dialog_bottom_right{height:10px;width:10px;overflow:hidden;position:absolute}.fb_dialog_top_left{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 0;left:-10px;top:-10px}.fb_dialog_top_right{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 -10px;right:-10px;top:-10px}.fb_dialog_bottom_left{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 -20px;bottom:-10px;left:-10px}.fb_dialog_bottom_right{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 -30px;right:-10px;bottom:-10px}.fb_dialog_vert_left,.fb_dialog_vert_right,.fb_dialog_horiz_top,.fb_dialog_horiz_bottom{position:absolute;background:#525252;filter:alpha(opacity=70);opacity:.7}.fb_dialog_vert_left,.fb_dialog_vert_right{width:10px;height:100%}.fb_dialog_vert_left{margin-left:-10px}.fb_dialog_vert_right{right:0;margin-right:-10px}.fb_dialog_horiz_top,.fb_dialog_horiz_bottom{width:100%;height:10px}.fb_dialog_horiz_top{margin-top:-10px}.fb_dialog_horiz_bottom{bottom:0;margin-bottom:-10px}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{-webkit-transform:none;height:100%;margin:0;overflow:visible;position:absolute;top:-10000px;left:0;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{width:auto;height:auto;min-height:initial;min-width:initial;background:none}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{color:#fff;display:block;padding-top:20px;clear:both;font-size:18px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .45);position:absolute;bottom:0;left:0;right:0;top:0;width:100%;min-height:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_content .dialog_header{-webkit-box-shadow:white 0 1px 1px -1px inset;background:-webkit-gradient(linear, 0% 0%, 0% 100%, from(#738ABA), to(#2C4987));border-bottom:1px solid;border-color:#1d4088;color:#fff;font:14px Helvetica, sans-serif;font-weight:bold;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{-webkit-font-smoothing:subpixel-antialiased;height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:-webkit-gradient(linear, 0% 0%, 0% 100%, from(#4966A6), color-stop(.5, #355492), to(#2A4887));border:1px solid #29487d;-webkit-background-clip:padding-box;-webkit-border-radius:3px;-webkit-box-shadow:rgba(0, 0, 0, .117188) 0 1px 1px inset, rgba(255, 255, 255, .167969) 0 1px 0;display:inline-block;margin-top:3px;max-width:85px;line-height:18px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{border:none;background:none;color:#fff;font:12px Helvetica, sans-serif;font-weight:bold;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #555;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f6f7f9;border:1px solid #555;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_button{text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(http://static.xx.fbcdn.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-repeat:no-repeat;background-position:50% 50%;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_hide_iframes iframe{position:relative;left:-10000px}.fb_iframe_widget_loader{position:relative;display:inline-block}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}.fb_iframe_widget_loader iframe{min-height:32px;z-index:2;zoom:1}.fb_iframe_widget_loader .FB_Loader{background:url(http://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat;height:32px;width:32px;margin-left:-16px;position:absolute;left:50%;z-index:4}</style><style>.ita-icon-0{background-position:-14px -17px}.ita-icon-1{background-position:-64px -17px}.ita-icon-2{background-position:-114px -17px}.ita-icon-3{background-position:-164px -17px}.ita-icon-4{background-position:-214px -17px}.ita-icon-5{background-position:-264px -17px}.ita-icon-6{background-position:-314px -17px}.ita-icon-7{background-position:-364px -17px}.ita-icon-8{background-position:-414px -17px}.ita-icon-9{background-position:-464px -17px}.ita-icon-10{background-position:-14px -67px}.ita-icon-11{background-position:-64px -67px}.ita-icon-12{background-position:-114px -67px}.ita-icon-13{background-position:-164px -67px}.ita-icon-14{background-position:-214px -67px}.ita-icon-15{background-position:-264px -67px}.ita-icon-16{background-position:-314px -67px}.ita-icon-17{background-position:-364px -67px}.ita-icon-18{background-position:-414px -67px}.ita-icon-19{background-position:-464px -67px}.ita-icon-20{background-position:-14px -117px}.ita-icon-21{background-position:-64px -117px}.ita-icon-22{background-position:-114px -117px}.ita-icon-23{background-position:-164px -117px}.ita-icon-24{background-position:-214px -117px}.ita-icon-25{background-position:-264px -117px}.ita-icon-26{background-position:-314px -117px}.ita-icon-27{background-position:-364px -117px}.ita-icon-28{background-position:-414px -117px}.ita-icon-29{background-position:-464px -117px}.ita-icon-30{background-position:-14px -167px}.ita-icon-31{background-position:-64px -167px}.ita-icon-32{background-position:-114px -167px}.ita-icon-33{background-position:-164px -167px}.ita-icon-34{background-position:-214px -167px}.ita-icon-35{background-position:-264px -167px}.ita-icon-36{background-position:-314px -167px}.ita-icon-37{background-position:-364px -167px}.ita-icon-38{background-position:-414px -167px}.ita-icon-39{background-position:-464px -167px}.ita-icon-40{background-position:-14px -217px}.ita-icon-41{background-position:-64px -217px}.ita-icon-42{background-position:-114px -217px}.ita-icon-43{background-position:-164px -217px}.ita-icon-44{background-position:-214px -217px}.ita-icon-45{background-position:-264px -217px}.ita-icon-46{background-position:-314px -217px}</style></head>
    <body class="post-template-default single single-post postid-609 single-format-standard _masterslider _ms_version_2.9.8 en_US enable-keyboard rd-type-minimal cookies-not-accepted accept-lang-en chrome" data-gr-c-s-loaded="true">
    <div id="floating-social-media" style="right: 354px;">
        <div id="buffercode_banner_upload_info-28" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">Contact</h2><a href="http://www.prace-ri.eu/contact-prace" alt="/contact-prace" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/contact_icon.png" width="32px" height="32px"></a> </div><div id="buffercode_banner_upload_info-22" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">LinkedIn</h2><a href="http://www.linkedin.com/company/prace" alt="http://www.linkedin.com/company/prace" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/linkedin_in_icon_35px.jpg" width="32px" height="32px"></a> </div><div id="buffercode_banner_upload_info-23" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">Twitter</h2><a href="https://twitter.com/PRACE_RI" alt="https://twitter.com/PRACE_RI" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/twitter-bird-white-on-blue-1.png" width="32px" height="32px"></a> </div><div id="buffercode_banner_upload_info-24" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">ScienceNode</h2><a href="https://sciencenode.org/" alt="https://sciencenode.org/" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/sciencenode_logo_icon.png" width="32px" height="32px"></a> </div><div id="buffercode_banner_upload_info-25" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">Alpha Galileo</h2><a href="http://www.alphagalileo.org/" alt="http://www.alphagalileo.org/" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/alpha_galileo_logo_icon.png" width="32px" height="32px"></a> </div><div id="buffercode_banner_upload_info-26" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">Google+</h2><a href="https://plus.google.com/106259565187155540236/" alt="https://plus.google.com/106259565187155540236/" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/gicon_red1.png" width="32px" height="32px"></a> </div><div id="buffercode_banner_upload_info-27" class="widget widget_buffercode_banner_upload_info social-media-floating-buttons"><h2 class="widgettitle no-title">YouTube</h2><a href="https://www.youtube.com/user/PRACERI" alt="https://www.youtube.com/user/PRACERI" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/youtube-256.png" width="32px" height="32px"></a> </div>            <form method="get" name="searchform" id="searchform" action="http://www.prace-ri.eu/" role="search">
        <div class="searchform">
            <label class="screen-reader-text" for="s">Search for:</label>
            <input id="searchinput" type="text" value="" pattern="^[^(&lt;|&gt;)]+$" title="must not contain html tags" placeholder="Search" name="s">
            <input type="submit" id="searchsubmit" value=" &gt;&gt; ">
        </div>
    </form>
        </div>
        <div id="doc2" class="yui-t2 hfeed">
                        <header id="top">
                <div id="hd" role="banner">
                    <div class="skip-link screen-reader-text">
                        <a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#container">Skip to content</a>
                    </div>
                                    </div>

                                <a href="http://www.prace-ri.eu/"><div id="header-image"><p style="display:none;">THE TOP LEVEL OF THE EUROPEAN HPC ECOSYSTEM</p></div></a>            </header>
            <div id="bd" class="clearfix">
                <div id="yui-main">
    <div class="yui-b ">
        <div class="yui-ge" id="container">
            <div class="yui-u first raindrops-expand-width" role="main">
                                        <article id="post-609" class="clearfix post-609 post type-post status-publish format-standard hentry category-best-practice-guides">	
                        <p id="breadcrumbs"><span xmlns:v="http://rdf.data-vocabulary.org/#"><span typeof="v:Breadcrumb"><a href="http://www.prace-ri.eu/" rel="v:url" property="v:title">Home page</a> » <span rel="v:child" typeof="v:Breadcrumb"><a href="http://www.prace-ri.eu/training-and-documentation/" rel="v:url" property="v:title">Training and Documentation</a> » <span rel="v:child" typeof="v:Breadcrumb"><a href="http://www.prace-ri.eu/best-practice-guides/" rel="v:url" property="v:title">Best Practice Guides</a> » <strong class="breadcrumb_last">Best Practice Guide – Intel Xeon Phi</strong></span></span></span></span></p> 
<div class="ligne-debut"></div>
<h2 class="h2 entry-title"><span> Best Practice Guide – Intel Xeon Phi</span></h2>        <div class="entry-meta-default">
            <span class="meta-prep meta-prep-author"> </span><time class="entry-date updated" datetime="2014-02-18T10:45:07+00:00">Tuesday 18 February 2014</time><span class="meta-sep">        </span></div>

        <div class="entry-content clearfix">
            <div class="article" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h1 class="title"><a id="id-1"></a>Best Practice Guide Intel Xeon Phi v1.1</h1>
</div>
<div>
<div class="author">
<h3 class="author"><span class="firstname">Michaela</span> <span class="surname">Barth</span></h3>
<div class="affiliation"><span class="orgname">KTH Sweden<br>
</span><p></p>
</div>
</div>
</div>
<div>
<div class="author">
<h3 class="author"><span class="firstname">Mikko</span> <span class="surname">Byckling</span></h3>
<div class="affiliation"><span class="orgname">CSC Finland<br>
</span><p></p>
</div>
</div>
</div>
<div>
<div class="author">
<h3 class="author"><span class="firstname">Nevena</span> <span class="surname">Ilieva</span></h3>
<div class="affiliation"><span class="orgname">NCSA Bulgaria<br>
</span><p></p>
</div>
</div>
</div>
<div>
<div class="author">
<h3 class="author"><span class="firstname">Sami</span> <span class="surname">Saarinen</span></h3>
<div class="affiliation"><span class="orgname">CSC Finland<br>
</span><p></p>
</div>
</div>
</div>
<div>
<div class="author">
<h3 class="author"><span class="firstname">Michael</span> <span class="surname">Schliephake</span></h3>
<div class="affiliation"><span class="orgname">KTH Sweden<br>
</span><p></p>
</div>
</div>
</div>
<div>
<div class="author">
<h3 class="author"><span class="firstname">Volker</span> <span class="surname">Weinberg (Editor)</span></h3>
<div class="affiliation"><span class="orgname">LRZ Germany<br>
</span><p></p>
</div>
<p><code class="email">&lt;<a class="email" href="mailto:weinberg@lrz.de">weinberg@lrz.de</a>&gt;</code></p>
</div>
</div>
<div>
<p class="pubdate">14-02-2014
</p></div>
</div>
<hr>
</div>
<div class="toc">
<p><strong>Table of Contents</strong></p>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#introduction.section">1. Introduction</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.3">2. Intel MIC architecture &amp; system overview </a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.3.2">2.1. The Intel MIC architecture</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.3.2.2">2.1.1. Intel Xeon Phi coprocessor architecture overview</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.3.2.3">2.1.2. The cache hierarchy </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.3.3">2.2. Network configuration &amp; system access</a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-native">3. Native compilation </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-offload">4. Intel compiler’s offload pragmas</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#offload-simple-example">4.1. Simple example </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.5.4">4.2. Obtaining informations about the offloading </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.5.5">4.3. Syntax of pragmas </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.5.6">4.4. Recommendations </a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-explicit-worksharing">4.4.1. Explicit worksharing</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.5.6.3">4.4.2. Persistent data on the coprocessor </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#optimized-offload-code">4.4.3. Optimizing offloaded code </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.5.7">4.5. Intel Cilk Plus parallel extensions </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6">5. OpenMP and hybrid</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-openmp">5.1. OpenMP </a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.2.2">5.1.1. OpenMP basics</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.2.3">5.1.2. Threading and affinity</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.2.4">5.1.3. Loop scheduling</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.2.5">5.1.4. Scalability improvement </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.3">5.2. Hybrid OpenMP/MPI</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.3.2">5.2.1. Programming models</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.6.3.3">5.2.2. Threading of the MPI ranks</a></span></dt>
</dl>
</dd>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7">6. MPI</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7.3">6.1. Setting up the MPI environment </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7.4">6.2. MPI programming models </a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7.4.4">6.2.1. Coprocessor-only model </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7.4.5">6.2.2. Symmetric model </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7.4.6">6.2.3. Host-only model </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.7.5">6.3. Simplifying launching of MPI jobs </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8">7. Intel MKL (Math Kernel Library)</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8.4">7.1. MKL usage modes</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8.4.4">7.1.1. Automatic Offload (AO) </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8.4.5">7.1.2. Compiler Assisted Offload (CAO)</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8.4.6">7.1.3. Native Execution </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8.5">7.2. Example code </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.8.6">7.3. Intel Math Kernel Library Link Line Advisor </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9">8. TBB: Intel Threading Building Blocks</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.10">8.1. Advantages of TBB </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.11">8.2. Using TBB natively </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.12">8.3. Offloading TBB</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.13">8.4. Examples</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.13.3">8.4.1. Exposing parallelism to the processor</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.13.4">8.4.2. Vectorization and Cache-Locality</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.9.13.5">8.4.3. Work-stealing versus Work-sharing</a></span></dt>
</dl>
</dd>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10">9. IPP: The Intel Integrated Performance Primitives</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10.3">9.1. Overview of IPP</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10.4">9.2. Using IPP</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10.4.2">9.2.1. Getting Started</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10.4.3">9.2.2. Linking of Applications</a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10.5">9.3. Multithreading</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.10.6">9.4. Links and References</a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.11">10. Further programming models</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.11.3">10.1. OpenCL</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.11.4">10.2. OpenACC</a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.12">11. Debugging</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.12.5">11.1. Native debugging with gdb </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.12.6">11.2. Remote debugging with gdb </a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13">12. Tuning</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.5">12.1. Single core optimization</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.5.3">12.1.1. Memory alignment</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.5.4">12.1.2. SIMD optimization</a></span></dt>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-advanced-openmp">12.2. OpenMP optimization</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-tuning-openmp-affinity">12.2.1. OpenMP thread affinity</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.5">12.2.2. Example: Thread affinity</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.6">12.2.3. OpenMP thread placement</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.7">12.2.4. Multiple parallel regions and barriers</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.8">12.2.5. Example: Multiple parallel regions and barriers</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.9">12.2.6. False sharing</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.10">12.2.7. Example: False sharing</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.11">12.2.8. Memory limitations</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.12">12.2.9. Example: Memory limitations</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.13">12.2.10. Nested parallelism</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.13.6.14">12.2.11. Example: Nested parallelism</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-tuning-openmp-loadbalancing">12.2.12. OpenMP load balancing</a></span></dt>
</dl>
</dd>
</dl>
</dd>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.14">13. Performance analysis tools </a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.14.2">13.1. Intel performance analysis tools</a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.14.3">13.2. Scalasca</a></span></dt>
<dd>
<dl>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.14.3.3">13.2.1. Compilation of Scalasca </a></span></dt>
<dt><span class="section"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.14.3.4">13.2.2. Usage</a></span></dt>
</dl>
</dd>
</dl>
</dd>
<dt><span class="bibliography"><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#id-1.15">Further documentation</a></span></dt>
</dl>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="introduction.section"></a>1.&nbsp;Introduction</h2>
</div>
</div>
</div>
<div class="figure"><a id="id-1.2.2.1"></a><p></p>
<p class="title"><strong>Figure&nbsp;1.&nbsp;Intel Xeon Phi coprocessor </strong></p>
<div class="figure-contents">
<div class="mediaobject" align="center">
<table style="cellpadding: 0; cellspacing: 0;" border="0" summary="manufactured viewport for HTML img" width="514">
<tbody>
<tr>
<td align="center"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/Xeon_Phi_PCIe_Card_M.jpg" alt="Intel Xeon Phi coprocessor" width="514" align="middle"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>&nbsp;</p>
<p>This best practice guide provides information about Intel’s MIC architecture and programming models for the Intel Xeon Phi coprocessor in<br>
order to enable programmers to achieve good performance of<br>
their applications. The guide covers a wide range of topics from the<br>
description of the hardware of the Intel Xeon Phi coprocessor through information about the<br>
basic programming models<br>
as well as information about porting programs<br>
up to tools and strategies how to analyze and<br>
improve the performance of applications.</p>
<p>&nbsp;</p>
<p>This initial version of the guide contains contributions from CSC, KTH, LRZ, and NCSA. It also includes several informations from publicly available Intel documents and Intel webinars <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref7">[11]</a>.</p>
<p>In 2013 the first books about programming the Intel Xeon Phi coprocessor <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refbook1">[1]</a> <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refbook2">[2]</a> <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refbook3">[3]</a> have been published. We also recommend a book about structured parallel programming <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref2">[4]</a>.<br>
Useful online documentation about the Intel Xeon Phi coprocessor can be found in Intel’s developer zone for Xeon Phi Programming <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref3">[6]</a> and the Intel Many Integrated Core Architecture User Forum <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref4">[7]</a>. To get things going quickly have a look on the Intel Xeon Phi Coprocessor Developer’s Quick Start Guide <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref8">[15]</a> and also on the paper <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref13">[24]</a>.</p>
<p>Various experiences with application enabling for Intel Xeon Phi gained within PRACE on the<br>
EURORA-cluster at CINECA (Italy) in late 2013 can be found in whitepapers available<br>
online at <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refeurora">[16]</a>.</p>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.3"></a>2.&nbsp; Intel MIC architecture &amp; system overview</h2>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.3.2"></a>2.1.&nbsp; The Intel MIC architecture</h3>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.3.2.2"></a>2.1.1.&nbsp; Intel Xeon Phi coprocessor architecture overview</h4>
</div>
</div>
</div>
<p>The Intel Xeon Phi coprocessor consists of up to 61 cores connected by a<br>
high performance on-die bidirectional interconnect. The coprocessor runs a<br>
Linux operating system and supports all important Intel development tools,<br>
like C/C++ and Fortran compiler, MPI and OpenMP, high performance libraries<br>
like MKL, debugger and tracing tools like Intel VTune Amplifier XE.<br>
Traditional UNIX tools on the coprocessor are supported via BusyBox, which combines tiny versions of many common UNIX utilities into a single small executable.</p>
<p>The coprocessor is connected to an Intel Xeon processor – the “host” – via the PCI Express (PICe) bus. The implementation of a virtualized TCP/IP stack allows to access the coprocessor like a network node.</p>
<p>Summarized information about the hardware architecture can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref8a">[17]</a>.</p>
<p>In the following we cite the most important properties of the MIC architecture from the System Software Developers Guide <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref9">[18]</a>, which includes many details about the MIC architecture:</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"> <span class="bold"><strong> Core</strong></span></span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">the processor core (scalar unit) is an in-order architecture (based on the Intel Pentium processor family)</li>
<li class="listitem">fetches and decodes instructions from <span class="emphasis"><em> four hardware threads </em></span></li>
<li class="listitem">supports a 64-bit execution environment, along with Intel Initial Many Core Instructions</li>
<li class="listitem">does not support any previous Intel SIMD extensions like MME, SSE, SSE2, SSE3, SSE4.1 SSE4.2 or AVX instructions</li>
<li class="listitem">new vector instructions provided by the Intel Xeon Phi coprocessor instruction set utilize a dedicated 512-bit wide vector floating-point unit (VPU) that is provided for each core</li>
<li class="listitem">high performance support for reciprocal, square root, power and exponent operations, scatter/gather and streaming store capabilities to achieve higher effective memory bandwidth</li>
<li class="listitem">can execute 2 instructs per cycle, one on the U-pipe and one on the V-pipe (not all instruction types can be executed by the V-pipe, e.g. vector instructions can only be executed on the U-pipe)</li>
<li class="listitem">contains the L1 Icache and Dcache</li>
<li class="listitem">each core is connected to a ring interconnect via the Core Ring Interface (CRI)</li>
</ul>
</div>
</dd>
<dt><span class="term"> <span class="bold"><strong> Vector Processing Unit (VPU)</strong></span> </span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">the VPU includes the EMU (Extended Math Unit) and executes 16 single-precision floating point, 16 32bit integer operations or 8 double-precision floating point operations per cycle. Each operation can be a fused multiply-add, giving 32 single-precision or 16 double-precision floating-point operations per cycle.</li>
<li class="listitem">contains the vector register file: 32 512-bit wide registers per thread context, each register can hold 16 singles or 8 doubles</li>
<li class="listitem">most vector instructions have a 4-clock latency with a 1 clock throughput</li>
</ul>
</div>
</dd>
<dt><span class="term"> <span class="bold"><strong> Core Ring Interface (CRI)</strong></span></span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">hosts the L2 cache and the tag directory (TD)</li>
<li class="listitem">connects each core to an Intel Xeon Phi coprocessor Ring Stop (RS), which connects to the interprocessor core network</li>
</ul>
</div>
</dd>
<dt><span class="term"> <span class="bold"><strong> Ring</strong></span></span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">includes component interfaces, ring stops, ring turns, addressing and flow control</li>
<li class="listitem">a Xeon Phi coprocessor has 2 of theses rings, one travelling each direction</li>
</ul>
</div>
</dd>
<dt><span class="term"> <span class="bold"><strong> SBOX </strong></span></span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Gen2 PCI Express client logic</li>
<li class="listitem">system interface to the host CPU or PCI Express switch</li>
<li class="listitem">DMA engine</li>
</ul>
</div>
</dd>
<dt><span class="term"> <span class="bold"><strong> GBOX</strong></span></span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">coprocessor memory controller</li>
<li class="listitem">consists of the FBOX (interface to the ring interconnect), the MBOX (request scheduler) and the PBOX (physical layer that interfaces with the GDDR devices</li>
<li class="listitem">There are 8 memory controllers supporting up to 16 GDDR5 channels. With a transfer speed of up to 5.5 GT/s a theoretical aggregated bandwidth of 352 GB/s is provided.</li>
</ul>
</div>
</dd>
<dt><span class="term"> <span class="bold"><strong> Performance Monitoring Unit (PMU)</strong></span></span></dt>
<dd>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">allows data to be collected from all units in the architecture</li>
<li class="listitem">does not implement some advanced features found in mainline IA cores (e.g. precise event-based sampling, etc.)</li>
</ul>
</div>
</dd>
</dl>
</div>
<p>The following picture (from <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref9">[18]</a>) illustrates the building blocks of the architecture.</p>
<div class="figure"><a id="id-1.3.2.2.4.1"></a><p></p>
<p class="title"><strong>Figure&nbsp;2.&nbsp;Intel MIC architecture overview </strong></p>
<div class="figure-contents">
<div class="mediaobject" align="center">
<table style="cellpadding: 0; cellspacing: 0;" border="0" summary="manufactured viewport for HTML img" width="531">
<tbody>
<tr>
<td align="center"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/building-blocks.png" alt="Intel MIC architecture overview" width="531" align="middle"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>&nbsp;</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.3.2.3"></a>2.1.2.&nbsp; The cache hierarchy</h4>
</div>
</div>
</div>
<p>Details about the L1 and L2 cache can be found in the System Software Developers Guide <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref9">[18]</a>. We only cite the most important features here.</p>
<p>The L1 cache has a 32 KB L1 instruction cache and 32 KB L1 data cache. Associativity is 8-way, with a cache line-size of 64 byte. It has a load-to-use latency of 1 cycle, which means that an integer value loaded from the L1 cache can be used in the next clock by an integer instruction. (Vector instructions have different latencies than integer instructions.)</p>
<p>The L2 cache is a unified cache which is inclusive of the L1 data and instruction caches. Each core contributes 512 KB of L2 to the total global shared L2 cache storage. If no cores share any data or code, then the effective total L2 size of the chip is up to 31 MB. On the other hand, if every core shares exactly the same code and data in perfect synchronization, then the effective total L2 size of the chip is only 512 KB. The actual size of the workload-perceived L2 storage is a function of the degree of code and data sharing among cores and thread.</p>
<p>Like for the L1 cache, associativity is 8-way, with a cache line-size of 64 byte. The raw latency is 11 clock cycles. It has a streaming hardware prefetcher and supports ECC correction.</p>
<p>The main properties of the L1 and L2 caches are summarized in the following table (from <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref9">[18]</a>):</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col>
<col></colgroup>
<thead>
<tr>
<th>Parameter</th>
<th>L1</th>
<th>L2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coherence</td>
<td>MESI</td>
<td>MESI</td>
</tr>
<tr>
<td>Size</td>
<td>32 KB + 32 KB</td>
<td>512 KB</td>
</tr>
<tr>
<td>Associativity</td>
<td>8-way</td>
<td>8-way</td>
</tr>
<tr>
<td>Line Size</td>
<td>64 bytes</td>
<td>64 bytes</td>
</tr>
<tr>
<td>Banks</td>
<td>8</td>
<td>8</td>
</tr>
<tr>
<td>Access Time</td>
<td>1 cycle</td>
<td>11 cycles</td>
</tr>
<tr>
<td>Policy</td>
<td>pseudo LRU</td>
<td>pseudo LRU</td>
</tr>
<tr>
<td>Duty Cycle</td>
<td>1 per clock</td>
<td>1 per clock</td>
</tr>
<tr>
<td>Ports</td>
<td>Read or Write</td>
<td>Read or Write</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.3.3"></a>2.2.&nbsp; Network configuration &amp; system access</h3>
</div>
</div>
</div>
<p>Details about the system startup and the network configuration can be<br>
found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref9a">[19]</a> and in the documentation coming with the<br>
Intel Manycore Platform Software Stack (Intel MPSS) <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref6">[10]</a>.</p>
<p>To start the Intel MPSS stack and initialize the Xeon Phi coprocessor the following command has to be executed as root or during host system start-up:</p>
<pre class="screen">    weinberg@knf1:~&gt; sudo service mpss start
</pre>
<p>During start-up details are logged to <code class="code"> /var/log/messages</code>.</p>
<p>If MPSS with OFED support is needed, further the following commands have to be executed as root:</p>
<pre class="screen">    weinberg@knf1:~&gt; sudo service openibd start
    weinberg@knf1:~&gt; sudo service opensmd start
    weinberg@knf1:~&gt; sudo service ofed-mic start
</pre>
<p>Per default IP addresses 172.31.1.254 , 172.31.2.254 , 172.31.3.254 etc. are then assigned to the attached Intel Xeon Phi coprocessors. The IP addresses of the attached coprocessors can be listed via the traditional <code class="code"> ifconfig </code> Linux program.</p>
<pre class="screen">    weinberg@knf1:~&gt; /sbin/ifconfig
    ...
    mic0      Link encap:Ethernet  HWaddr 86:32:20:F3:1A:4A  
              inet addr:172.31.1.254  Bcast:172.31.1.255  Mask:255.255.255.0
              inet6 addr: fe80::8432:20ff:fef3:1a4a/64 Scope:Link
              UP BROADCAST RUNNING  MTU:64512  Metric:1
              RX packets:46407 errors:0 dropped:0 overruns:0 frame:0
              TX packets:42100 errors:0 dropped:0 overruns:0 carrier:0
              collisions:0 txqueuelen:1000 
              RX bytes:8186629 (7.8 Mb)  TX bytes:139125152 (132.6 Mb)

    mic1      Link encap:Ethernet  HWaddr FA:7A:0D:1E:65:B0  
              inet addr:172.31.2.254  Bcast:172.31.2.255  Mask:255.255.255.0
              inet6 addr: fe80::f87a:dff:fe1e:65b0/64 Scope:Link
              UP BROADCAST RUNNING  MTU:64512  Metric:1
              RX packets:5097 errors:0 dropped:0 overruns:0 frame:0
              TX packets:4240 errors:0 dropped:0 overruns:0 carrier:0
              collisions:0 txqueuelen:1000 
              RX bytes:522169 (509.9 Kb)  TX bytes:124671342 (118.8 Mb)



</pre>
<p>Further information can be obtained by running the <code class="code"> micinfo </code> program on the host. To get also PCIe related details the command has to be run with root privileges. Here is an example output for a C0 stepping Intel Xeon Phi prototype:</p>
<pre class="screen">weinberg@knf1:~&gt; sudo /opt/intel/mic/bin/micinfo
MicInfo Utility Log

Created Thu Dec  5 14:43:39 2013


        System Info
                HOST OS                 : Linux
                OS Version              : 3.0.13-0.27-default
                Driver Version          : 5889-16
                MPSS Version            : 2.1.5889-16
                Host Physical Memory    : 66056 MB

Device No: 0, Device Name: mic0

        Version
                Flash Version            : 2.1.02.0381
                SMC Boot Loader Version  : 1.8.4326
                uOS Version              : 2.6.38.8-g9b2c036
                Device Serial Number     : ADKC31202722

        Board
                Vendor ID                : 8086
                Device ID                : 225d
                Subsystem ID             : 3608
                Coprocessor Stepping ID  : 2
                PCIe Width               : x16
                PCIe Speed               : 5 GT/s
                PCIe Max payload size    : 256 bytes
                PCIe Max read req size   : 4096 bytes
                Coprocessor Model        : 0x01
                Coprocessor Model Ext    : 0x00
                Coprocessor Type         : 0x00
                Coprocessor Family       : 0x0b
                Coprocessor Family Ext   : 0x00
                Coprocessor Stepping     : C0
                Board SKU                : C0-3120P/3120A
                ECC Mode                 : Enabled
                SMC HW Revision          : Product 300W Active CS

        Cores
                Total No of Active Cores : 57
                Voltage                  : 1081000 uV
                Frequency                : 1100000 KHz

        Thermal
                Fan Speed Control        : On
                SMC Firmware Version     : 1.11.4404
                FSC Strap                : 14 MHz
                Fan RPM                  : 2700
                Fan PWM                  : 50
                Die Temp                 : 54 C

        GDDR
                GDDR Vendor              : Elpida
                GDDR Version             : 0x1
                GDDR Density             : 2048 Mb
                GDDR Size                : 5952 MB
                GDDR Technology          : GDDR5
                GDDR Speed               : 5.000000 GT/s
                GDDR Frequency           : 2500000 KHz
                GDDR Voltage             : 1000000 uV

Device No: 1, Device Name: mic1
...


</pre>
<p>Users can log in directly onto the Xeon Phi coprocessor via ssh.</p>
<pre class="screen">weinberg@knf1:~&gt; ssh mic0
[weinberg@knf1-mic0 weinberg]$ hostname
knf1-mic0
[weinberg@knf1-mic0 weinberg]$ cat /etc/issue
Intel MIC Platform Software Stack release 2.1
Kernel 2.6.38.8-g9b2c036 on an k1om
</pre>
<p>Per default the home directory on the coprocessor is <code class="code"> /home/username </code>.</p>
<p>Since the access to the coprocessor is ssh-key based users have to generate a private/public key pair via <code class="code"> ssh-keygen</code> before accessing the coprocessor for the first time.</p>
<p>After the keys have been generated, the following commands have to be executed as root to populate the filesystem image for the coprocessor on the host (<code class="code">/<wbr>opt/<wbr>intel/<wbr>mic/<wbr>filesystem/<wbr>mic0/<wbr>home</code>) with the new keys. Since the coprocessor has to be restarted to copy the new image to the coprocessor, the following commands have to be used (preferrably only by the system administrator) with care.</p>
<pre class="screen">  weinberg@knf1:~&gt; sudo service mpss stop
  weinberg@knf1:~&gt; sudo micctrl --resetconfig
  weinberg@knf1:~&gt; sudo service mpss start
</pre>
<p>On production systems access to reserved cards might be realized by the job scheduler.</p>
<p>Informations how to set up and configure a cluster with hosts containing Intel<br>
Xeon Phi coprocessors, based on how Intel configured its own Endeavor cluster<br>
can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref9b">[20]</a>.</p>
<p>Since a Linux kernel is running on the coprocessor, further information about the cores, memory etc. can be obtained from the virtual Linux /proc or /sys filesystems:</p>
<pre class="screen">[weinberg@knf1-mic0 weinberg]$ tail -n 25 /proc/cpuinfo 
processor       : 227
vendor_id       : GenuineIntel
cpu family      : 11
model           : 1
model name      : 0b/01
stepping        : 2
cpu MHz         : 1100.000
cache size      : 512 KB
physical id     : 0
siblings        : 228
core id         : 56
cpu cores       : 57
apicid          : 227
initial apicid  : 227
fpu             : yes
fpu_exception   : yes
cpuid level     : 4
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic mtrr mca pat fxsr ht syscall nx lm rep_good nopl lahf_lm
bogomips        : 2208.11
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:



[weinberg@knf1-mic0 weinberg]$ head -5 /proc/meminfo 
MemTotal:        5878664 kB
MemFree:         5638568 kB
Buffers:               0 kB
Cached:            76464 kB
SwapCached:            0 kB



</pre>
<p>To run MKL, OpenMP or MPI based programs on the coprocessor, some libraries (exact path and filenames may differ depending on the version) need to be copied to the coprocessor. On production systems the libraries might be installed or mounted on the coprocessor already. Root privileges are necessary for the destination directories given in the following example:</p>
<pre class="screen">scp /opt/intel/composerxe/mkl/lib/mic/libmkl_intel_lp64.so root@mic0:/lib64/
scp /opt/intel/composerxe/mkl/lib/mic/libmkl_intel_thread.so root@mic0:/lib64/
scp /opt/intel/composerxe/mkl/lib/mic/libmkl_core.so root@mic0:/lib64/

scp  /opt/intel/composerxe/lib/mic/libiomp5.so root@mic0:/lib64/
scp  /opt/intel/composerxe/lib/mic/libimf.so root@mic0:/lib64/
scp  /opt/intel/composerxe/lib/mic/libsvml.so root@mic0:/lib64/
scp  /opt/intel/composerxe/lib/mic/libirng.so root@mic0:/lib64/
scp  /opt/intel/composerxe/lib/mic/libintlc.so.5 root@mic0:/lib64/

scp /opt/intel/mpi-rt/4.1.0/mic/lib/libmpi_mt.so.4 root@mic0:/lib64/
scp /opt/intel/mpi-rt/4.1.0/mic/lib/libmpigf.so.4 root@mic0:/lib64/
scp /opt/intel/impi/4.1.0.024/mic/lib/libmpi.so.4 root@mic0:/lib64/

scp /opt/intel/impi/4.1.0.024/mic/bin/mpiexec.hydra root@mic0:/bin
scp /opt/intel/impi/4.1.0.024/mic/bin/pmi_proxy root@mic0:/bin


</pre>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="section-native"></a>3.&nbsp; Native compilation</h2>
</div>
</div>
</div>
<p>The simplest model of running applications on the Intel Xeon Phi coprocessor is native mode. Detailed information about building a native application for Intel Xeon Phi coprocessors can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref14a">[26]</a>.</p>
<p>In native mode an application is compiled on the host using the compiler switch <code class="code"> -mmic </code> to generate code for the MIC architecture. The binary can then be copied to the coprocessor and has to be started there.</p>
<pre class="screen">weinberg@knf1:~/c&gt; . /opt/intel/composerxe/bin/compilervars.sh intel64
weinberg@knf1:~/c&gt; icc -O3 -mmic  program.c -o program
weinberg@knf1:~/c&gt; scp program mic0:
program                                       100%   10KB  10.2KB/s   00:00    
weinberg@knf1:~/c&gt; ssh mic0 ~/program
hello, world
</pre>
<p>To achieve good performance one should mind the following items.</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Data should be <span class="bold"><strong> aligned to 64 Bytes (512 Bits) </strong></span>for the MIC architecture, in contrast to 32 Bytes (256 Bits) for AVX and 16 Bytes (128 Bits) for SSE.</li>
<li class="listitem">Due to the large SIMD width of 64 Bytes <span class="bold"><strong> vectorization is even more important for the MIC architecture than for Intel Xeon!</strong></span> The MIC architecture offers new instructions like gather/scatter, fused multiply-add, masked vector instructions etc. which allow more loops to be parallelized on the coprocessor than on an Intel Xeon based host.</li>
<li class="listitem">Use pragmas like <code class="code"> #pragma ivdep, #pragma vector always, #pragma vector aligned, #pragma simd</code> etc. to achieve autovectorization. Autovectorization is enabled at default optimization level <code class="code"> -O2 </code>. Requirements for vectorizable loops can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref23">[43]</a>.</li>
<li class="listitem">Let the compiler generate vectorization reports using the compiler option <code class="code"> -vecreport2</code> to see if loops were vectorized for MIC (Message “*MIC* Loop was vectorized” etc). The options <code class="code"> -opt-report-phase hlo</code> (High Level Optimizer Report) or <code class="code"> -opt-report-phase ipo_inl</code> (Inlining report) may also be useful.</li>
<li class="listitem">Explicit vector programming is also possible via Intel Cilk Plus language extensions (C/C++ array notation, vector elemental functions, …) or the new SIMD constructs from OpenMP 4.0 RC1.</li>
<li class="listitem">Vector elemental functions can be declared by using <code class="code">__attributes__((vector)).</code> The compiler then generates a vectorized version of a scalar function which can be called from a vectorized loop.</li>
<li class="listitem">One can use intrinsics to have full control over the vector registers and the instruction set. Include <code class="code"> &lt;immintrin.h&gt;</code> for using intrinsics.</li>
<li class="listitem">Hardware prefetching from the L2 cache is enabled per default. In addition, software prefetching is on by default at compiler optimization level <code class="code"> -O2 </code> and above. Since Intel Xeon Phi is an inorder architecture, care about prefetching is more important than on out-of-order architectures. The compiler prefetching can be influenced by setting the compiler switch <code class="code"> -opt-prefetch=n</code>. Manual prefetching can be done by using intrinsics (<code class="code">_mm_prefetch()</code>) or pragmas (<code class="code">#pragma prefetch var</code>).</li>
</ul>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="section-offload"></a>4.&nbsp;Intel compiler’s offload pragmas</h2>
</div>
</div>
</div>
<p>One can simply add OpenMP-like pragmas to C/C++ or Fortran code to mark regions of code that should be offloaded to the Intel Xeon Phi Coprocessor and be run there. This approach is quite similar to the accelerator pragmas introduced by the PGI compiler, CAPS HMPP or OpenACC to offload code to GPGPUs. When the Intel compiler encounters an offload pragma, it generates code for both the coprocessor and the host. Code to transfer the data to the coprocessor is automatically created by the compiler, however the programmer can influence the data transfer by adding data clauses to the offload pragma. Details can be found under “Offload Using a Pragma” in the Intel compiler documentation <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref17">[30]</a>.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="offload-simple-example"></a>4.1.&nbsp; Simple example</h3>
</div>
</div>
</div>
<p>In the following we show a simple example how to offload a matrix-matrix computation to the coprocessor.</p>
<pre class="programlisting">main(){
  
  double *a, *b, *c;
  int i,j,k, ok, n=100;
  
  // allocated memory on the heap aligned to 64 byte boundary
  ok = posix_memalign((void**)&amp;a, 64, n*n*sizeof(double));
  ok = posix_memalign((void**)&amp;b, 64, n*n*sizeof(double));
  ok = posix_memalign((void**)&amp;c, 64, n*n*sizeof(double));

  // initialize matrices
  ...
  //offload code
#pragma offload target(mic) in(a,b:length(n*n)) inout(c:length(n*n)) 
  {
  //parallelize via OpenMP on MIC
#pragma omp parallel for
    for( i = 0; i &lt; n; i++ ) {
      for( k = 0; k &lt; n; k++ ) {
#pragma vector aligned
#pragma ivdep
	for( j = 0; j &lt; n; j++ ) {
	  //c[i][j] = c[i][j] + a[i][k]*b[k][j];
	  c[i*n+j] = c[i*n+j] + a[i*n+k]*b[k*n+j];

        }
      }
    }
  }
}
 



</pre>
<p>This example (with quite bad performance) shows how to offload the matrix computation to the coprocessor using the <code class="code">#pragma offload target(mic)</code>. One could also specify the specific coprocessor <code class="code"> num </code> in a system with multiple coprocessors by using <code class="code"> #pragma offload target(mic:num)</code>.</p>
<p>Since the matrices have been dynamically allocated using <code class="code">posix_memalign()</code>, their sizes must be specified via the <code class="code">length()</code> clause. Using <code class="code">in</code>, <code class="code">out</code> and <code class="code"> inout </code> one can specify which data has to be copied in which direction. It is recommended that for Intel Xeon Phi data is 64-byte aligned. <code class="code"> #pragma vector aligned </code> tells the compiler that all array data accessed in the loop is properly aligned. <code class="code"> #pragma ivdep </code> discards any data dependencies assumed by the compiler.</p>
<p>Offloading is enabled per default for the Intel compiler. Use <code class="code"> -no-offload </code> to disable the generation of offload code.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.5.4"></a>4.2.&nbsp; Obtaining informations about the offloading</h3>
</div>
</div>
</div>
<p>Using the compiler option <code class="code"> -vec-report2 </code> one can see which loops have been vectorized on the host and the MIC coprocessor:</p>
<pre class="screen">weinberg@knf1:~/c&gt; icc  -vec-report2   -openmp offload.c
offload.c(57): (col. 2) remark: loop was not vectorized: vectorization 
                                       possible but seems inefficient.
...
offload.c(57): (col. 2) remark: *MIC* LOOP WAS VECTORIZED.
offload.c(54): (col. 7) remark: *MIC* loop was not vectorized: not inner loop.
offload.c(53): (col. 5) remark: *MIC* loop was not vectorized: not inner loop.
</pre>
<p>By setting the environment variable <code class="code">OFFLOAD_REPORT </code> one can obtain information about performance and data transfers at runtime:</p>
<pre class="screen">weinberg@knf1:~/c&gt; export OFFLOAD_REPORT=2
weinberg@knf1:~/c&gt; ./a.out 
[Offload] [MIC 0] [File]            offload2.c
[Offload] [MIC 0] [Line]            50
[Offload] [MIC 0] [CPU Time]        12.853562 (seconds)
[Offload] [MIC 0] [CPU-&gt;MIC Data]   9830416 (bytes)
[Offload] [MIC 0] [MIC Time]        12.208636 (seconds)
[Offload] [MIC 0] [MIC-&gt;CPU Data]   3276816 (bytes)

</pre>
<p>If a function is called within the offloaded code block, this function has to be declared with <code class="code"> __attribute__((target(mic))) </code>.</p>
<p>For example one could put the matrix-matrix multiplication of the previous example into a subroutine and call that routine within an offloaded block region:</p>
<pre class="programlisting">__attribute__((target(mic))) void mxm( int n,  double * restrict a, 
                           double * restrict b, double *restrict c ){
  int i,j,k;
  for( i = 0; i &lt; n; i++ ) {
  ...
}

main(){

...

#pragma offload target(mic) in(a,b:length(n*n)) inout(c:length(n*n)) 
  {
    mxm(n,a,b,c);
  }

}
</pre>
<p>Mind the C99 restrict keyword that specifies that the vectors do not overlap. (Compile with -std=c99)</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.5.5"></a>4.3.&nbsp; Syntax of pragmas</h3>
</div>
</div>
</div>
<p>The following offload pragmas are available (from <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref7">[11]</a>):</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col class="c1">
<col class="c2">
<col class="c3"></colgroup>
<thead>
<tr>
<th>Pragma</th>
<th>Syntax</th>
<th>Semantic</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3" align="center"><span class="bold"><strong> C/C++</strong></span></td>
</tr>
<tr>
<td>Offload pragma</td>
<td><code class="code">#pragma offload &lt;clauses&gt; &lt;statement&gt;</code></td>
<td>Allow next statement to execute on coprocessor or host CPU</td>
</tr>
<tr>
<td>Variable/function offload properties</td>
<td><code class="code">_attribute__ ((target(mic)))</code></td>
<td>Compile function for, or allocate variable on, both host CPU and coprocessor</td>
</tr>
<tr>
<td>Entire blocks of data/code defs</td>
<td>
<pre class="screen">#pragma offload_attribute(push, 
                      target(mic))
...
#pragma offload_attribute(pop)</pre>
</td>
<td>Mark entire files or large blocks of code to compile for both host CPU and coprocessor</td>
</tr>
<tr>
<td colspan="3" align="center"><span class="bold"><strong> Fortran </strong></span></td>
</tr>
<tr>
<td>Offload directive</td>
<td><code class="code">!dir$ omp offload &lt;clauses&gt; &lt;statement&gt;</code></td>
<td>Execute OpenMP parallel block on coprocessor</td>
</tr>
<tr>
<td>Variable/function offload properties</td>
<td><code class="code">!dir$ attributes offload:&lt;mic&gt; :: &lt;ret-name&gt; OR &lt;var1,var2,…&gt; </code></td>
<td>Compile function or variable for CPU and coprocessor</td>
</tr>
<tr>
<td>Entire code blocks</td>
<td>
<pre class="screen">!dir$ offload begin &lt;clauses&gt;
...
!dir$ end offload</pre>
</td>
<td>Mark entire files or large blocks of code to compile for both host CPU and coprocessor</td>
</tr>
</tbody>
</table>
</div>
<p>The following clauses can be used to control data transfers:</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col class="c1">
<col class="c2">
<col class="c3"></colgroup>
<thead>
<tr>
<th>Clause</th>
<th>Syntax</th>
<th>Semantic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multiple coprocessors</td>
<td><code class="code">target(mic[:unit])</code></td>
<td>Select specific coprocessors</td>
</tr>
<tr>
<td>Inputs</td>
<td><code class="code">in(var-list modifiers) </code></td>
<td>Copy from host to coprocessor</td>
</tr>
<tr>
<td>Outputs</td>
<td><code class="code">out(var-list modifiers)</code></td>
<td>Copy from coprocessor to host</td>
</tr>
<tr>
<td>Inputs &amp; outputs</td>
<td><code class="code">inout(var-list modifiers)</code></td>
<td>Copy host to coprocessor and back when offload completes</td>
</tr>
<tr>
<td>Non-copied data</td>
<td><code class="code">nocopy(var-list modifiers) </code></td>
<td>Data is local to target</td>
</tr>
</tbody>
</table>
</div>
<p>The following (optional) modifiers are specified:</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col class="c1">
<col class="c2">
<col class="c3"></colgroup>
<thead>
<tr>
<th>Modifier</th>
<th>Syntax</th>
<th>Semantic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Specify copy length</td>
<td><code class="code">length(N)</code></td>
<td>Copy N elements of pointer’s type</td>
</tr>
<tr>
<td>Coprocessor memory allocation</td>
<td><code class="code">alloc_if ( bool )</code></td>
<td>Allocate coprocessor space on this offload (default: TRUE)</td>
</tr>
<tr>
<td>Coprocessor memory release</td>
<td><code class="code">free_if ( bool ) </code></td>
<td>Free coprocessor space at the end of this offload (default: TRUE)</td>
</tr>
<tr>
<td>Control target data alignment</td>
<td><code class="code">align ( N bytes ) </code></td>
<td>Specify minimum memory alignment on coprocessor</td>
</tr>
<tr>
<td>Array partial allocation &amp; variable relocation</td>
<td><code class="code">alloc ( array-slice )<br>
into ( var-expr )</code></td>
<td>Enables partial array allocation and data copy into other vars &amp; ranges</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.5.6"></a>4.4.&nbsp; Recommendations</h3>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="section-explicit-worksharing"></a>4.4.1.&nbsp; Explicit worksharing</h4>
</div>
</div>
</div>
<p>To explicitly share work between the coprocessor and the host one can use OpenMP sections to manually distribute the work. In the following example both the host and the coporcessor will run a matrix-matrix multiplication in parallel.</p>
<pre class="programlisting">#pragma omp parallel
  {
#pragma omp sections
    {
#pragma omp section
      {
//section running on the coprocessor
#pragma offload target(mic) in(a,b:length(n*n)) inout(c:length(n*n)) 
	{
	  mxm(n,a,b,c);
	}
      }
#pragma omp section
      {
//section running on the host
	mxm(n,d,e,f);
      }
    }
    
  }
</pre>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.5.6.3"></a>4.4.2.&nbsp; Persistent data on the coprocessor</h4>
</div>
</div>
</div>
<p>The main bottleneck of accelerator based programming are data transfers over the slow PCIe bus from the host to the accelerator and vice versa. To increase the performance one should minimize data transfers as much as possible and keep the data on the coprocessor between computations using the same data.</p>
<p>Defining the following macros</p>
<pre class="screen">#define ALLOC   alloc_if(1)
#define FREE    free_if(1)
#define RETAIN  free_if(0)
#define REUSE   alloc_if(0)
</pre>
<p>one can simply use the following notation:</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">to allocate data and keep it for the next offload
<p><code class="code"><br>
#pragma offload target(mic) in (p:length(l) ALLOC RETAIN)<br>
</code></p></li>
<li class="listitem">to reuse the data and still keep it on the coprocessor
<p><code class="code"><br>
#pragma offload target(mic) in (p:length(l) REUSE RETAIN)<br>
</code></p></li>
<li class="listitem">to reuse the data again and free the memory. (FREE is the default, and does not need to be explicitly specified)
<p><code class="code"><br>
#pragma offload target(mic) in (p:length(l) REUSE FREE)<br>
</code></p></li>
</ul>
</div>
<p>More information can be found in the section “Managing Memory Allocation for Pointer Variables” under “Offload Using a Pragma” in the compiler documentation <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref17">[30]</a> .</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="optimized-offload-code"></a>4.4.3.&nbsp; Optimizing offloaded code</h4>
</div>
</div>
</div>
<p>The implementation of the matrix-matrix multiplication given in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#offload-simple-example">Section&nbsp;4.1</a> can be optimized by defining appropriate ROWCHUNK and COLCHUNK chunk sizes, rewriting the code with 6 nested loops (using OpenMP collapse for the 2 outermost loops) and some manual loop unrolling (thanks to A. Heinecke for input for this section).</p>
<pre class="programlisting">#define ROWCHUNK 96
#define COLCHUNK 96

#pragma omp parallel for collapse(2) private(i,j,k)
    for(i = 0; i &lt; n; i+=ROWCHUNK ) {
      for(j = 0; j &lt; n; j+=ROWCHUNK ) {
	for(k = 0; k &lt; n; k+=COLCHUNK ) {
	  for (ii = i; ii &lt; i+ROWCHUNK; ii+=6)  {
	    for (kk = k; kk &lt; k+COLCHUNK; kk++ ) {
#pragma ivdep
#pragma vector aligned 
	      for ( jj = j; jj &lt; j+ROWCHUNK; jj++){
		c[(ii*n)+jj] += a[(ii*n)+kk]*b[kk*n+jj];
		c[((ii+1)*n)+jj] += a[((ii+1)*n)+kk]*b[kk*n+jj];
		c[((ii+2)*n)+jj] += a[((ii+2)*n)+kk]*b[kk*n+jj];
		c[((ii+3)*n)+jj] += a[((ii+3)*n)+kk]*b[kk*n+jj];
		c[((ii+4)*n)+jj] += a[((ii+4)*n)+kk]*b[kk*n+jj];
		c[((ii+5)*n)+jj] += a[((ii+5)*n)+kk]*b[kk*n+jj];
              }
            }
          }
        }
      }
    }
} 


</pre>
<p>Using intrinsics with manual data prefetching and register blocking can still considerably increase the performance. Generally speaking, the programmer should try to get a suitable vectorization and write cache and register efficient code, i.e. values stored in registers should be reused as often as possible in order to avoid cache and memory access. The tuning techniques for native implementations discussed in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-native">Section&nbsp;3</a> also apply for offloaded code, of course.</p>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.5.7"></a>4.5.&nbsp; Intel Cilk Plus parallel extensions</h3>
</div>
</div>
</div>
<p>More complex data structures can be handled by Virtual Shared Memory. In this case the same virtual address space is used on both the host and the coprocessor, enabling a seamless sharing of data. Virtual shared data is specified using the <code class="code"> _Cilk_shared </code> allocation specifier. This model is integrated in Intel Cilk Plus parallel extensions and is only available in C/C++. There are also Cilk functions to specify offloading of functions and <code class="code"> _Cilk_for</code> loops. More information on Intel Cilk Plus can be found online under <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref7a">[12]</a>.</p>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.6"></a>5.&nbsp; OpenMP and hybrid</h2>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="section-openmp"></a>5.1.&nbsp; OpenMP</h3>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.6.2.2"></a>5.1.1.&nbsp; OpenMP basics</h4>
</div>
</div>
</div>
<p>OpenMP parallelization on an Intel Xeon + Xeon Phi coprocessor machine can be<br>
applied both on the host and on the device with the <code class="code"> -openmp </code> compiler option. It can run on the Xeon host, natively on the Xeon Phi coprocessor and also in different offload schemes. It is introduced with regular pragma statements in the code for either case.</p>
<p>In applications that run both on the host and on the Xeon Phi coprocessor OpenMP threads do not interfere with each other and offload statements are executed on the device based on the available resources. If there are no free threads on the Xeon Phi coprocessor or less than requested, the parallel region will be executed on the host. Offload pragma statements and OpenMP pragma statements are independent from each other and must both be present in the code. Within the latter construct apply usual semantics of shared and private data.</p>
<p>There are 16 threads available on every Xeon host CPU and 4 times the number of cores threads on every Xeon Phi coprocessor. For offload schemes the maximal amount of threads that can be used on the device is 4 times the number of cores minus one, because one core is reserved for the OS and its services. Offload to the Xeon Phi coprocessor can be done at any time by multiple host CPUs until filling the resources available. If there are no free threads, the task meant to be offloaded may be done on the host.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.6.2.3"></a>5.1.2.&nbsp; Threading and affinity</h4>
</div>
</div>
</div>
<p>The Xeon Phi coprocessor supports 4 threads per core. Unlike some CPU-intensive HPC applications that are run on Xeon architecture, which do not benefit from hyperthreading, applications run on Xeon Phi coprocessors do and using more than one thread per core is recommended.</p>
<p>The most important considerations for OpenMP threading and affinity are the total number of threads that should be utilized and the scheme for binding threads to processor cores. These are controlled with the environmental variables <code class="code"> OMP_NUM_THREADS</code> and <code class="code">KMP_AFFINITY</code>.</p>
<p>The default settings are as follows:</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col></colgroup>
<thead>
<tr>
<th></th>
<th><code class="code">OMP_NUM_THREADS</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenMP on host without HT</td>
<td>1 x ncore-host</td>
</tr>
<tr>
<td>OpenMP on host with HT</td>
<td>2 x ncore-host</td>
</tr>
<tr>
<td>OpenMP on Xeon Phi in native mode</td>
<td>4 x ncore-phi</td>
</tr>
<tr>
<td>OpenMP on Xeon Phi in offload mode</td>
<td>4 x (ncore-phi – 1)</td>
</tr>
</tbody>
</table>
</div>
<p>For host-only and native-only execution the number of threads and all other environmental variables are set as usual:</p>
<pre class="screen">% export OMP_NUM_THREADS=16
% export OMP_NUM_THREADS=240
% export KMP_AFFINITY=compact/scatter/balanced
</pre>
<p>Setting affinity to “compact” will place OpenMP threads by filling cores one by one, while “scatter” will place one thread in every core until reaching the total number of cores and then continue with the first core. When using “balanced”, it is similar to “scatter”, but threads with adjacent numbers will be placed on the same core. “Balanced” is only available for the Xeon Phi coprocessor. Another useful option is the verbosity modifier:</p>
<pre class="screen">	% export KMP_AFFINITY=verbose,balanced
</pre>
<p>With compiler version 13.1.0 and newer one can use the <code class="code">KMP_PLACE_THREADS</code> variable to point out the topology of the system to the OpenMP runtime, for example:</p>
<pre class="screen">	% export OMP_NUM_THREADS=180
	% export KMP_PLACE_THREADS=60c,3t
</pre>
<p>meaning that 60 cores and 3 threads per core should be used. Still one should use the <code class="code">KMP_AFFINITY</code> variable to bind the threads to the cores.</p>
<p>If OpenMP regions exist on the host and on the part of the code offloaded to the Phi, two separate OpenMP runtimes exist. Environment variables for controlling OpenMP behavior are to be set for both runtimes, for example the <code class="code">KMP_AFFINITY</code> variable which can be used to assign a particular thread to a particular physical node. For Phi it can be done like this:</p>
<pre class="screen">$ export MIC_ENV_PREFIX=MIC
# specify affinity for all cards
$ export MIC_KMP_AFFINITY=...
# specify number of threads for all cards
$ export MIC_OMP_NUM_THREADS=120
# specify the number of threads for card #2
$ export MIC_2_OMP_NUM_THREADS=200
# specify the number of threads and affinity for card #3
$ export MIC_3_ENV=”OMP_NUM_THREADS=60 | KMP_AFFINITY=balanced”
</pre>
<p>If <code class="code">MIC_ENV_PREFIX</code> is not set and <code class="code">KMP_AFFINITY</code> is set to “balanced” it will be ignored by the runtime.</p>
<p>One can also use special API calls to set the environment for the coprocessor only, e.g.</p>
<pre class="screen">	omp_set_num_threads_target()
	omp_set_nested_target()
</pre>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.6.2.4"></a>5.1.3.&nbsp; Loop scheduling</h4>
</div>
</div>
</div>
<p>OpenMP accepts four different kinds of loop scheduling – static, dynamic, guided and auto. In this way the amount of iterations done by different threads can be controlled. The <code class="code">schedule</code> clause can be used to set the loop scheduling at compile time. Another way to control this feature is to specify <code class="code">schedule(runtime)</code> in your code and select the loop scheduling at runtime through setting the <code class="code">OMP_SCHEDULE</code> environment variable.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.6.2.5"></a>5.1.4.&nbsp; Scalability improvement</h4>
</div>
</div>
</div>
<p>If the amount of work that should be done by each thread is non-trivial and consists of nested for-loops, one might use the <code class="code"> collapse() </code> directive to specify how many for-loops are associated with the OpenMP loop construct. This often improves scalability of OpenMP applications (see <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#optimized-offload-code">Section&nbsp;4.4.3</a>).</p>
<p>Another way to improve scalability is to reduce barrier synchronization overheads by using the <code class="code">nowait</code> directive. The effect of it is that the threads will not synchronize after they have completed their individual pieces of work. This approach is applicable combined with static loop scheduling because all threads will execute the same amount of iterations in each loop but is also a potential threat on the correctness of the code.</p>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.6.3"></a>5.2.&nbsp; Hybrid OpenMP/MPI</h3>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.6.3.2"></a>5.2.1.&nbsp; Programming models</h4>
</div>
</div>
</div>
<p>For hybrid OpenMP/MPI programming there are two major approaches: an MPI<br>
offload approach, where MPI ranks reside on the host CPU and work is offloaded to the Xeon Phi coprocessor and a symmetric approach in which MPI ranks reside both on the CPU and on the Xeon Phi. An MPI program can be structured using either model.</p>
<p>When assigning MPI ranks, one should take into account that there is a data transfer overhead over the PCIe, so minimizing the communication from and to the Xeon Phi is a good idea. Another consideration is that there is limited amount of memory on the coprocessor which favors the shared memory parallelism ideology so placing 120 MPI ranks on the coprocessor each of which starts 2 threads might be less effective than placing less ranks but allowing them to start more threads. Note that MPI directives cannot be called within a pragma offload statement.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.6.3.3"></a>5.2.2.&nbsp; Threading of the MPI ranks</h4>
</div>
</div>
</div>
<p>For hybrid OpenMP/MPI applications use the thread safe version of the Intel MPI Library by using the <code class="code">-mt_mpi</code> compiler driver option. A desired process pinning scheme can be set with the <code class="code">I_MPI_PIN_DOMAIN</code> environment variable. It is recommended to use the following setting:</p>
<pre class="screen">      $ export I_MPI_PIN_DOMAIN=omp
</pre>
<p>By setting this to omp, one sets the process pinning domain size to be to <code class="code">OMP_NUM_THREADS</code>. In this way, every MPI process is able to create <code class="code">OMP_NUM_THREADS</code> number of threads that will run within the corresponding domain. If this variable is not set, each process will create a number of threads per MPI process equal to the number of cores, because it will be treated as a separate domain.</p>
<p>Further, to pin OpenMP threads within a particular domain, one could use the <code class="code">KMP_AFFINITY</code> environment variable.</p>
</div>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.7"></a>6.&nbsp;MPI</h2>
</div>
</div>
</div>
<p>Details about using the Intel MPI library on Xeon Phi coprocessor systems can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref10">[21]</a>.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.7.3"></a>6.1.&nbsp; Setting up the MPI environment</h3>
</div>
</div>
</div>
<p>The following commands have to be executed to set up the MPI environment:</p>
<pre class="screen"># copy MPI libraries and binaries to the card (as root)
# only copying really necessary files saves memory

scp /opt/intel/impi/4.1.0.024/mic/lib/* mic0:/lib
scp /opt/intel/impi/4.1.0.024/mic/bin/* mic0:/bin

# setup Intel compiler variables
. /opt/intel/composerxe/bin/compilervars.sh intel64

# setup Intel MPI variables
. /opt/intel/impi/4.1.0.024/bin64/mpivars.sh

</pre>
<p>The following network fabrics are available for the Intel Xeon Phi coprocessor:</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col></colgroup>
<thead>
<tr>
<th>Fabric Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>shm</td>
<td>Shared-memory</td>
</tr>
<tr>
<td>tcp</td>
<td>TCP/IP-capable network fabrics, such as Ethernet and InfiniBand (through IPoIB)</td>
</tr>
<tr>
<td>ofa</td>
<td>OFA-capable network fabric including InfiniBand (through OFED verbs)</td>
</tr>
<tr>
<td>dapl</td>
<td>DAPL–capable network fabrics, such as InfiniBand, iWarp,<br>
Dolphin, and XPMEM (through DAPL)</td>
</tr>
</tbody>
</table>
</div>
<p>The Intel MPI library tries to automatically use the best available network fabric detected (usually shm for intra-node communication and InfiniBand (dapl, ofa) for inter-node communication).</p>
<p>The default can be changed by setting the <code class="code"> I_MPI_FABRICS </code> environment variable to <code class="code">I_MPI_FABRICS=&lt;fabric&gt;</code> or <code class="code">I_MPI_FABRICS=&lt;intra-node fabric&gt;:&lt;inter-nodes fabric&gt;</code>.</p>
<p>The availability is checked in the following order: shm:dapl, shm:ofa, shm:tcp.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.7.4"></a>6.2.&nbsp; MPI programming models</h3>
</div>
</div>
</div>
<p>Intel MPI for the Xeon Phi coprocessors offers various MPI programming models:</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"> <span class="bold"><strong> Symmetric model</strong></span></span></dt>
<dd>The MPI ranks reside on both the host and the coprocessor. Most general MPI case.
</dd>
<dt><span class="term"> <span class="bold"><strong> Coprocessor-only model</strong></span></span></dt>
<dd>All MPI ranks reside only on the coprocessors.
</dd>
<dt><span class="term"> <span class="bold"><strong> Host-only model</strong></span></span></dt>
<dd>All MPI ranks reside on the host. The coprocessors can be used by using offload pragmas. (Using MPI calls inside offloaded code is not supported.)
</dd>
</dl>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.7.4.4"></a>6.2.1.&nbsp; Coprocessor-only model</h4>
</div>
</div>
</div>
<p>To build and run an application in coprocessor-only mode, the following commands have to be executed:</p>
<pre class="screen">  
# compile the program for the coprocessor (-mmic)

mpiicc -mmic -o test.MIC test.c

#copy the executable to the coprocessor
scp test.MIC mic0:/tmp

#set the I_MPI_MIC variable
export I_MPI_MIC=1 

#launch MPI jobs on the coprocessor mic0 from the host
#(alternatively one can login to the coprocessor and run mpirun there)
mpirun -host mic0  -n 2 /tmp/test.MIC


</pre>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.7.4.5"></a>6.2.2.&nbsp; Symmetric model</h4>
</div>
</div>
</div>
<p>To build and run an application in symmetric mode, the following commands have to be executed:</p>
<pre class="screen">  
# compile the program for the coprocessor (-mmic)
mpiicc -mmic -o test.MIC test.c

# compile the program for the host 
mpiicc -mmic -o test test.c


#copy the executable to the coprocessor
scp test.MIC mic0:/tmp/test.MIC

#set the I_MPI_MIC variable
export I_MPI_MIC=1 


#launch MPI jobs on the host knf1 and on the coprocessor mic0
mpirun -host knf1 -n 1 ./test : -n 1  -host mic0 /tmp/test.MIC
</pre>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.7.4.6"></a>6.2.3.&nbsp; Host-only model</h4>
</div>
</div>
</div>
<p>To build and run an application in host-only mode, the following commands have to be executed:</p>
<pre class="screen">  
# compile the program for the host, 
# mind that offloading is enabled per default
mpiicc -o test test.c

# launch MPI jobs on the host knf1, the MPI process will offload code 
# for acceleration
mpirun -host knf1 -n 1 ./test 
</pre>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.7.5"></a>6.3.&nbsp; Simplifying launching of MPI jobs</h3>
</div>
</div>
</div>
<p>Instead of specifying the hosts and coprocessors via <code class="code"> -n hostname</code> one can also put the names into a hostfile and launch the jobs via</p>
<pre class="screen"> mpirun -f hostfile -n 4 ./test</pre>
<p>Mind that the executable must have the same name on the hosts and the coprocessors in this case.</p>
<p>If one sets</p>
<pre class="screen">export I_MPI_POSTFIX=.mic
</pre>
<p>the <code class="code">.mix</code> postfix is automatically added to the executable name by mpirun, so in the case of the example above <code class="code"> test </code> is launched on the host and <code class="code"> test.mic</code> on the coprocessors. It is also possible to specify a prefix using</p>
<pre class="screen">export I_MPI_PREFIX=./MIC/
</pre>
<p>In this case <code class="code"> ./MIC/test </code> will be launched on the coprocessor. This is specially useful if the host and the coprocessors share the same NFS filesystem.</p>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.8"></a>7.&nbsp;Intel MKL (Math Kernel Library)</h2>
</div>
</div>
</div>
<p>The Intel Xeon Phi coprocessor is supported since MKL 11.0. Details on using MKL with Intel Xeon Phi coprocessors can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref15">[27]</a>, <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref16">[28]</a> and <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref18">[29]</a>. Also the MKL developer zone <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref5">[8]</a> contains useful information.</p>
<p>All functions can be used on the Xeon Phi, however the optimization level for wider 512-bit SIMD instructions differs.</p>
<p>As of Intel MKL 11.0 Update 2 the following functions are highly optimized for the Intel Xeon Phi coprocessor:</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">BLAS Level 3, and much of Level 1 &amp; 2</li>
<li class="listitem">Sparse BLAS: ?CSRMV, ?CSRMM</li>
<li class="listitem">Some important LAPACK routines (LU, QR, Cholesky)</li>
<li class="listitem">Fast Fourier Transformations</li>
<li class="listitem">Vector Math Library</li>
<li class="listitem">Random number generators in the Vector Statistical Library</li>
</ul>
</div>
<p>Intel plans to optimize a wider range of functions in future MKL releases.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.8.4"></a>7.1.&nbsp; MKL usage modes</h3>
</div>
</div>
</div>
<p>The following 3 usage models of MKL are available for the Xeon Phi:</p>
<div class="orderedlist">
<ol class="orderedlist" type="1">
<li class="listitem">Automatic Offload</li>
<li class="listitem">Compiler Assisted Offload</li>
<li class="listitem">Native Execution</li>
</ol>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.8.4.4"></a>7.1.1.&nbsp; Automatic Offload (AO)</h4>
</div>
</div>
</div>
<p>In the case of automatic offload the user does not have to change the code at all. For automatic offload enabled functions the runtime may automatically download data to the Xeon Phi coprocessor and execute (all or part of) the computations there. The data transfer and the execution management is completely automatic and transparent for the user.</p>
<p>As of Intel MKL 11.0.2 only the following functions are enabled for automatic offload:</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Level-3 BLAS functions
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle;">
<li class="listitem">?GEMM (for m,n &gt; 2048, k &gt; 256)</li>
<li class="listitem">?TRSM (for M,N &gt; 3072)</li>
<li class="listitem">?TRMM (for M,N &gt; 3072)</li>
<li class="listitem">?SYMM (for M,N &gt; 2048)</li>
</ul>
</div>
</li>
<li class="listitem">LAPACK functions
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle;">
<li class="listitem">LU (M,N &gt; 8192)</li>
<li class="listitem">QR</li>
<li class="listitem">Cholesky</li>
</ul>
</div>
</li>
</ul>
</div>
<p>In the above list also the matrix sizes for which MKL decides to offload the computation are given in brackets.</p>
<p>To enable automatic offload either the function<br>
<code class="code"><br>
mkl_mic_enable()<br>
</code><br>
has to be called within the source code or the environment variable<br>
<code class="code"><br>
MKL_MIC_ENABLE=1<br>
</code><br>
has to be set. If no Xeon Phi coprocessor is detected the application runs on the host without penalty.</p>
<p>To build a program for automatic offload, the same way of building code as on the Xeon host is used:</p>
<pre class="screen">icc -O3 -mkl file.c -o file
</pre>
<p>By default, the MKL library decides when to offload and also tries to determine the optimal work division between the host and the targets (MKL can take advantage of multiple coprocessors). In case of the BLAS routines the user can specify the work division between the host and the coprocessor by calling the routine</p>
<pre class="screen">mkl_mic_set_Workdivision(MKL_TARGET_MIC,0,0.5) 
</pre>
<p>or by setting the environment variable</p>
<pre class="screen">MKL_MIC_0_WORKDIVISION=0.5
</pre>
<p>Both examples specify to offload 50% of computation only to the 1st card (card #0).</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.8.4.5"></a>7.1.2.&nbsp; Compiler Assisted Offload (CAO)</h4>
</div>
</div>
</div>
<p>In this mode of MKL the offloading is explicitly controlled by compiler pragmas or directives. In contrast to the automatic offload mode, all MKL function can be offloaded in CAO-mode.</p>
<p>A big advantage of this mode is that it allows for data persistence on the device.</p>
<p>For Intel compilers it is possible to use AO and CAO in the same program, however the work division must be explicitly set for AO in this case. Otherwise, all MKL AO calls are executed on the host.</p>
<p>MKL functions are offloaded in the same way as any other offloaded function (see section <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#section-offload">Section&nbsp;4</a>). An example for offloading MKL’s sgemm routine looks as follows:</p>
<pre class="screen">#pragma offload target(mic) \ 
  in(transa, transb, N, alpha, beta) \
  in(A:length(N*N)) \ 
  in(B:length(N*N)) \ 
  in(C:length(N*N)) \ 
  out(C:length(N*N) alloc_if(0)) { 

     sgemm(&amp;transa, &amp;transb, &amp;N, &amp;N, &amp;N, &amp;alpha, A, &amp;N, B, &amp;N, &amp;beta, C, &amp;N); 

  }

</pre>
<p>To build a program for compiler assisted offload, the following command is recommended by Intel:</p>
<pre class="screen">icc –O3 -openmp -mkl \
–offload-option,mic,ld, “-L$MKLROOT/lib/mic -Wl,\
--start-group -lmkl_intel_lp64 -lmkl_intel_thread \
-lmkl_core -Wl,--end-group” file.c –o file
</pre>
<p>To avoid using the OS core, it is recommended to use the following environment setting (in case of a 61-core coprocessor):</p>
<pre class="screen">MIC_KMP_AFFINITY=explicit,granularity=fine,proclist=[1-240:1]
</pre>
<p>Setting larger pages by the environment setting <code class="code"> MIC_USE_2MB_BUFFERS=16K</code> usually increases performance. It is also recommended to exploit data persistence with CAO.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.8.4.6"></a>7.1.3.&nbsp; Native Execution</h4>
</div>
</div>
</div>
<p>In this mode of MKL the Intel Xeon Phi coprocessor is used as an independent compute node.</p>
<p>To build a program for native mode, the following compiler settings should be used:</p>
<pre class="screen">icc -O3 -mkl -mmic file.c -o file
</pre>
<p>The binary must then be manually copied to the coprocessor via ssh and directly started on the coprocessor.</p>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.8.5"></a>7.2.&nbsp; Example code</h3>
</div>
</div>
</div>
<p>Example code can be found under <code class="code"> $MKLROOT/examples/mic_ao </code> and <code class="code"> $MKLROOT/examples/mic_offload </code>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.8.6"></a>7.3.&nbsp; Intel Math Kernel Library Link Line Advisor</h3>
</div>
</div>
</div>
<p>To determine the appropriate link line for MKL the Intel Math Kernel Library Link Line Advisor available under <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref5a">[9]</a> has been extended to include support for the Intel Xeon Phi specific options.</p>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.9"></a>8.&nbsp;TBB: Intel Threading Building Blocks</h2>
</div>
</div>
</div>
<p>The Intel TBB library is a template based runtime library for C++ code using threads that allows us to fully utilize the scaling capabilities within our code by increasing the number of threads and supporting task oriented load balancing.</p>
<p>Intel TBB is open source and available on many different platforms with most operating systems and processors. It is already popular in the C++ community. You should be able to use it with any compiler supporting ISO C++. So this is one of the advantages of Intel TBB when you intend to keep your code as easily portable as possible.</p>
<p>Typically as a rule of thumb an application must scale well past one hundred threads on Intel Xeon processors to profit from the possible higher parallel performance offered with e.g. the Intel Xeon Phi coprocessor. To check if the scaling would profit from utilising the highly parallel capabilities of the MIC architecture, you should start to create a simple performance graph with a varying number of threads (from one up to the number of cores).</p>
<p>From a programming standpoint we treat the coprocessor as a 64-bit x86 SMP-on-a-chip with an high-speed bi-directional ring interconnect, (up to) four hardware threads per core and 512-bit SIMD instructions. With the available number of cores we have easily 200 hardware threads at hand on a single coprocessor. The multi-threading on each core is primarily used to hide latencies that come implicitly with an in-order microarchitecture. Unlike hyper-threading these hardware threads cannot be switched off and should never be ignored. Generally it should be impossible for a single thread per core to approach the memory or floating point capability limit. Highly tuned codesnippets may reach saturation already at two threads, but in general a minimum of three or four active threads per cores will be needed. This is one of the reasons why the number of threads per core should be parameterized as well as the number of cores. The other reason is of course to be future compatible.</p>
<p>TBB offers programming methods that support creating this many threads in a program. In the easiest way the one main production loop is transformed by adding a single directive or pragma enabling the code for many threads.<br>
The chunk size used is chosen automatically.</p>
<p>The new Intel Cilk Plus which offers support for a simpler set of tasking capabilities fully interoperates with Intel TBB. Apart from that Intel Cilk Plus also supports vectorization. So shared memory programmers have Intel TBB and Intel Cilk Plus to assist them with built-in tasking models. Intel Cilk Plus extends Intel TBB to offer C programmers a solution as well as help with vectorization in C and C++ programs.</p>
<p>Intel TBB itself does not offer any explicit vectorization support. However it does not interfere with any vectorization solution either.</p>
<p>In relevance to the Intel Xeon Phi coprocessor TBB is just one available runtime-based parallel programming model alongside OpenMP, Intel Cilk Plus and pthreads that are also already available on the host system. Any code running natively on the coprocessor can put them to use just like it would on the host with the only difference being the larger number of threads.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.9.10"></a>8.1.&nbsp; Advantages of TBB</h3>
</div>
</div>
</div>
<p>There exists a variety of approaches to parallel programming, but there are several advantages to using Intel TBB when writing scalable applications:</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">TBB relies on generic programming: Writing the best possible algorithms with the fewest possible constraints enables to deliver high performance algorithms which can be applied in a broader context. Other more traditional libraries specify interfaces in terms of particular types or base classes. Intel TBB specifies the requirements on the types instead and in this way keeps the algorithms themselves generic and easily adaptable to different data representations.</li>
<li class="listitem">It is easy to start: You don’t have to be a threading expert to leverage multi-core performance with the help of TBB. Normally you can successfully thread some programs just by adding a single directive or pragma to the main production loop.</li>
<li class="listitem">It obeys to logical parallelism: Since with TBB you specify tasks instead of threads, you automatically produce more portable code which emphasizes scalable, data parallel programming. You are not bound to platform-dependent threading primitives; most threading packages require you to directly code on low-level constructs close to the hardware. Direct programming on raw thread level is error-prone, tedious and typically hard work since it forces you to efficiently map logical tasks into threads and it is not always leading to the desired results. With the higher level of data-parallel programming on the other hand, where you have multiple threads working on different parts of a collection, performance continues to increase as you add more cores since for a larger number of processors the collections are just divided into smaller chunks. This is a great feature when it comes to portability.</li>
<li class="listitem">TBB is compatible with other programming models: Since the library is not designed to address all kinds of threading problems, it can coexist seamlessly with other threading packages.</li>
<li class="listitem">The template-based approach allows Intel TBB to make no excuses for performance. Other general-purpose threading packages tend to be low-level tools that are still far from the actual solution, while at the same time supporting many different kinds of threading. In TBB every template solves a computationally intensive problem in a generic, simple way instead.</li>
</ul>
</div>
<p>All of these advantages make TBB popular and easily portable while at the same time facilitating data parallel programming.</p>
<p>Further advanced concepts in TBB that are not MIC specific can be found in<br>
the Intel TBB User Guide or in the Reference Manual, both available under <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref7b">[14]</a>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.9.11"></a>8.2.&nbsp; Using TBB natively</h3>
</div>
</div>
</div>
<p>Code that runs natively on the Intel Xeon Phi coprocessor can apply the TBB parallel programming model just as they would on the host, with no unusual complications beyond the larger number of threads.</p>
<p>In order to initialize your compiler environment variables needed to set up TBB correctly, typically the<br>
<code class="code"> /opt/intel/composerxe/tbb/bin/tbbvars.csh</code> or <code class="code">tbbvars.sh</code> script with <code class="code">intel64</code> as the argument is called by the <code class="code">/<wbr>opt/<wbr>intel/<wbr>composerxe/<wbr>bin/<wbr>compilervars.csh</code> or <code class="code">compilervars.sh</code> script with <code class="code">intel64</code> as argument.<br>
(e.g. <code class="code">source /opt/intel/composerxe/bin/compilervars.sh intel64</code>)</p>
<p>Normally there is no need to call the <code class="code">tbbvars</code> script directly and it is not advisable either since the <code class="code"> compilervars </code> script also calls other subscripts taking i.e. care of the debugger or Intel MKL and running the subscripts out of order might result in unpredictable behavior.</p>
<p>A minimal C++ TBB example looks as follows:</p>
<pre class="programlisting">#include “tbb/task_scheduler_init.h”
#include “tbb/parallel_for.h”
#include "tbb/blocked_range.h"

using namespace tbb;

int main() {

task_scheduler_init init;

return 0;
}
</pre>
<p>The <code class="code">using</code> directive imports the namespace <code class="code">tbb</code> where all of the library’s classes and functions are found. The namespace is explicit in the first mention of a component, but implicit afterwards. So with the <code class="code">using namespace</code> statement present you can use the library component identifiers without having to write out the namespace prefix <code class="code">tbb</code> before each of them.</p>
<p>The task scheduler is initialized by instantiating a <code class="code">task_scheduler_init</code> object in the main function. The definition for the <code class="code">task_scheduler_init</code> class is included from the corresponding header file.<br>
Actually any thread using one of the provided TBB template algorithms must have such an initialized <code class="code">task_scheduler_init</code> object. The default constructor for the <code class="code">task_scheduler_init </code> object informs the task scheduler that the thread is participating in task execution, and the destructor informs the scheduler that the thread no longer needs the scheduler. With the newer versions of Intel TBB as used in a MIC environment the task scheduler is automatically initialized, so there is no need to explicitely initialize it if you don’t need to have control over when the task scheduler is constructed or destroyed. When initializing it you also have the further possibility to tell the task scheduler explicitly how many worker threads there are to be used and what their stack size would be.</p>
<p>In the simplest form scalable parallelism can be achieved by parallelizing a loop of iterations that can each run independently from each other.</p>
<p>The <code class="code">parallel_for</code> template function replaces a serial loop where it is safe to process each element concurrently.</p>
<p>A typical example would be to apply a function <code class="code">Foo</code> on all elements of an array over the iterations space of type <code class="code">size_t</code> going from 0 to n-1:</p>
<pre class="programlisting">void SerialApplyFoo( float a[], size_t n ) {
for( size_t i=0; i!=n; ++i )
Foo(a[i]);
}
</pre>
<p>becomes</p>
<pre class="programlisting">void ParallelApplyFoo( float a[], size_t n) {
parallel_for(size_t(0), n, [=](size_t i) {Foo(a[i]);});
}
</pre>
<p>This is the TBB short form of a <code class="code"> parallel_for</code> over a loop based on a one-dimensional iteration space consisting of a consecutive range of integers (which is one of the most common cases). The expression <code class="code">parallel_for(first,last,step,f)</code> is synonymous to <code class="code">for(auto i=first; i!=last; i+=step) f(i)</code> except that each <code class="code">f(i)</code> can be evaluated in parallel if resources permit. The omitted step parameter is optional. The short form implicitly uses automatic chunking.</p>
<p>The long form would be:</p>
<pre class="programlisting">void ParallelApplyFoo( float* a, size_t n ) {
parallel_for( blocked_range&lt;size_t&gt;(0,n),
[=](const blocked_range&lt;size_t&gt;&amp; r) {
for(size_t i=r.begin(); i!=r.end(); ++i)
Foo(a[i]);
});
}
</pre>
<p>Here the key feature of the TBB library is more clearly revealed. The template function <code class="code">tbb::parallel_for</code> breaks the iteration space into chunks, and runs each chunk on a separate thread. The first parameter of template function call <code class="code">parallel_for</code> is a <code class="code">blocked_range</code> object that describes the entire iteration space from 0 to n-1. The <code class="code">parallel_for</code> divides the iteration space into subspaces for each of the over 200 hardware threads.</p>
<p><code class="code"> blocked_range&lt;T&gt;</code>is a template class provided by the TBB library describing a one-dimensional iteration space over type <code class="code">T</code>. The <code class="code">parallel_for</code> class works just as well with other kinds of iteration spaces. The library provides <code class="code">blocked_range2d</code> for two-dimensional spaces. There exists also the possibility to define own spaces.<br>
The general constructor of the <code class="code">blocked_range</code> template class is <code class="code">blocked_range&lt;T&gt;(begin,end,grainsize)</code>. The <code class="code">T</code> specifies the value type. <code class="code">begin</code> represents the lower bound of the half-open range interval <code class="code">[begin,end)</code> representing the iteration space. <code class="code">end</code> represents the excluded upper bound of this range. The <code class="code">grainsize</code> is the approximate number of elements per sub-range. The default <code class="code">grainsize</code> is 1.</p>
<p>A parallel loop construct introduces overhead cost for every chunk of work that it schedules. The MIC adapted Intel TBB library chooses chunk sizes automatically, depending upon load balancing needs. The heuristic normally works well with the default <code class="code">grainsize</code>. It attempts to limit overhead cost while still providing ample opportunities for load balancing.<br>
For most use cases automatic chunking is the recommended choice. There might be situations though where controlling the chunk size more precisely might yield better performance.</p>
<p>When compiling programs that employ TBB constructs, be sure to link in the Intel TBB shared library with <code class="code">–ltbb</code>. If you don’t undefined references will occur.</p>
<pre class="programlisting">icc -mmic –ltbb foo.cpp
</pre>
<p>Afterwards you can use <code class="code">scp</code> to upload the binary and any shared libraries required by your application to the coprocessor.<br>
On the coprocessor you can then export the library path and run the application.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.9.12"></a>8.3.&nbsp; Offloading TBB</h3>
</div>
</div>
</div>
<p>The Intel TBB header files are not available on the Intel MIC target environment by default (the same is also true for Intel Cilk Plus). To make them available on the coprocessor the header files have to be wrapped with <code class="code">#pragma offload</code> directives as demonstrated in the example below:</p>
<pre class="programlisting">#pragma offload_attribute (push,target(mic))
#include “tbb/task_scheduler_init.h”
#include “tbb/parallel_for.h”
#include "tbb/blocked_range.h"
#pragma offload_attribute (pop)
</pre>
<p>Functions called from within the offloaded construct and global data required on the Intel Xeon Phi coprocessor should be appended by the special function attribute <code class="code"> __attribute__((target(mic)))</code>.</p>
<p>Codes using Intel TBB with an offload should be compiled with <code class="code">-tbb</code> flag instead of <code class="code">-ltbb</code>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.9.13"></a>8.4.&nbsp;Examples</h3>
</div>
</div>
</div>
<p>The TBB library provides support for parallel algorithms,<br>
concurrent containers, tasks, synchronization, memory allocation<br>
and thread control. Amongst it, the templates cover well-known<br>
parallel-programming patterns like parallel loops or reduction<br>
operations, task-based constructs or pipelines. An easy and<br>
flexible formulation of parallel computations is possible. The<br>
programmer’s task is mainly to expose a high degree of parallelism<br>
to the processor, to allow or support the vectorization of<br>
calculations, and to maximize the cache-locality of algorithms.<br>
Principles of vectorization and striving for implementations with<br>
high cache-locality are general topics of performance<br>
optimization.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.9.13.3"></a>8.4.1.&nbsp;Exposing parallelism to the processor</h4>
</div>
</div>
</div>
<p>The concrete parallelization of a certain algorithm is of course<br>
dependent on the specific floating-point operations used, its<br>
memory access pattern, the possible decoupling into parallel<br>
operations resp. vice-versa the dependencies between the<br>
calculations. However, some estimation can be made what degree<br>
the needed parallelism should have in order to achieve efficient<br>
execution on the coprocessor.</p>
<p>Xeon Phi processors can execute 4 hyperthreads on each core.<br>
That results in 240 threads active at the same time on the 60<br>
cores of the coprocessor. A rule of thumb is that one should<br>
have about 10 computational tasks per thread in order to<br>
compensate delays from load imbalances. That results in 2400<br>
parallel tasks. Efficient calculations have to apply the vector<br>
units that can process 16 single precision floating-point<br>
numbers in one instruction. Therefore, we would need several ten<br>
thousand operations that can be executed independently from each<br>
other in order to keep the processor busy.</p>
<p>On the other hand, good load balancing or the placement of<br>
several MPI tasks on the Xeon Phi can lower the needed degree of<br>
parallelism. Considerations also have to be made if it would<br>
really increase the performance to use all 4 hyperthreads of a<br>
core. Those hyperthreads share certain hardware resources, and<br>
depending on the algorithm, competition for the resources can<br>
occur causing an increase of the overall execution speed.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h5 class="title"><a id="id-1.9.13.3.5"></a>8.4.1.1.&nbsp;Parallelisation with the task construct</h5>
</div>
</div>
</div>
<p>This example shows how one can implement a multi-threaded,<br>
task-oriented algorithm. Nevertheless, there is no need to<br>
deal with the subtleties of thread creation and their control<br>
for the implementer of the algorithm.</p>
<p>The class <code class="literal">tbb:task</code> represents a low-level<br>
task with low overhead. It will be used by higher-level<br>
constructs like <code class="literal">parallel_for</code> or<br>
<code class="literal">task_group</code>.</p>
<p>Our example shows the use of task groups in a walkthrough<br>
through a binary tree. The sum of values stored in each node<br>
will be calculated during this walkthrough.</p>
<p>The tree nodes are defined as follows:</p>
<pre class="programlisting">struct tree {
    struct tree *l, *r;  // left and right subtree
    long depth;          // depth of the tree starting from here
    float v;             // an important value
};
</pre>
<p>The serial implementation could be written as:</p>
<pre class="programlisting">float tree_sum( struct tree *r )
{
    float mysum = r-&gt;v;

    if ( root-&gt;l != NULL ) mysum += tree_sum( r-&gt;l );
    if ( root-&gt;r != NULL ) mysum += tree_sum( r-&gt;r );

    return mysum;
}
</pre>
<p>A TTB task that performs the same computation is given with<br>
the following class <code class="literal">ParallelSumTask</code>. It<br>
must contain a method <code class="literal">exec()</code>. This method<br>
does either a serial computation of the value sum if its<br>
subtree has a depth lower than 10 levels or creates two<br>
subtasks that calculate the value sums of the left resp. right<br>
child tree. These tasks will be added to the task list of the<br>
active scheduler. After their termination will the current<br>
task add the value sums of both child trees and their own<br>
value and terminate itself.</p>
<pre class="programlisting">class ParallelSumTask : public tbb::task {
    float       *mysum;
    struct tree *myr;

public:

    ParallelSumTask( TreeNode *r, float *sum ) : myr(r), mysum(sum) {}

    task *execute() {
        if( root-&gt;depth &lt; 10 ) {
            *mysum = tree_sum(myr);
        }
        else {
            float sum_l, sum_r;
            tbb::task_list tlist;

            // Create subtasks for processing of left and right subtree.
            int count = 1;
            if( myr-&gt;l ) {
                ++count;
                tlist.push_back( *new(allocate_child())
                                    ParallelSumTask(myr-&gt;l, &amp;sum_l));
            }
            if( myr-&gt;r ) {
                ++count;
                list.push_back( *new(allocate_child())
                                   ParallelSumTask(myr-&gt;r, &amp;sum_r));
            }
            // Start task processing
            set_ref_count(count);
            spawn_and_wait_for_all(list);

            // Add own value and sums of left and right subtree.
            *mysum = myr-&gt;v;
            if( myr-&gt;l ) *mysum += sum_l;
            if( myr-&gt;r ) *mysum += sum_r;
        }
        return NULL;
    }
};
</pre>
<p>The calculation of the value sum could be started in the<br>
following way:</p>
<pre class="programlisting">float value_sum;
struct tree *tree_root;

// ... Initialize tree ...

// Initialize a task scheduler with the maximum number of threads.
tbb::task_scheduler_init init( 240 );
// Perform the calculation.
ParallelSumTask *a = new(tbb::task::allocate_root())
                            ParallelSumTask(tree_root, &amp;value_sum);
tbb::task::spawn_root_and_wait*(a);
// The sum of all values stored in the tree nodes
// has been calculated in variable "value_sum"
</pre>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h5 class="title"><a id="id-1.9.13.3.6"></a>8.4.1.2.&nbsp;Loop parallelisation</h5>
</div>
</div>
</div>
<p>TBB provides several algorithms that can be used for the<br>
parallelisation of loops, again without forcing the<br>
implementer do deal with the details of low-level threads. The<br>
following example provides the details how a loop can be<br>
parallelized with the <code class="literal">parallel_for</code><br>
template. The implemented algorithm shall calculate the sum of<br>
the vector elements.</p>
<p>The serial implementation could be written as follows:</p>
<pre class="programlisting">float serial_vecsum( float *vec, int len) {
   float sum = 0;

   for (int i = 0; i &lt; len; ++i)
      sum += vec[i];
   return sum;
}
</pre>
<p>The parallel algorithm shall compute the partial sums of<br>
partial vectors. The overall sum of all partial vectors will<br>
be computed as sum of these partial sums.</p>
<p>The TBB template <code class="literal">parallel_reduce</code> provides<br>
the basic implementation of subdividing a range to iterate<br>
over into several subranges. The subranges will be assigned to<br>
different threads that perform the iterations on them. The<br>
program has to provide a “loop body” that that will<br>
be applied to each array element as well as a function that<br>
processes the results of the iterations over the subranges of<br>
the vector.</p>
<p>These functionality for the vector summation will be provided<br>
in the class <code class="literal">adder</code>. The transformation<br>
function working on the vector elements has to be provided as<br>
<code class="literal">operator()</code> method. The function that<br>
reduces the results of the parallel work on different<br>
subvectors has to be implemented as method<br>
<code class="literal">join()</code>, which takes a reference to another<br>
instance of the adder class as argument. Finally, we have to<br>
equip the adder class with a copy constructor because each<br>
thread will get one copy of the originally provided adder<br>
instance.</p>
<pre class="programlisting">class adder {
    float *myvec // pointer o array with values to sum.

public:
    float sum;

    adder ( float *a ): myvec(a), sum(0) { }

    adder ( adder &amp;a, split ): myvec(a.myvec, su (0) { }

    void operator()( const tbb::blocked_range&lt;float&gt; &amp;r ) {
        for( float i = r.begin(); i != r.end( ); ++i )
            sum += myvec[i];
    }

    void join( const adder &amp;other ) {
        sum += other.sum;
    }
};
</pre>
<p>The listing shows how the partial sum will be calculated by<br>
iterating over the elements of the subvector in<br>
<code class="literal">operator()</code>. The reduction of two partial<br>
sums to one value is implemented in <code class="literal">join()</code>.</p>
<p>The summation of all array elements can be performed with the<br>
following code piece:</p>
<pre class="programlisting">float *vec; // Vector data, initialise them.
int veclen; // ...

adder vec_adder(vec); // Create root object for decomposition.
parallel_reduce(tbb::blocked_range&lt;int&gt;(0, size, 5), vec_adder);
   // Define range and grainsize for iteration.
// vec_adder.sum contains the sum of all vector elements.
</pre>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.9.13.4"></a>8.4.2.&nbsp;Vectorization and Cache-Locality</h4>
</div>
</div>
</div>
<p>Vectorization and cache-locality should be used in order to<br>
increase the program efficiency. Many details and examples are<br>
given in the programming and compilation guide of the MIC<br>
architecture <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref14">[25]</a>.</p>
<p>Vectorization can be achieved by<br>
<a class="link" href="http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/composerxe/compiler/cpp-win/index.htm#GUID-7D541D6D-4929-4F35-A58D-B67F9A897AA0.htm" target="_top">auto<br>
vectorization</a> and checked with the<br>
<a class="link" href="http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/composerxe/compiler/cpp-win/index.htm#GUID-3D61D83A-857D-49C3-A6C9-A1037BFA63CD.htm" target="_top">vec-report</a><br>
option. There is the possibility to apply<br>
<a class="link" href="http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/composerxe/compiler/cpp-win/index.htm#GUID-42986DEF-8710-453A-9DAC-2086EE55F1F5.htm" target="_top">user<br>
mandated vectorization</a> for constructs that are not<br>
covered by automatic vectorization. Users of Cilk Plus also have<br>
an option to use<br>
<a class="link" href="http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/composerxe/compiler/cpp-win/index.htm#GUID-B4E06ED4-184F-40E6-A8B4-117947D8C7AD.htm" target="_top">extensions<br>
for array notation</a>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.9.13.5"></a>8.4.3.&nbsp;Work-stealing versus Work-sharing</h4>
</div>
</div>
</div>
<p>Using work-stealing for task scheduling means that idle threads<br>
will take over tasks from other busy threads. This approach has<br>
been mentioned already above when it was said that there should<br>
be a certain amount of tasks per thread available in order to<br>
compensate load imbalances. The scheduler could keep idle<br>
processors busy if sufficient many tasks are available.</p>
<p>Work-sharing as method of the task scheduling is worthwhile for<br>
well-balanced workloads. Work-sharing is typically implemented<br>
as task pool and achieves near optimal occupancy for balanced<br>
workload.</p>
</div>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.10"></a>9.&nbsp;IPP: The Intel Integrated Performance Primitives</h2>
</div>
</div>
</div>
<p><span class="strong"><strong>Remark.</strong></span> This section gives a<br>
general overview about IPP based on version 8.0, the latest version<br>
at the moment of writing. IPP cannot be used on Xeon Phi devices at<br>
the moment. There is not yet a decision whether or when IPP will be<br>
supported in the coprocessors. The latest release, published in<br>
September 2013, contains a technology preview of asynchronous<br>
implementations as well as algorithms that have been implemented in<br>
the OpenCL programming language in order to support processing on<br>
GPU.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.10.3"></a>9.1.&nbsp;Overview of IPP</h3>
</div>
</div>
</div>
<p><span class="strong"><strong>Intel Integrated Performance Primitives (IPP)</strong></span><br>
is a software library, which provides a large collection of<br>
functions for signal processing and multimedia processing. It is<br>
useable on MS-Windows, Linux, and Mac OS X platforms.</p>
<p>The covered topics are</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Signal processing, including
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle;">
<li class="listitem">FFT, FIR, Transformations, Convolutions etc</li>
</ul>
</div>
</li>
<li class="listitem">Image &amp; frame processing, including
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle;">
<li class="listitem">Transformations, filters, color manipulation</li>
</ul>
</div>
</li>
<li class="listitem">General functionality, including
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle;">
<li class="listitem">Statistical functions</li>
<li class="listitem">Vector, matrix and linear algebra for small matrices.</li>
</ul>
</div>
</li>
</ul>
</div>
<p>The functions in this library are optimised to use advanced<br>
features of the processors like SSE and AVX instruction sets. Many<br>
functions are internally parallelised using OpenMP.</p>
<p><span class="strong"><strong>Practical aspects.</strong></span> IPP requires<br>
support of Streaming SIMD extensions (SSE) or Advanced Vector<br>
Extensions (AVX). The validated operating systems include newer<br>
versions of MS Windows (Windows 2003, Windows 7, Windows 2008),<br>
Redhat Enterpreise Linux (RHEL) 5 and 6, and Mac OS X 10.6 or<br>
higher. More detailed information can be found in the product<br>
documentation.</p>
<p>The library is compatible to Intel, Microsoft, GNU and cctools<br>
compilers. It contains freely distributable runtime libraries in<br>
order to allow the execution of programs for users without having<br>
the need to install the development tools.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.10.4"></a>9.2.&nbsp;Using IPP</h3>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.10.4.2"></a>9.2.1.&nbsp;Getting Started</h4>
</div>
</div>
</div>
<p>The following example has been taken from the IPP<br>
User’s Guide <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refipp0">[31]</a>.</p>
<pre class="programlisting">#include "ipp.h"
#include &lt;stdio.h&gt;

int main(int argc, char* argv[])
{

    const IppLibraryVersion *lib;
    Ipp64u fm;

    ippInit();                 //Automatic best static dispatch
    //ippInitCpu(ippCpuSSSE3); //Select a specific static library level
                               //Can be useful for debugging/profiling
    // Get version info
    lib = ippiGetLibVersion();
    printf(%s %s\n, lib-&gt;Name, lib-&gt;Version);
    //Get CPU features enabled with selected library level
    fm = ippGetEnabledCpuFeatures();
    printf(SSE2  %c\n,(fm&gt;&gt;2)&amp;1? 'Y' : 'N');
    printf(SSE3  %c\n,(fm&gt;&gt;3)&amp;1? 'Y' : 'N');
    printf(SSSE3 %c\n,(fm&gt;&gt;4)&amp;1? 'Y' : 'N');
    printf(SSE41 %c\n,(fm&gt;&gt;6)&amp;1? 'Y' : 'N');
    printf(SSE42 %c\n,(fm&gt;&gt;7)&amp;1? 'Y' : 'N');
    printf(AVX   %c OS Enabled %c\n,
           (fm&gt;&gt;8)&amp;1 ? 'Y' : 'N', (fm&gt;&gt;9)&amp;1 ? 'Y' : 'N');
    printf(AES   %c CLMUL      %c\n,
           (fm&gt;&gt;10)&amp;1 ? 'Y' : 'N', (fm&gt;&gt;11)&amp;1 ? 'Y': 'N');

    return 0;
}
</pre>
<p>This program contains three steps. The initialisation with<br>
<code class="literal">ippInit()</code> makes sure that the best possible<br>
implementation of IPP functions will be used during the program<br>
execution. The next step provides nformation about the used<br>
library version, and finally will be the enabled hardware<br>
features listed.</p>
<p><span class="strong"><strong>Building of the program.</strong></span> The<br>
first step to build the program is to provide the correct<br>
environment settings for the compiler. Intel’s default solution<br>
is a shellscript compilervars.sh in the bin directory of the<br>
installation that should be executed. Some other software like<br>
the modules environment is available on some HPC computer<br>
systems too. Please refer to the locally available<br>
documentation.</p>
<p>The program, saved in the file ipptest.cpp can be compiled and<br>
linked with the following command line:</p>
<pre class="programlisting">icc ipptest.cpp -o ipptest \
     -I$IPPROOT/include -L$IPPROOT/lib/intel4 \
     -lippi -lipps -lippcore
</pre>
<p>The executable can be started with the following command:</p>
<pre class="programlisting">./ipptest
</pre>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.10.4.3"></a>9.2.2.&nbsp;Linking of Applications</h4>
</div>
</div>
</div>
<p>IPP contains different implementations of each function that<br>
provide the best performance on different processor<br>
architectures. They will be selected by means of dispatching<br>
while the programmer uses always the same API.</p>
<p><span class="strong"><strong>Dynamic Linking.</strong></span> Dynamic<br>
linking of IPP can be achieved by using Intel’s compiler switch<br>
<code class="literal">-ipp</code> or by linking with the default libraries<br>
that have names that do not end in <code class="literal">_l</code> resp.<br>
<code class="literal">_t</code>. The dynamic libraries use internally<br>
OpenMP in order to achive multi-threaded execution. The<br>
multithreading within the IPP routines can be turned off by<br>
calling the function <code class="literal">ippSetNumThreads(1)</code>.<br>
Other options are static single-threaded linking or to build a<br>
single-threaded shared library from the static single-threaded<br>
libraries.</p>
<p><span class="strong"><strong>Static Linking.</strong></span> The libraries<br>
with names ending in <code class="literal">_l</code> must be used for<br>
static single- threaded linking, while the libraries with names<br>
ending in <code class="literal">_t</code> provide will provide<br>
multi-threaded implementations based again on OpenMP.</p>
<p><span class="strong"><strong>More information.</strong></span> The exact<br>
choice of the linking model depends on several factors like<br>
intended processor architectures for the execution, useable<br>
memory size during the execution, installation aspects and more.<br>
A white paper available online focuses on such aspects and<br>
provides an in-depth discussion of the topic.</p>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.10.5"></a>9.3.&nbsp;Multithreading</h3>
</div>
</div>
</div>
<p>The use of treads within the IPP should also be taken into<br>
consideration by the application developer.</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">IPP uses OpenMP in order to achieve internally a<br>
multi-threaded execution as mentioned in the section about<br>
linking. IPP uses processors up to the minimum of<br>
<code class="literal">$OMP_NUM_THREADS</code> and the number of<br>
available processors. Another possibility to get and set the<br>
number of used processors are the functions<br>
<code class="literal">ippSetNumThreads(int n)</code> and<br>
<code class="literal">ippGetNumThreads()</code>.</li>
<li class="listitem">Some functions (for example FFT) are designed to use two<br>
threads that should be mapped onto the same die in order to<br>
use a shared L2 cache if available. The user should set the<br>
following environment variable when using processors with more<br>
than two cores per die in order to ensure best performance:<br>
<code class="literal">KMP_AFFINITY=compact</code>.</li>
<li class="listitem">There is a risk of thread oversubscription and performance<br>
degradation in the case that an application uses OpenMP<br>
additionally. The use of threads within IPP can be turned off<br>
by calling <code class="literal">ippSetNumThreads(1)</code>. However,<br>
some OpenMP-related functionality could be active regardless<br>
of that. Therefore, single-threaded execution can be achieved<br>
best by using the single-threaded libraries.</li>
</ul>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.10.6"></a>9.4.&nbsp;Links and References</h3>
</div>
</div>
</div>
<p>Intel Software Documentation Library <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refipp1">[32]</a>.</p>
<p>Technical information:<br>
Selecting<br>
the Intel Integrated Performance Primitives (Intel IPP) libraries<br>
needed by your application <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refipp2">[33]</a>.</p>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.11"></a>10.&nbsp;Further programming models</h2>
</div>
</div>
</div>
<p>The programming models OpenCL and OpenACC have become popular to program<br>
GPGPUs and have also been enabled for the Intel MIC architecture.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.11.3"></a>10.1.&nbsp;OpenCL</h3>
</div>
</div>
</div>
<p>OpenCL (Open Computing Language) <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopencl0">[34]</a> is the first open, royalty-free<br>
standard for cross-platform, parallel programming of modern processors<br>
found in personal computers, servers and handheld/embedded devices.</p>
<p>OpenCL is maintained by the non-profit technology consortium<br>
Khronos Group. It has been adopted by Apple, Intel, Qualcomm, Advanced Micro<br>
Devices (AMD), Nvidia, Altera, Samsung, Vivante and ARM Holdings.</p>
<p>OpenCL 2.0 is the latest significant evolution of the OpenCL standard, designed to further simplify cross-platform programming, while enabling a rich range of algorithms and programming patterns to be easily accelerated.</p>
<p>A coding guide for developing OpenCL applications for the Intel Xeon Phi<br>
coprocessor can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopencl1">[35]</a>.<br>
More details are provided in the Intel SDK for OpenCL Applications XE – Optimization Guide <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopencl2">[36]</a>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.11.4"></a>10.2.&nbsp;OpenACC</h3>
</div>
</div>
</div>
<p>The OpenACC Application Program Interface <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopenacc1">[37]</a><br>
describes a collection of compiler directives to specify loops and regions of<br>
code in standard C, C++ and Fortran to be offloaded from a host CPU to an<br>
attached accelerator, providing portability across operating systems, host<br>
CPUs and accelerators. OpenACC is designed for portability across a wide range<br>
of accelerators and coprocessors, including APUs, GPUs, many-core and<br>
multi-core implementations. The standard is developed by Cray, CAPS, Nvidia and PGI.</p>
<p>Intel Xeon Phi support is provided by the French company CAPS <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopenacc2">[38]</a>. Based on the directive-based OpenACC and OpenHMPP standards, CAPS compilers enable developers to incrementally build portable applications for various many-core systems such as NVIDIA and AMD GPUs, and Intel Xeon Phi.</p>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.12"></a>11.&nbsp;Debugging</h2>
</div>
</div>
</div>
<p>Information about debugging on Intel Xeon Phi coprocessors can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref11">[22]</a>.</p>
<p>The GNU debugger (gdb) has been enabled by Intel to support the Intel Xeon Phi coprocessor. The debugger is now part of the recent MPSS release and does not have to be downloaded separately any more.</p>
<p>There are 2 different modes of debugging supported: native debugging on the coprocessor or remote cross-debugging on the host.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.12.5"></a>11.1.&nbsp; Native debugging with gdb</h3>
</div>
</div>
</div>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Run gdb on the coprocessor
<pre class="screen"> ssh -t mic0 /usr/bin/gdb</pre>
</li>
<li class="listitem">One can then attach to a running application with process ID pid via
<pre class="screen"> (gdb) attach pid</pre>
</li>
<li class="listitem">or alternatively start an application from within gdb via
<pre class="screen">(gdb) file /path/to/application 
(gdb) start
</pre>
</li>
</ul>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.12.6"></a>11.2.&nbsp; Remote debugging with gdb</h3>
</div>
</div>
</div>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Run the special gdb version with Xeon Phi support on the host
<pre class="screen">/<wbr>usr/<wbr>linux-k1om-4.7/<wbr>bin/<wbr>x86_64-k1om-linux-gdb</pre>
</li>
<li class="listitem">Start the gdbserver on the coprocessor by typing on the host gdb
<pre class="screen"> (gdb) target extended-remote| ssh -T mic0 gdbserver -multi -</pre>
</li>
<li class="listitem">Attach to a remotely running application with the remote process ID pid
<pre class="screen">(gdb) file /local/path/to/application 
(gdb) attach pid
</pre>
</li>
<li class="listitem">It is also possible to run an application directly from the host gdb
<pre class="screen">(gdb) file /local/path/to/application 
(gdb) set remote exec-file /remote/path/to/application
</pre>
</li>
</ul>
</div>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.13"></a>12.&nbsp;Tuning</h2>
</div>
</div>
</div>
<p>Information and material on performance tuning from Intel can<br>
be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref20">[40]</a> and<br>
<a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref21">[41]</a>.</p>
<p>A single Xeon Phi core is slower than a Xeon core due to lower<br>
clock frequency, smaller caches and lack of sophisticated features<br>
such as out-of-order execution and branch prediction. To fully<br>
exploit the processing power of a Xeon Phi, parallelism on both<br>
intruction level (SIMD) and thread level (OpenMP) is<br>
needed.</p>
<p>The following sections describe a few basic methodologies for<br>
improving the performance of a code on a Xeon Phi. As a<br>
first step, we consider methods for improving the performance on a<br>
single core. We then continue by thread-level parallelization in<br>
shared memory.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.13.5"></a>12.1.&nbsp;Single core optimization</h3>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.5.3"></a>12.1.1.&nbsp;Memory alignment</h4>
</div>
</div>
</div>
<p>Xeon Phi can only perform memory reads/writes on 64-byte<br>
aligned data. Any unaligned data will fetched and stored by<br>
performing a masked unpack or pack operation on the first and<br>
last unaligned bytes. This may cause performance degredation,<br>
especially if the data to be operated on is small in size and<br>
mostly unaligned. In the following, we list a few of the most<br>
common ways to let the compiler to align the data or to assume<br>
that the data has been aligned.</p>
<p>Compiler flags for alignment</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C/C++<br>
</span></dt>
<dd><code class="code">n/a</code><p></p>
</dd>
<dt><span class="term"><br>
Fortran<br>
</span></dt>
<dd>Align arrays: <code class="code">-align<br>
array64byte</code><p></p>
<p>Align fields of derived types: <code class="code">-align<br>
rec64byte</code></p>
</dd>
</dl>
</div>
<p>Compiler directives for alignment</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C/C++<br>
</span></dt>
<dd>Align variable <code class="varname">var</code>:<p></p>
<p><code class="code">float var[100] __attribute__((aligned(64)));</code></p>
<p>Inform the compiler of the<br>
alignment of variable <code class="varname">var</code>:</p>
<p><code class="code"> __assume_aligned(var, 64)</code></p>
<p>Declare a loop to be aligned:</p>
<p><code class="code">#pragma vector aligned</code></p>
</dd>
<dt><span class="term"><br>
Fortran<br>
</span></dt>
<dd>Align variable <code class="varname">var</code>:<p></p>
<p><code class="code">real var(100)</code></p>
<p><code class="code">!dir$ attributes align:64::var</code></p>
<p>Inform the compiler of the<br>
alignment of variable <code class="varname">var</code>:</p>
<p><code class="code">!dir$ assume_aligned<br>
var:64</code></p>
<p>Declare a loop to operate on aligned data:</p>
<p><code class="code">!dir$ vector aligned</code></p>
</dd>
</dl>
</div>
<p>Allocation of aligned dynamic memory</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C/C++<br>
</span></dt>
<dd>Use <code class="code">_mm_malloc()</code><br>
and <code class="code">_mm_free()</code> to allocate and free<br>
memory. These take the desired byte-alignment as a<br>
second input argument. When using a aligned<br>
variable <code class="code">var</code>,<br>
use <code class="code">__assume_aligned(var, 64)</code> to inform<br>
the compiler about the alignment<p></p>
</dd>
<dt><span class="term"><br>
Fortran<br>
</span></dt>
<dd>Use <code class="code">-align array64byte</code> compiler<br>
flag to enforce aligned heap memory allocation.<p></p>
</dd>
</dl>
</div>
<p>For a more detailed description of memory alignment on<br>
Xeon Phi,<br>
see <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#reftuning1">[45]</a>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.5.4"></a>12.1.2.&nbsp;SIMD optimization</h4>
</div>
</div>
</div>
<p>Each Xeon Phi core has a 512-bit VPU unit which is capable<br>
of performing 16SP flops or 8 DP flops per clock cycle. VPU<br>
units are also capable of Fused Multiply-Add (FMA) or Fused<br>
Multiply-Subtract (FMS) operations which effectively double the<br>
theoretical floating point performance.</p>
<p>Intel compilers have several directives to aid<br>
vectorization of loops. These are listed in the following in<br>
short. For details, refer to the compiler manuals.</p>
<p>Let the compiler know there are no loop carried<br>
dependencies, but only affect compiler heuristics.</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C/C++<br>
</span></dt>
<dd><code class="code">#pragma IVDEP</code><p></p>
</dd>
<dt><span class="term"><br>
Fortran<br>
</span></dt>
<dd><code class="code">!DIR$ IVDEP</code><p></p>
</dd>
</dl>
</div>
<p>Let the compiler know the loop should be<br>
vectorized, but only affect compiler heuristics.</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C/C++<br>
</span></dt>
<dd><code class="code">#pragma VECTOR</code><p></p>
</dd>
<dt><span class="term"><br>
Fortran<br>
</span></dt>
<dd><code class="code">!DIR$ VECTOR</code><p></p>
</dd>
</dl>
</div>
<p>Force the compiler vectorize a loop, independent of<br>
heuristics.</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C/C++<br>
</span></dt>
<dd><code class="code">#pragma SIMD</code><br>
or <code class="code">#pragma vector always</code><p></p>
</dd>
<dt><span class="term"><br>
Fortran<br>
</span></dt>
<dd><code class="code">!DIR$ SIMD</code><br>
or <code class="code">!DIR$ VECTOR ALWAYS</code><p></p>
</dd>
</dl>
</div>
<p>In order to aid vectorization, Intel compilers can report<br>
details on whether vectorization is successful or not. The<br>
reports can be generated with <code class="code">-vec-reportN</code> compiler<br>
flag, where <code class="code">N</code> denotes the report level. The<br>
vector report levels are:</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
<span class="bold"><strong>Level</strong></span><br>
</span></dt>
<dd><span class="bold"><strong>Description</strong></span><p></p>
</dd>
<dt><span class="term"><br>
N=0<br>
</span></dt>
<dd>No diagnostic information.
</dd>
<dt><span class="term"><br>
N=1<br>
</span></dt>
<dd>Report vectorized loops.
</dd>
<dt><span class="term"><br>
N=2<br>
</span></dt>
<dd>Report vectorized and non-vectorized loops.
</dd>
<dt><span class="term"><br>
N=3<br>
</span></dt>
<dd>Report vectorized and non-vectorized loops with any<br>
proven or assumed data dependencies.<p></p>
</dd>
<dt><span class="term"><br>
N=4<br>
</span></dt>
<dd>Report non-vectorized loops.
</dd>
<dt><span class="term"><br>
N=5<br>
</span></dt>
<dd>Report non-vectorized loops with a reason why they were<br>
not vectorized.<p></p>
</dd>
<dt><span class="term"><br>
N=6<br>
</span></dt>
<dd>Use greater detail on reporting vectorized and<br>
non-vectorized loops with any proven or assumed data<br>
dependencies.<p></p>
</dd>
<dt><span class="term"><br>
N=7<br>
</span></dt>
<dd>Very detailed report, not intended to be human<br>
readable. Python script for gathering information and<br>
annotating the vector report with source<br>
code available from Intel <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#reftuning1b">[46]</a>.<p></p>
</dd>
</dl>
</div>
<p>For a more detailed description of SIMD vectorization with<br>
Intel Xeon Phi,<br>
see <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#reftuning2">[47]</a> and references therein.</p>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="section-advanced-openmp"></a>12.2.&nbsp;OpenMP optimization</h3>
</div>
</div>
</div>
<p>Threading parallelism with Intel Xeon Phi can be readily<br>
exploited with OpenMP. All threading constructs are equivalent<br>
for both offload and native models. We expect the basic concepts<br>
and syntax of OpenMP to be known. For OpenMP, see for instance<br>
“Using OpenMP” By Chapman et<br>
al <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopenmpbook">[5]</a>, the OpenMP<br>
forum <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#refopenmpforum">[13]</a>, and references therein.</p>
<p>The high level of parallelism available on a Xeon Phi<br>
available is very likely to reveal any performance problems<br>
related to threading previously been unnoticed in the code. In<br>
the following, we introduce a few of the most common OpenMP<br>
performance problems and suggest some ways to correct them. We<br>
begin by considering thread to core affinity and thread<br>
placement among the cores. For further details, we refer to<br>
<a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#reftuning4">[48]</a> .</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="section-tuning-openmp-affinity"></a>12.2.1.&nbsp;OpenMP thread affinity</h4>
</div>
</div>
</div>
<p>Each Xeon Phi card contains a shared-memory environment<br>
with approximately 60 physical cores which, in turn, are divided into<br>
4 logical cores each. We refer to this as node topology.</p>
<p>Each memory bank resides closer to some of the cores in<br>
the topology and therefore access to data laying in a memory<br>
bank attached to another socket is generally more<br>
expensive. Such a non-uniform memory access (NUMA) can create<br>
performance issues if threads are allowed to migrate from one<br>
logical core to another during their execution.</p>
<p>In order to extract maximum performance, consider binding<br>
OpenMP threads to logical and physical cores across different<br>
sockets on a single Xeon Phi card. The layout of this binding in<br>
respect to the node topology has performance implications<br>
depending on the computational task and is referred as thread<br>
affinity.</p>
<p>We now briefly show how to set thread affinity using Intel<br>
compilers and OpenMP-library. For a complete description, see<br>
“Thread affinity interface” in the Intel compiler manual.</p>
<p>The thread affinity interface of the Intel runtime library<br>
can be controlled by using the <code class="varname">KMP_AFFINITY</code><br>
environment variable or by using a proprietary Intel API. We now<br>
focus on the former. A standardized method for setting affinity<br>
is available with OpenMP 4.0.</p>
<p><code class="varname">KMP_AFFINITY</code>=[modifier,…]&lt;type&gt;[,&lt;permute&gt;][,&lt;offset&gt;]]
</p><div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
modifier<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=noverbose,<br>
respect, granularity=core<p></p>
<p>granularity=&lt;{fine, thread, core}&gt;, norespect,<br>
noverbose, nowarnings, proclist={&lt;proc-list&gt;},<br>
respect, verbose,<br>
warnings.</p>
</dd>
<dt><span class="term"><br>
type<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=none<p></p>
<p>balanced, compact, disabled, explicit, none,<br>
scatter</p>
</dd>
<dt><span class="term"><br>
permute<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=0.<p></p>
<p>≥ 0, integer. Not valid with type=explicit,none,disabled.</p>
</dd>
<dt><span class="term"><br>
offset<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=0.<p></p>
<p>≥ 0, integer. Not valid with<br>
type=explicit,none,disabled</p>
</dd>
</dl>
</div>
<p>In most cases it is sufficient only to specify the<br>
affinity and granularity. The most important affinity types<br>
supported by Intel Xeon Phi are</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
balanced<br>
</span></dt>
<dd>Thread affinity balanced is a mixture of scatter<br>
and compact affinities. Threads from &lt;1&gt; to<br>
&lt;n<sub>p</sub>&gt; will be spread<br>
across the topology as evenly as evenly as possible in<br>
the granularity context, where<br>
&lt;n<sub>p</sub>&gt; denotes the number<br>
of physical cores. For thread &lt;k&gt; from threads<br>
&lt;n<sub>p</sub>+1&gt; to &lt;n&gt; will<br>
be assigned as close as possible to thread<br>
&lt;k+1&gt;.<p></p>
</dd>
<dt><span class="term"><br>
compact<br>
</span></dt>
<dd>Thread &lt;k+1&gt; will be assigned as close as<br>
possible to thread &lt;k&gt; in the granularity<br>
context according to which the threads are placed.<p></p>
</dd>
<dt><span class="term"><br>
none<br>
</span></dt>
<dd>Threads are not bound to any contexts. Use of<br>
affinity none is not recommended in general.<p></p>
</dd>
<dt><span class="term"><br>
scatter<br>
</span></dt>
<dd>Threads from &lt;1&gt; to &lt;n&gt; will be spread<br>
across the topology as evenly as possibly in the<br>
granularity context according to which the threads are<br>
placed.<p></p>
</dd>
</dl>
</div>
<p>The most important granularity types supported by<br>
Intel Xeon Phi are</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
core<br>
</span></dt>
<dd>Threads are bound to a single core, but allowed to<br>
float within the context of a physical core.<p></p>
</dd>
<dt><span class="term"><br>
fine/thread<br>
</span></dt>
<dd>Threads are bound to a single context, i.e., a<br>
logical core.<p></p>
</dd>
</dl>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.5"></a>12.2.2.&nbsp;Example: Thread affinity</h4>
</div>
</div>
</div>
<p>We now consider the effect of thread affinity to<br>
matrix-matrix multiply. Let <code class="varname">A</code>,<br>
<code class="varname">B</code> and <code class="varname">C=AB</code> be real<br>
matrices of size 4000-by-4000. We implement the data<br>
initialization and multiplication operation in Fortran90<br>
(without blocking) by using jki loop-ordering and OpenMP as<br>
follows</p>
<pre class="programlisting">! Initialize data per thread
!$OMP PARALLEL DO DEFAULT(NONE) &amp;
!$OMP SHARED(A,B,C) &amp;
!$OMP PRIVATE(j)
DO j=1,n
   ! Initialize A
   CALL RANDOM_NUMBER(A(1:n,j))
   ! Initialise B
   CALL RANDOM_NUMBER(B(1:n,j))
   ! Initialise C
   C(1:n,j)=0D0
END DO
!$OMP END PARALLEL DO

! C=A*B
!$OMP PARALLEL DO DEFAULT(NONE) &amp;
!$OMP SHARED(A,B,C) &amp;
!$OMP PRIVATE(i,j,k)
DO j=1,n
  DO k=1,n
    DO i=1,n
      C(i,j)=C(i,j)+A(i,k)*B(k,j)
    END DO
  END DO
END DO
!$OMP END PARALLEL DO
</pre>
<p>Running and timing the matrix-matrix multiplication part on a<br>
single Intel Xeon Phi 7110 card, we have the following<br>
results.</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col>
<col>
<col>
<col></colgroup>
<thead>
<tr>
<th>Threads</th>
<th>balanced, t(s)</th>
<th>compact, t(s)</th>
<th>none, t(s)</th>
<th>scatter, t(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1.43E+00</td>
<td>1.45E+00</td>
<td>1.42E+00</td>
<td>1.58E+00</td>
</tr>
<tr>
<td>30</td>
<td>5.21E-02</td>
<td>9.37E-02</td>
<td>5.65E-02</td>
<td>6.45E-02</td>
</tr>
<tr>
<td>60</td>
<td>2.64E-02</td>
<td>5.00E-02</td>
<td>2.77E-02</td>
<td>3.26E-02</td>
</tr>
<tr>
<td>120</td>
<td>1.86E-02</td>
<td>2.68E-02</td>
<td>1.89E-02</td>
<td>2.18E-02</td>
</tr>
<tr>
<td>240</td>
<td>2.31E-02</td>
<td>2.35E-02</td>
<td>4.66E-02</td>
<td>2.54E-02</td>
</tr>
</tbody>
</table>
</div>
<div class="figure"><a id="id-1.13.6.5.4.1"></a><p></p>
<p class="title"><strong>Figure&nbsp;3.&nbsp;Results of Example: Thread affinity</strong></p>
<div class="figure-contents">
<div class="mediaobject" align="center">
<table style="cellpadding: 0; cellspacing: 0;" border="0" summary="manufactured viewport for HTML img" width="496">
<tbody>
<tr>
<td align="center"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/results_affinity.png" alt="Results of Example: Thread affinity" width="496" align="middle"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>&nbsp;</p>
<p>As seen from the results, changing the thread affinity<br>
has implications to the performance of even a very simple<br>
test case. Using affinity <code class="varname">none</code> is<br>
generally not recommended and is the slowest when all<br>
available threads are being used. Using<br>
affinity <code class="varname">balanced</code> is generally a good<br>
compromise, especially if one wishes to use less than the<br>
number of threads available at maximum.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.6"></a>12.2.3.&nbsp;OpenMP thread placement</h4>
</div>
</div>
</div>
<p>Xeon Phi runtime for OpenMP includes an extension for<br>
placing the threads over the cores on a single card. For a given<br>
number of cores and threads, the user has control where<br>
(relative to the first physical core, i.e., core 0) the OpenMP<br>
threads are placed. In conjunction with offloading from several<br>
MPI processes to a single Xeon Phi card, such control can be<br>
very useful to avoid oversubscription of cores.</p>
<p>The placement of the threads can be controlled with the<br>
environment variable<br>
<code class="varname">KMP_PLACE_THREADS</code>. It specifies the number of<br>
cores to allocate with an optional offset value and number of<br>
threads per core to use. Effectively<br>
<code class="varname">KMP_PLACE_THREADS</code> defines the node topology<br>
for <code class="varname">KMP_AFFINITY</code>.</p>
<p><code class="varname">KMP_PLACE_THREADS</code>=<br>
(<span class="emphasis"><em>int</em></span> [ “C” | “T” ]
[ <span class="emphasis"><em>delim</em></span> ] | <span class="emphasis"><em>delim</em></span>)<br>
[ <span class="emphasis"><em>int</em></span> [ “T” ] [ <span class="emphasis"><em>delim</em></span><br>
] ] [ <span class="emphasis"><em>int</em></span> [ “O” ] ],</p>
<p>where <span class="emphasis"><em>int</em></span> is a simple integer<br>
constant and <span class="emphasis"><em>delim</em></span> is either “,” or<br>
“x”.</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
C<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span> if “C” or<br>
“T” not specified<p></p>
<p>Indicates number of cores</p>
</dd>
<dt><span class="term"><br>
T<br>
</span></dt>
<dd>Indicates number of threads
</dd>
<dt><span class="term"><br>
O<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=0O.<p></p>
<p>Indicates number of cores to offset, starting from<br>
core 0. Offset ignores granularity.</p>
</dd>
</dl>
</div>
<p>As an example, we consider the case<br>
with <code class="varname">OMP_NUM_THREADS</code>=20.</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
<code class="varname">KMP_PLACE_THREADS</code> value<br>
</span></dt>
<dd><span class="bold"><strong>Thread placement</strong></span><p></p>
</dd>
<dt><span class="term"><br>
5C,4T,0O or 5,4,0 or 5C or 5<br>
</span></dt>
<dd>Threads are placed on cores from 0 to 4 with 4<br>
threads per core.<p></p>
</dd>
<dt><span class="term"><br>
5C,4T,10O or 5,4,10 or 5C,10O<br>
</span></dt>
<dd>Threads are placed on cores from 10 to 14 with 4<br>
threads per core.<p></p>
</dd>
<dt><span class="term"><br>
20C,1T,20O or 20,1,20<br>
</span></dt>
<dd>Threads are placed on cores from 20 to 40 with 1<br>
thread per core.<p></p>
</dd>
</dl>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.7"></a>12.2.4.&nbsp;Multiple parallel regions and barriers</h4>
</div>
</div>
</div>
<p>Whenever an OpenMP parallel region is encountered, a<br>
team of threads is formed and launched to execute the<br>
computations. Whenever the parallel region is ended, threads<br>
are joined and the computation proceeds with a single<br>
thread. Between different parallel regions it is up to the<br>
OpenMP implementation to decide whether the threads are shut<br>
down or left in an idle state.</p>
<p>Intel OpenMP -library leaves the threads in a running<br>
state for a predefined amount of time before setting them to<br>
sleep. The time is defined<br>
by <code class="varname">KMP_BLOCKTIME</code><br>
and <code class="varname">KMP_LIBRARY</code> environment<br>
variables. The default is 200ms. For more details, see<br>
Sections “Intel Environment Variables Extensions” and<br>
“Execution modes” in the Intel compiler manual.</p>
<p>Repeatedly forming and disbanding thread-teams and<br>
setting idle threads to sleep has some overhead associated<br>
with it. Another common source of threading overhead in<br>
OpenMP computations are implicit or explicit<br>
barriers. Recall that many OpenMP constructs have an<br>
implicit barrier attached to the end of the construct. Then,<br>
especially if the amount of work done inside an OpenMP<br>
construct is relatively small, thread synchronization with<br>
several threads may be a source of significant overhead. If<br>
the computations are independent, the implicit barrier at<br>
the end of OpenMP constructs can be removed with the<br>
optional NOWAIT parameter.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.8"></a>12.2.5.&nbsp;Example: Multiple parallel regions and barriers</h4>
</div>
</div>
</div>
<p>We now consider the effect of multiple parallel regions<br>
and barriers to performance. Let <code class="varname">v</code> be a<br>
vector with real entries with<br>
size <code class="varname">n</code>=1000000. Let <code class="varname">f(x)</code><br>
denote a function, defined<br>
as <code class="varname">f(x)=x+1</code>.</p>
<p>We implement an OpenMP loop to<br>
apply <code class="varname">f(x)</code><br>
successively <code class="varname">repeats</code>=10000 times to a given<br>
vector <code class="varname">v</code>. We consider three different<br>
implementations. In the first one, OpenMP parallel region is<br>
re-initialized for each successive application<br>
of <code class="varname">f(x)</code>. The second one initializes the<br>
parallel region once, but contains two implicit barriers<br>
from OpenMP constructs. In the third implementation the<br>
parallel region is initialized once and one barrier is used<br>
to synchronize the repetitions.</p>
<p>Implementation 1: parallel region re-initialized<br>
repeatedly.</p>
<pre class="programlisting">DO rep=1,repeats
  !$OMP PARALLEL DO DEFAULT(NONE) NUM_THREADS(threads) &amp;
  !$OMP SHARED(vec1, rep, n) &amp;
  !$OMP PRIVATE(i)
  DO i=1,n
    vec1(i)=vec1(i)+1D0
  END DO
  !$OMP END PARALLEL DO

  ops = ops + 1
END DO
</pre>
<p>Implementation 2: parallel region initialized once,<br>
two implicit barriers from OpenMP constructs.</p>
<pre class="programlisting">!$OMP PARALLEL DEFAULT(NONE) NUM_THREADS(threads) &amp;
!$OMP SHARED(vec1, repeats, ops, n) &amp;
!$OMP PRIVATE(i, rep)
DO rep=1,repeats
  !$OMP DO
  DO i=1,n
    vec1(i)=vec1(i)+1D0
  END DO
  !$OMP END DO
      
  !$OMP SINGLE
  ops = ops + 1
  !$OMP END SINGLE
END DO
!$OMP END PARALLEL
</pre>
<p>Implementation 3: parallel region initialized once,<br>
one explicit barrier from OpenMP construct.</p>
<pre class="programlisting">!$OMP PARALLEL DEFAULT(NONE) NUM_THREADS(threads) &amp;
!$OMP SHARED(vec1, repeats, ops, n) &amp;
!$OMP PRIVATE(i, rep)
DO rep=1,repeats      
  !$OMP DO
  DO i=1,n
    vec1(i)=vec1(i)+1D0
  END DO
  !$OMP END DO NOWAIT
      
  !$OMP SINGLE
  ops = ops + 1
  !$OMP END SINGLE NOWAIT

  ! Synchronize threads once per round      
  !$OMP BARRIER
  END DO
!$OMP END PARALLEL
</pre>
<p>The results on a single Intel Xeon Phi 7110 card<br>
with <code class="varname">KMP_AFFINITY</code>=granularity=fine,balanced<br>
and <code class="varname">KMP_BLOCKTIME</code>=200 are presented in<br>
the following table.</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col>
<col>
<col></colgroup>
<thead>
<tr>
<th>Threads</th>
<th>Implementation 1, t(s)</th>
<th>Implementation 2, t(s)</th>
<th>Implementation 3, t(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2.40E+01</td>
<td>2.37E+01</td>
<td>2.37E+01</td>
</tr>
<tr>
<td>30</td>
<td>7.33E-01</td>
<td>7.18E-01</td>
<td>6.31E-01</td>
</tr>
<tr>
<td>60</td>
<td>4.48E-01</td>
<td>5.53E-01</td>
<td>4.00E-01</td>
</tr>
<tr>
<td>120</td>
<td>4.34E-01</td>
<td>5.04E-01</td>
<td>3.83E-01</td>
</tr>
<tr>
<td>240</td>
<td>4.40E-01</td>
<td>5.23E-01</td>
<td>3.81E-01</td>
</tr>
</tbody>
</table>
</div>
<div class="figure"><a id="id-1.13.6.8.9.1"></a><p></p>
<p class="title"><strong>Figure&nbsp;4.&nbsp;Results of Example: Multiple parallel regions and barriers</strong></p>
<div class="figure-contents">
<div class="mediaobject" align="center">
<table style="cellpadding: 0; cellspacing: 0;" border="0" summary="manufactured viewport for HTML img" width="496">
<tbody>
<tr>
<td align="center"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/results_parallel.png" alt="Results of Example: Multiple parallel regions and barriers" width="496" align="middle"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>&nbsp;</p>
<p>As the number of threads used increases, parallel<br>
threading overhead becomes more apparent. The implementation<br>
with only one barrier is the fastest by a fair<br>
margin. Implementation 1 comes out as the second<br>
fastest. This is due to Implementation 1 having only one<br>
barrier (at the end of the parallel region) versus two in<br>
Implementation 2 (at the end of both of the OpenMP<br>
constructs). With Implementation 1, the parallel region is<br>
re-initialized immediately after it has ended and thus<br>
waiting time for threads is less<br>
than <code class="varname">KMP_BLOCKTIME</code>, i.e., the threads are<br>
not being put to sleep before the next parallel iteration<br>
begins.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.9"></a>12.2.6.&nbsp;False sharing</h4>
</div>
</div>
</div>
<p>On a multiprocessor shared-memory system, each core has<br>
some local cache, which must be kept coherent the among the<br>
cores in the system. Processor cache is organized into<br>
several cache lines, each of which map to some part of the<br>
main memory. On an Intel Xeon Phi, cache line size is 64<br>
bytes. For reference and details, see <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#reftuning5">[50]</a><br>
.</p>
<p>If more than one core accesses the same data in the main<br>
memory, a cache line is shared. Whenever a shared cache line<br>
is updated, to maintain coherency an update is forced to the<br>
caches of all the cores accessing the cache line.</p>
<p>False sharing occurs when several cores access and<br>
update different variables which reside on a single shared<br>
cache line. The resulting updates to maintain cache<br>
coherency may cause a significant performance<br>
degradation. The processors may not be actually sharing any<br>
data, it is sufficient that the data resides on a same cache<br>
line, hence the name false sharing. Due to the ring-bus<br>
architecture of the Xeon Phi, false sharing among the cores can<br>
cause severe performance degredation.</p>
<p>False sharing can be avoided by carefully considering<br>
write access to shared variables. If a variable is updated<br>
often, it may be worthwhile to use a private variable in<br>
stead of a shared one and use reduction at the end of the<br>
work sharing loop.</p>
<p>Given a code with performance problems, false sharing<br>
may be hard to localize. Intel VTune Performance Analyzer<br>
can be used to locate false sharing. For details, we refer<br>
to <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#reftuning4b">[49]</a>.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.10"></a>12.2.7.&nbsp;Example: False sharing</h4>
</div>
</div>
</div>
<p>We now consider a simple example where false sharing<br>
occurs. Let <code class="varname">v</code> be a vector with real<br>
entries and<br>
size <code class="varname">n</code>=1E+08. Let <code class="varname">f(x)</code><br>
denote a function which counts the number of entries<br>
in <code class="varname">v</code> which are smaller than<br>
zero.</p>
<p>We implement <code class="varname">f(x)</code> with OpenMP in<br>
two different ways. In the first implementation, each thread<br>
counts the number of negative entries it has found<br>
in <code class="varname">v</code> to a globally shared array. To<br>
avoid race conditions, each thread uses its own entry in the<br>
shared array, uniquely determined by thread id. When a<br>
thread has finished its portion of vector, a global counter<br>
is atomically incremented. The second implementation is<br>
practically equivalent to the first one, except that each<br>
thread has its own private array for counting the<br>
data.</p>
<p>Implementation 1: False sharing with an array counter.</p>
<pre class="programlisting">!$OMP PARALLEL DEFAULT(NONE) NUM_THREADS(threads) &amp;
!$OMP SHARED(vec, count, counter, n) &amp;
!$OMP PRIVATE(i,TID)

TID=1
!$ TID=omp_get_thread_num()+1
    
!$OMP DO
DO i=1,n
  IF (vec(i)&lt;0) counter(TID)=counter(TID)+1
END DO
!$OMP END DO
 
!OMP ATOMIC
count = counter(TID)+count

!$OMP END PARALLEL
</pre>
<p>Implementation 2: Private array used to avoid false sharing.<br>
<a id="falseimpl2"></a></p>
<pre class="programlisting">!$OMP PARALLEL DEFAULT(NONE) NUM_THREADS(threads) &amp;
!$OMP SHARED(vec, count, n) &amp;
!$OMP PRIVATE(i, counter, TID)

TID=1
!$ TID=omp_get_thread_num()+1
    
!$OMP DO
DO i=1,n
  IF (vec(i)&lt;0) counter(TID)=counter(TID)+1
END DO
!$OMP END DO
 
!OMP ATOMIC
count = counter(TID)+count

!$OMP END PARALLEL
</pre>
<p>We note that a better implementation for this<br>
particular problem will be given in the next section.</p>
<p>The results on a single Intel Xeon Phi 7110 card<br>
with <code class="varname">KMP_AFFINITY</code>=granularity=fine,balanced<br>
are presented in the following table. We note that to obtain<br>
the results, we compiled the test code<br>
with <code class="varname">-O1</code>. On optimization<br>
level <code class="varname">-O2</code> and higher, at least in this<br>
case, the possibility of false sharing with multiple threads was<br>
recognized and corrected by the Intel Fortran<br>
compiler.</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col>
<col></colgroup>
<thead>
<tr>
<th>Threads</th>
<th>Implementation 1, t(s)</th>
<th>Implementation 2, t(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1.94E+00</td>
<td>2.05E+00</td>
</tr>
<tr>
<td>30</td>
<td>3.26E+00</td>
<td>8.30E-02</td>
</tr>
<tr>
<td>60</td>
<td>1.57E+00</td>
<td>4.18E-02</td>
</tr>
<tr>
<td>120</td>
<td>4.13E-01</td>
<td>2.53E-02</td>
</tr>
<tr>
<td>240</td>
<td>1.40E-01</td>
<td>1.34E-02</td>
</tr>
</tbody>
</table>
</div>
<div class="figure"><a id="id-1.13.6.10.9.1"></a><p></p>
<p class="title"><strong>Figure&nbsp;5.&nbsp;Results of Example: False sharing</strong></p>
<div class="figure-contents">
<div class="mediaobject" align="center">
<table style="cellpadding: 0; cellspacing: 0;" border="0" summary="manufactured viewport for HTML img" width="496">
<tbody>
<tr>
<td align="center"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/results_falsesharing.png" alt="Results of Example: False sharing" width="496" align="middle"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>&nbsp;</p>
<p>As expected, Implementation 2 is faster than<br>
Implementation 1, with a difference of an order of<br>
magnitude. Although in this case we had to lower the<br>
optimization level to prevent the compiler from correcting<br>
the situation, we cannot completely rely on the compiler to<br>
detect false sharing, especially if the code to be compiled<br>
is relatively complex.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.11"></a>12.2.8.&nbsp;Memory limitations</h4>
</div>
</div>
</div>
<p>Available memory per core on Xeon Phi is very<br>
limited. When an application is run using all the available<br>
threads, approximately 30Mb of memory is available per<br>
thread assuming none of the data is shared. Excessive memory<br>
allocation per thread is therefore highly discouraged. Care<br>
should be also taken when assigning private variables in<br>
order to avoid unnecessary data duplication among<br>
threads.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.12"></a>12.2.9.&nbsp;Example: Memory limitations</h4>
</div>
</div>
</div>
<p>We now return to the example given in the previous<br>
section. In the example, we prevented threads from doing<br>
false sharing by modifying the definition of the vector<br>
containing the counters. What is important to note is that<br>
in doing so, each thread now implicitly allocates a vector<br>
of length nthreads, i.e., the memory consumption is<br>
quadratic in terms of the number of threads. A better<br>
alternative is to let each thread to store the local result<br>
in a temporary variable and use a reduction to count the<br>
number of elements smaller than zero.</p>
<p>Implementation 3: Temporary variable with reduction used<br>
to store local results.</p>
<pre class="programlisting">count = 0
!$OMP PARALLEL DEFAULT(NONE) NUM_THREADS(threads) &amp;
!$OMP SHARED(vec, n) &amp;
!$OMP PRIVATE(i, Lcount, TID) &amp;
!$OMP REDUCTION(+:count)

TID=1
!$ TID=omp_get_thread_num()+1
    
Lcount = 0
!$OMP DO
DO i=1,n
  IF (vec(i)&lt;0) Lcount=Lcount+1
END DO
!$OMP END DO
    
count = Lcount+count
!$OMP END PARALLEL
</pre>
<p>We have the following results,<br>
where <a class="link" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#falseimpl2">Implementation 2</a><br>
refers to second implementation given in the previous<br>
section. As previously, results have been computed on a single<br>
Intel Xeon Phi 7110 card by using<br>
<code class="varname">KMP_AFFINITY</code>=granularity=fine,balanced and<br>
optimization level <code class="varname">-O1</code>.</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col>
<col></colgroup>
<thead>
<tr>
<th>Threads</th>
<th>Implementation 2, t(s)</th>
<th>Implementation 3, t(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2.04E+00</td>
<td>1.78E+00</td>
</tr>
<tr>
<td>30</td>
<td>6.81E-02</td>
<td>5.93E-02</td>
</tr>
<tr>
<td>60</td>
<td>3.57E-02</td>
<td>2.99E-02</td>
</tr>
<tr>
<td>120</td>
<td>2.73E-02</td>
<td>2.19E-02</td>
</tr>
<tr>
<td>240</td>
<td>1.50E-02</td>
<td>1.13E-02</td>
</tr>
</tbody>
</table>
</div>
<div class="figure"><a id="id-1.13.6.12.6.1"></a><p></p>
<p class="title"><strong>Figure&nbsp;6.&nbsp;Results of Example: Memory limitations</strong></p>
<div class="figure-contents">
<div class="mediaobject" align="center">
<table style="cellpadding: 0; cellspacing: 0;" border="0" summary="manufactured viewport for HTML img" width="496">
<tbody>
<tr>
<td align="center"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/results_memsave.png" alt="Results of Example: Memory limitations" width="496" align="middle"></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>&nbsp;</p>
<p>The main difference between Implementation 2 and 3 is<br>
memory use. This is because Implementation 2 uses a<br>
private array for storing the result (memory usage grows<br>
quadratically with the number of threads), whereas in<br>
Implementation 3 the result is stored a single scalar per<br>
thread. In total 57600 elements have to be stored in<br>
Implementation 2 with 240 threads, whereas just 240<br>
elements suffice in Implementation 3 for the same amount<br>
of threads. We note that if an array of results is to be<br>
computed, one should prefer using an implementation where<br>
the size of the work arrays does grow with the number of<br>
threads used.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.13"></a>12.2.10.&nbsp;Nested parallelism</h4>
</div>
</div>
</div>
<p>Due to the limited amount of memory available, sometimes<br>
using all available threads on an Intel Xeon Phi to<br>
parallelize the outer loop of some computation is not<br>
possible. In some cases this may not be due to inefficient<br>
structure of the code, but because the data needed for<br>
computations per thread is too large. In this case, to take<br>
advantage of all the processing power of the coprocessor, an<br>
option is to use nested OpenMP parallelism.</p>
<p>When nested parallelism is enabled, any inner OpenMP<br>
parallel region which is enclosed within an outer parallel<br>
region will be executed with multiple threads. The<br>
performance impact of using nested parallelism is similar to<br>
performance impact of using multiple parallel regions and<br>
barriers.</p>
<p>Enabling OpenMP nested parallelism is done by setting<br>
environment variable <code class="varname">OMP_NESTED=TRUE</code> or<br>
with an API call to <code class="code">omp_set_nested</code><br>
-function. The number of nested threads within each OpenMP<br>
parallel region is done by setting the environment variable<br>
<code class="varname">OMP_NUM_THREADS</code>=<code class="varname">n_1</code>,<br>
<code class="varname">n_2</code>,<code class="varname">n_3</code>,…,<br>
where <code class="varname">n_j</code> refers to the number of threads<br>
on the <code class="varname">j</code>th level. The number or threads<br>
within each nesting level can be also set with an API call<br>
to <code class="code">omp_set_num_threads</code> -function.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.13.6.14"></a>12.2.11.&nbsp;Example: Nested parallelism</h4>
</div>
</div>
</div>
<p>Consider a case where several independent matrix-matrix<br>
multiplications have to be computed. Let <code class="varname">A</code><br>
be an <code class="varname">n</code>-by-<code class="varname">n</code> matrix and<br>
let matrix <code class="varname">B<sub>k</sub></code> be defined<br>
as <code class="varname">B<sub>k</sub></code><br>
= <code class="varname">A<sup>T</sup>A</code>.</p>
<p>We<br>
let <code class="varname">m</code>=1000, <code class="varname">n</code>=1000<br>
and <code class="varname">k</code>=240 and study the effect of<br>
parallelizing the computation<br>
of <code class="varname">B<sub>k</sub></code>‘s in three<br>
different ways. The first case is to use parallelize the<br>
loop over <code class="varname">k</code> with all available threads. A<br>
second case is to parallelize the computation over<br>
different <code class="varname">k</code>‘s to physical Intel Xeon Phi<br>
cores and use nested parallelism with varying levels of<br>
hardware threads in the computation of the matrix-matrix<br>
multiplications. The third case uses parallelization only<br>
over physical cores. For this, we have the following<br>
implementation.</p>
<p>Implementation: possibly nested parallel loop for<br>
computing <code class="varname">A<sup>T</sup>A</code>.</p>
<pre class="programlisting">!$OMP PARALLEL DO DEFAULT(NONE) NUM_THREADS(nthreads) &amp;
!$OMP SCHEDULE(STATIC) &amp;
!$OMP SHARED(A, B, m, n, k)
DO i=1,k
  CALL DGEMM('T','N', n, n, m, 1D0, A, m, A, m, 0D0, B(1,1,i), n)
END DO
!$OMP END PARALLEL DO
</pre>
<p>The results on a single Intel Xeon Phi 7110 card<br>
with <code class="varname">KMP_AFFINITY</code>=granularity=fine,balanced<br>
are presented in the following table.</p>
<div class="informaltable">
<table border="1">
<colgroup>
<col>
<col>
<col>
<col>
<col>
<col></colgroup>
<thead>
<tr>
<th>Threads</th>
<th>240/1, t(s)</th>
<th>60/4, t(s)</th>
<th>120/1, t(s)</th>
<th>60/2, t(s)</th>
<th>60/1, t(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>time</td>
<td>1.06E+01</td>
<td>4.98E+00</td>
<td>5.64E+00</td>
<td>4.80E+00</td>
<td>4.81E+00</td>
</tr>
</tbody>
</table>
</div>
<p>Intuitively one would expect using 240 threads to be<br>
the most efficient in this case. The results show otherwise,<br>
however. Since the threads are competing for the same cache,<br>
the performance is lowered. Nested parallelism does not<br>
offer significant improvements from just using 60 threads in<br>
a flat fashion. One reason for this might be that with the<br>
affinity policies used, it is difficult to control the thread<br>
placement on the second level of thread parallelism.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="section-tuning-openmp-loadbalancing"></a>12.2.12.&nbsp;OpenMP load balancing</h4>
</div>
</div>
</div>
<p>With any parallel processing, some processes or threads<br>
may require more resources and be more time consuming than<br>
others. In a regular distributed memory program, load<br>
balancing requires programming effort to redistribute parts<br>
of the computation among the processors. In a shared memory<br>
program with a runtime, such as OpenMP, load balancing can<br>
be in some cases automatically handled by the runtime itself<br>
with little overhead.</p>
<p>OpenMP loop constructs support an additional<br>
SCHEDULE-clause. The syntax for this is</p>
<p>SCHEDULE(&lt;kind&gt;[,chunk_size]),</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
kind<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=STATIC.<p></p>
<p>&lt;STATIC,DYNAMIC,GUIDED,RUNTIME&gt;</p>
</dd>
<dt><span class="term"><br>
chunk_size<br>
</span></dt>
<dd><span class="bold"><strong>default</strong></span>=value<br>
depends on the schedule kind.<p></p>
<p>&gt;0, integer.</p>
</dd>
</dl>
</div>
<p>Different schedule kinds supported by OpenMP runtime<br>
on a Xeon Phi are</p>
<div class="variablelist">
<dl class="variablelist">
<dt><span class="term"><br>
STATIC<br>
</span></dt>
<dd>With the static scheduling policy, iteration<br>
indices are divided into chucks of chunk_size and<br>
distributed to threads in a round-robin fashion. If<br>
chuck_size is not defined, iteration indices are<br>
divided into chucks that are roughly equal in<br>
size.<p></p>
</dd>
<dt><span class="term"><br>
DYNAMIC<br>
</span></dt>
<dd>With the dynamic scheduling policy, iteration<br>
indices are assigned to threads in chunks of<br>
chunk_size. Threads request and process new chucks<br>
with assigned iteration indices until the whole index<br>
range has been processed.<p></p>
</dd>
<dt><span class="term"><br>
GUIDED<br>
</span></dt>
<dd>With the guided scheduling policy, iteration<br>
indices are assigned to threads in chunks of size<br>
chunk_size at minimum. In the beginning of the<br>
iteration, the chunk_size actually assigned to be<br>
processed is proportional to the number of unassigned<br>
iteration indices versus the number of available<br>
threads. The assigned chunk_size and can be larger<br>
than the minimum.<p></p>
</dd>
<dt><span class="term"><br>
RUNTIME<br>
</span></dt>
<dd>The scheduling policy and chunk size will be<br>
decided at runtime based on<br>
the <code class="varname">OMP_SCHEDULE</code> environment<br>
variable.<p></p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="section" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title" style="clear: both;"><a id="id-1.14"></a>13.&nbsp; Performance analysis tools</h2>
</div>
</div>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.14.2"></a>13.1.&nbsp;Intel performance analysis tools</h3>
</div>
</div>
</div>
<p>The following Intel performance analysis tools have been enabled for the Intel Xeon Phi coprocessor:</p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc;">
<li class="listitem">Intel trace analyzer and collector (ITAC)</li>
<li class="listitem">Intel VTune Amplifier XE</li>
</ul>
</div>
<p>More information on performance analysis can be found in <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref19">[39]</a> <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref22">[42]</a>. Details will be included in a future version of this guide.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h3 class="title"><a id="id-1.14.3"></a>13.2.&nbsp;Scalasca</h3>
</div>
</div>
</div>
<p>Scalasca <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref24">[44]</a> is a scalable automatic performance analysis toolset designed to profile large scale parallel Fortran, C and C++ applications that use MPI, OpenMP and hybrid MPI+OpenMP parallelization models. It is portable on Intel Xeon Phi architecture in native, symmetric and offload models. Version 2.x uses the Score-P instrumenter and measurment libraries. The following examples are taken from <a class="xref" href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#ref25">[51]</a>.</p>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.14.3.3"></a>13.2.1.&nbsp; Compilation of Scalasca</h4>
</div>
</div>
</div>
<p>On the host Scalasca can be normally compiled, while on the device one must perform a cross-compilation and add -mmic compiler option.</p>
</div>
<div class="section">
<div class="titlepage">
<div>
<div>
<h4 class="title"><a id="id-1.14.3.4"></a>13.2.2.&nbsp;Usage</h4>
</div>
</div>
</div>
<p>Instrumentation of the code to be profiled is done with the command skin.</p>
<pre class="screen">$ skin mpiifort -O -openmp *.f
</pre>
<p>This produces the instrumented executable that will be executed on the host, while</p>
<pre class="screen">$ skin mpiifort -O -openmp -mmic *.f
</pre>
<p>produces an executable for the coprocessor. Further, measurement is than performed with the scan command:</p>
<pre class="screen">$ scan mpiexec -n 2 a.out.cpu       // on the host
% scan mpiexec -n 61 a.out.mic    // on the device
</pre>
<p>It can also be launched on more than one node on the host:</p>
<pre class="screen">$ scan mpiexec.hydra -host node0 -n 1 a.out.cpu : -host node1 -n 1 a.out.cpu
</pre>
<p>and on more than one coprocessor, if available:</p>
<pre class="screen">$ scan mpiexec.hydra -host mic0 -n 30 a.out.mic : -host mic1 -n 31 a.out.mic
</pre>
<p>For symmetric execution one can use:</p>
<pre class="screen">$ scan mpiexec.hydra -host node0 -n 2 a.out.cpu : -host mic0 -n 61 a.out.mic
</pre>
<p>Finally, the collected data can be analyzed with the square command. The scan output would look something like <code class="code">epik_a_2x16_sum</code> for the runs performed on the host and <code class="code">epik_a_mic61x4_sum</code> for those performed on the coprocessor. For data collected on the host and on the device we have:</p>
<pre class="screen">$ square epik_a_2x16_sum
$ square epik_a_61x4_sum
</pre>
<p>respectively. To analyze the data collected in a run from a symmetric execution type:</p>
<pre class="screen">$ square epik_a_2x16+mic61x4_sum.
</pre>
</div>
</div>
</div>
<div class="bibliography" lang="en" xml:lang="en">
<div class="titlepage">
<div>
<div>
<h2 class="title"><a id="id-1.15"></a>Further documentation</h2>
</div>
</div>
</div>
<div class="bibliodiv">
<h3 class="title"><a id="id-1.15.2"></a>Books</h3>
<div class="biblioentry"><a id="refbook1"></a>[1] <span class="title"><em>James Reinders, James Jeffers, Intel Xeon Phi Coprocessor High Performance Programming, Morgan Kaufman Publ Inc, 2013 <a class="link" href="http://lotsofcores.com/" target="_top"> http://lotsofcores.com </a><br>
</em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refbook2"></a>[2] <span class="title"><em>Rezaur Rahman: Intel Xeon Phi Coprocessor Architecture and Tools: The Guide for Application Developers, Apress 2013<br>
</em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refbook3"></a>[3] <span class="title"><em>Parallel Programming and Optimization with Intel Xeon Phi Coprocessors, Colfax 2013 <a class="link" href="http://www.colfax-intl.com/nd/xeonphi/book.aspx" target="_top"> http://www.colfax-intl.com/nd/xeonphi/book.aspx </a><br>
</em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref2"></a>[4] <span class="title"><em>Michael McCool, James Reinders, Arch Robison, Structured Parallel Programming: Patterns for Efficient Computation , Morgan Kaufman Publ Inc, 2013 <a class="link" href="http://parallelbook.com/" target="_top"> http://parallelbook.com </a> </em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopenmpbook"></a>[5] <span class="title"><em>Barbara Chapman, Gabriele Jost and Ruud van der Pas, Using OpenMP, MIT Press Cambridge, 2007, <a class="link" href="http://mitpress.mit.edu/books/using-openmp" target="_top"> http://mitpress.mit.edu/books/using-openmp </a></em>. </span><p></p>
</div>
</div>
<div class="bibliodiv">
<h3 class="title"><a id="id-1.15.3"></a>Forums, Download Sites, Webinars</h3>
<div class="biblioentry"><a id="ref3"></a>[6] <span class="title"><em>Intel Developer Zone: Intel Xeon Phi Coprocessor, <a class="link" href="http://software.intel.com/en-us/mic-developer" target="_top"> http://software.intel.com/en-us/mic-developer </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref4"></a>[7] <span class="title"><em>Intel Many Integrated Core Architecture User Forum, <a class="link" href="http://software.intel.com/en-us/forums/intel-many-integrated-core" target="_top">http://software.intel.com/en-us/forums/intel-many-integrated-core </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref5"></a>[8] <span class="title"><em>Intel Developer Zone: Intel Math Kernel Library, <a class="link" href="http://software.intel.com/en-us/forums/intel-math-kernel-library" target="_top"> http://software.intel.com/en-us/forums/intel-math-kernel-library </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref5a"></a>[9] <span class="title"><em>Intel Math Kernel Library Link Line Advisor, <a class="link" href="http://software.intel.com/sites/products/mkl/" target="_top"> http://software.intel.com/sites/products/mkl/ </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref6"></a>[10] <span class="title"><em>Intel Manycore Platform Software Stack (MPSS), <a class="link" href="http://software.intel.com/en-us/articles/intel-manycore-platform-software-stack-mpss" target="_top">http://software.intel.com/en-us/articles/intel-manycore-platform-software-stack-mpss </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref7"></a>[11] <span class="title"><em>Intel Xeon Processors &amp; Intel Xeon Phi Coprocessors – Introduction to High Performance Applications Development for Multicore and Manycore – Live Webinar, 26.-27.2.2013, recorded <a class="link" href="http://software.intel.com/en-us/articles/intel-xeon-phi-training-m-core" target="_top"> http://software.intel.com/en-us/articles/intel-xeon-phi-training-m-core </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref7a"></a>[12] <span class="title"><em>Intel Cilk Plus Home Page, <a class="link" href="http://cilkplus.org/" target="_top"> http://cilkplus.org/ </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopenmpforum"></a>[13] <span class="title"><em>OpenMP forum, <a class="link" href="http://openmp.org/wp/" target="_top"> http://openmp.org/wp/ </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref7b"></a>[14] <span class="title"><em>Intel Threading Building Blocks Documentation Site, <a class="link" href="http://threadingbuildingblocks.org/documentation" target="_top"> http://threadingbuildingblocks.org/documentation </a></em>. </span><p></p>
</div>
</div>
<div class="bibliodiv">
<h3 class="title"><a id="id-1.15.4"></a>Manuals, Papers</h3>
<div class="biblioentry"><a id="ref8"></a>[15] <span class="title"><em>Intel Xeon Phi Coprocessor Developer’s Quick Start Guide, <a class="link" href="http://software.intel.com/en-us/articles/intel-xeon-phi-coprocessor-developers-quick-start-guide" target="_top">http://software.intel.com/en-us/articles/intel-xeon-phi-coprocessor-developers-quick-start-guide </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refeurora"></a>[16] <span class="title"><em>PRACE-1IP Whitepapers, Evaluations on Intel MIC, <a class="link" href="http://www.prace-ri.eu/Evaluation-Intel-MIC" target="_top">http:/<wbr>/<wbr>www.prace-ri.eu/<wbr>Evaluation-Intel-MIC</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref8a"></a>[17] <span class="title"><em>Intel Xeon Phi Coprocessor (codename Knights Corner), <a class="link" href="http://software.intel.com/en-us/articles/intel-xeon-phi-coprocessor-codename-knights-corner" target="_top"> http://software.intel.com/en-us/articles/intel-xeon-phi-coprocessor-codename-knights-corner </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref9"></a>[18] <span class="title"><em>Intel Xeon Phi Coprocessor System Software Developers Guide, <a class="link" href="https://secure-software.intel.com/sites/default/files/article/334766/intel-xeon-phi-systemsoftwaredevelopersguide.pdf" target="_top"> https://secure-software.intel.com/sites/default/files/article/334766/intel-xeon-phi-systemsoftwaredevelopersguide.pdf </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref9a"></a>[19] <span class="title"><em>System Administration for the Intel Xeon Phi Coprocessor, <a class="link" href="http://software.intel.com/sites/default/files/article/373934/system-administration-for-the-intel-xeon-phi-coprocessor.pdf" target="_top"> http://software.intel.com/sites/default/files/article/373934/system-administration-for-the-intel-xeon-phi-coprocessor.pdf </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref9b"></a>[20] <span class="title"><em>Configuring Intel Xeon Phi coprocessors inside a cluster, <a class="link" href="http://software.intel.com/en-us/articles/configuring-intel-xeon-phi-coprocessors-inside-a-cluster" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>configuring-intel-xeon-phi-coprocessors-inside-a-cluster</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref10"></a>[21] <span class="title"><em>Using the Intel MPI Library on Intel Xeon Phi Coprocessor Systems, <a class="link" href="http://software.intel.com/en-us/articles/using-the-intel-mpi-library-on-intel-xeon-phi-coprocessor-systems" target="_top"> http://software.intel.com/en-us/articles/using-the-intel-mpi-library-on-intel-xeon-phi-coprocessor-systems </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref11"></a>[22] <span class="title"><em>Debugging on Intel Xeon Phi Coprocessor Use Case Overview, <a class="link" href="http://software.intel.com/en-us/articles/debugging-on-intel-xeon-phi-coprocessor-use-case-overview" target="_top"> http://software.intel.com/en-us/articles/debugging-on-intel-xeon-phi-coprocessor-use-case-overview </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref12"></a>[23] <span class="title"><em>Intel Xeon Phi Coprocessor Instruction Set Reference Manual, <a class="link" href="http://software.intel.com/sites/default/files/forum/278102/327364001en.pdf" target="_top">http://software.intel.com/sites/default/files/forum/278102/327364001en.pdf </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref13"></a>[24] <span class="title"><em>An Overview of Programming for Intel Xeon processors and Intel Xeon Phi coprocessors, <a class="link" href="http://software.intel.com/sites/default/files/article/330164/an-overview-of-programming-for-intel-xeon-processors-and-intel-xeon-phi-coprocessors_1.pdf" target="_top"> http://software.intel.com/sites/default/files/article/330164/an-overview-of-programming-for-intel-xeon-processors-and-intel-xeon-phi-coprocessors_1.pdf </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref14"></a>[25] <span class="title"><em>Programming and Compiling for Intel Many Integrated Core Architecture, <a class="link" href="http://software.intel.com/en-us/articles/programming-and-compiling-for-intel-many-integrated-core-architecture" target="_top"> http://software.intel.com/en-us/articles/programming-and-compiling-for-intel-many-integrated-core-architecture </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref14a"></a>[26] <span class="title"><em>Building a Native Application for Intel Xeon Phi Coprocessors, <a class="link" href="http://software.intel.com/en-us/articles/building-a-native-application-for-intel-xeon-phi-coprocessors" target="_top"> http://software.intel.com/en-us/articles/building-a-native-application-for-intel-xeon-phi-coprocessors </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref15"></a>[27] <span class="title"><em>“Using Intel Math Kernel Library on Intel Xeon Phi Coprocessors” section in the MKL User’s Guide, <a class="link" href="http://software.intel.com/sites/products/documentation/doclib/mkl_sa/11/mkl_userguide_lnx/index.htm" target="_top"> http://software.intel.com/sites/products/documentation/doclib/mkl_sa/11/mkl_userguide_lnx/index.htm </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref16"></a>[28] <span class="title"><em>“Support Functions for Intel Many Integrated Core Architecture” section in the MKL Reference Manual, <a class="link" href="http://software.intel.com/sites/products/documentation/doclib/mkl_sa/11/mklman/index.htm" target="_top"> http://software.intel.com/sites/products/documentation/doclib/mkl_sa/11/mklman/index.htm </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref18"></a>[29] <span class="title"><em>Intel Math Kernel Library on the Intel Xeon Phi Coprocessor, <a class="link" href="http://software.intel.com/en-us/articles/intel-mkl-on-the-intel-xeon-phi-coprocessors" target="_top"> http://software.intel.com/en-us/articles/intel-mkl-on-the-intel-xeon-phi-coprocessors </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref17"></a>[30] <span class="title"><em>Intel Compiler 13.0 User Guide and Reference Manual, <a class="link" href="http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/composerxe/compiler/cpp-lin/index.htm" target="_top"> http://software.intel.com/sites/products/documentation/doclib/stdxe/2013/composerxe/compiler/cpp-lin/index.htm </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refipp0"></a>[31] <span class="title"><em>IPP User’s Guide, <a class="link" href="http://software.intel.com/sites/products/documentation/doclib/ipp_sa/71/ipp_userguide_lnx/index.htm" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>sites/<wbr>products/<wbr>documentation/<wbr>doclib/<wbr>ipp_sa/<wbr>71/<wbr>ipp_userguide_lnx/<wbr>index.htm</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refipp1"></a>[32] <span class="title"><em>Intel Software Documentation Library, <a class="link" href="http://software.intel.com/en-us/intel-software-technical-documentation" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>intel-software-technical-documentation</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refipp2"></a>[33] <span class="title"><em>Selecting the Intel Integrated<br>
Performance Primitives (Intel IPP) libraries needed by your application, <a class="link" href="http://software.intel.com/en-us/articles/selecting-the-intelr-ipp-libraries-needed-by-your-application/" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>selecting-the-intelr-ipp-libraries-needed-by-your-application/<wbr></a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopencl0"></a>[34] <span class="title"><em>OpenCL Webpage, <a class="link" href="https://www.khronos.org/opencl/" target="_top">https:/<wbr>/<wbr>www.khronos.org/<wbr>opencl/<wbr></a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopencl1"></a>[35] <span class="title"><em>OpenCL Design and Programming Guide for the Intel® Xeon Phi Coprocessor, <a class="link" href="http://software.intel.com/en-us/articles/opencl-design-and-programming-guide-for-the-intel-xeon-phi-coprocessor" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>opencl-design-and-programming-guide-for-the-intel-xeon-phi-coprocessor</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopencl2"></a>[36] <span class="title"><em>Intel SDK for OpenCL Applications XE 2013, <a class="link" href="http://software.intel.com/sites/products/documentation/ioclsdk/2013XE/OG/index.htm" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>sites/<wbr>products/<wbr>documentation/<wbr>ioclsdk/<wbr>2013XE/<wbr>OG/<wbr>index.htm</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopenacc1"></a>[37] <span class="title"><em>OpenACC Webpage, <a class="link" href="http://www.openacc-standard.org/" target="_top">http:/<wbr>/<wbr>www.openacc-standard.org/<wbr></a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="refopenacc2"></a>[38] <span class="title"><em>CAPS Compilers , <a class="link" href="http://www.caps-entreprise.com/products/caps-compilers/" target="_top">http:/<wbr>/<wbr>www.caps-entreprise.com/<wbr>products/<wbr>caps-compilers/<wbr></a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref19"></a>[39] <span class="title"><em>“Using Intel Trace Analyzer and Collector for Intel Many Integrated Core Architecture” in Intel Cluster Studio 2013 Tutorial, <a class="link" href="http://software.intel.com/sites/products/documentation/hpc/ics/ics2013/ics_tutorial/index.htm#Linux_itac.htm" target="_top"> http://software.intel.com/sites/products/documentation/hpc/ics/ics2013/ics_tutorial/index.htm#Linux_itac.htm </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref20"></a>[40] <span class="title"><em>Advanced Optimizations for Intel MIC Architecture, <a class="link" href="http://software.intel.com/en-us/articles/advanced-optimizations-for-intel-mic-architecture" target="_top"> http://software.intel.com/en-us/articles/advanced-optimizations-for-intel-mic-architecture </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref21"></a>[41] <span class="title"><em>Optimization and Performance Tuning for Intel Xeon Phi Coprocessors – Part 1: Optimization Essentials, <a class="link" href="http://software.intel.com/en-us/articles/optimization-and-performance-tuning-for-intel-xeon-phi-coprocessors-part-1-optimization" target="_top"> http://software.intel.com/en-us/articles/optimization-and-performance-tuning-for-intel-xeon-phi-coprocessors-part-1-optimization </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref22"></a>[42] <span class="title"><em>Optimization and Performance Tuning for Intel Xeon Phi Coprocessors, Part 2: Understanding and Using Hardware Events, <a class="link" href="http://software.intel.com/en-us/articles/optimization-and-performance-tuning-for-intel-xeon-phi-coprocessors-part-2-understanding" target="_top"> http://software.intel.com/en-us/articles/optimization-and-performance-tuning-for-intel-xeon-phi-coprocessors-part-2-understanding </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref23"></a>[43] <span class="title"><em>Requirements for Vectorizable Loops, <a class="link" href="http://software.intel.com/en-us/articles/requirements-for-vectorizable-loops/" target="_top"> http://software.intel.com/en-us/articles/requirements-for-vectorizable-loops/ </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref24"></a>[44] <span class="title"><em>Scalasca , <a class="link" href="http://www.scalasca.org/" target="_top">http://www.scalasca.org/ </a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="reftuning1"></a>[45] <span class="title"><em>Data Alignment to Assist Vectorization, <a class="link" href="http://software.intel.com/en-us/articles/data-alignment-to-assist-vectorization" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>data-alignment-to-assist-vectorization</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="reftuning1b"></a>[46] <span class="title"><em>VecAnalysis Python Script for Annotating Intel C++ and Fortran Compilers Vectorization Reports , <a class="link" href="http://software.intel.com/en-us/articles/vecanalysis-python-script-for-annotating-intelr-compiler-vectorization-report" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>vecanalysis-python-script-for-annotating-intelr-compiler-vectorization-report</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="reftuning2"></a>[47] <span class="title"><em>Vectorization essentials, <a class="link" href="http://software.intel.com/en-us/articles/vectorization-essential" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>vectorization-essential</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="reftuning4"></a>[48] <span class="title"><em>Open MP Thread Affinity Control, <a class="link" href="http://software.intel.com/en-us/articles/openmp-thread-affinity-control" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>openmp-thread-affinity-control</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="reftuning4b"></a>[49] <span class="title"><em>Avoiding and Identifying False Sharing Among Threads, <a class="link" href="http://software.intel.com/en-us/articles/avoiding-and-identifying-false-sharing-among-threads" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>avoiding-and-identifying-false-sharing-among-threads</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="reftuning5"></a>[50] <span class="title"><em>Intel Xeon Phi Core Micro-architecture, <a class="link" href="http://software.intel.com/en-us/articles/intel-xeon-phi-core-micro-architecture" target="_top">http:/<wbr>/<wbr>software.intel.com/<wbr>en-us/<wbr>articles/<wbr>intel-xeon-phi-core-micro-architecture</a></em>. </span><p></p>
</div>
<div class="biblioentry"><a id="ref25"></a>[51] <span class="title"><em>Brian Wylie, Wolfgang Frings: Scalasca support for Intel Xeon Phi, XSEDE13 22-25 July 2013, San Diego, <a class="link" href="https://www.xsede.org/documents/384387/561679/XSEDE13-Wylie.pdf" target="_top">https://www.xsede.org/documents/384387/561679/XSEDE13-Wylie.pdf </a></em>. </span><p></p>
</div>
</div>
</div>
</div>
<ul class="post-attachments"><h3 class="related-documents">Related documents:</h3><li class="post-attachment mime-applicationpdf"><a href="http://www.prace-ri.eu/IMG/pdf/Best-Practice-Guide-Intel-Xeon-Phi.pdf"> Best Practice Guide - Intel Xeon Phi</a><br></li></ul><!-- Simple Share Buttons Adder (6.3.4) simplesharebuttons.com --><div class="ssba ssba-wrap"><div style="text-align:right">Share: <a data-site="linkedin" class="ssba_linkedin_share ssba_share_link" href="http://www.linkedin.com/shareArticle?mini=true&amp;url=http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/linkedin.png" title="LinkedIn" class="ssba ssba-img" alt="Share on LinkedIn"></a><a data-site="" class="ssba_twitter_share" href="http://twitter.com/share?url=http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/&amp;text=Best+Practice+Guide+%E2%80%93+Intel+Xeon+Phi+" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/twitter.png" title="Twitter" class="ssba ssba-img" alt="Tweet about this on Twitter"></a><a data-site="" class="ssba_facebook_share" href="http://www.facebook.com/sharer.php?u=http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/facebook.png" title="Facebook" class="ssba ssba-img" alt="Share on Facebook"></a><a data-site="" class="ssba_google_share" href="https://plus.google.com/share?url=http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/" target="_blank"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/google.png" title="Google+" class="ssba ssba-img" alt="Share on Google+"></a><a data-site="email" class="ssba_email_share" href="mailto:?subject=Best%20Practice%20Guide%20%26%238211;%20Intel%20Xeon%20Phi&amp;body=%20http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/"><img src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/email.png" title="Email" class="ssba ssba-img" alt="Email this to someone"></a></div></div>            <br class="clear">
                    </div>

        <div class="entry-utility entry-meta">
                    </div>

                </article>
            </div>
                        </div>
    </div>
</div>
                <div class="yui-b">
    <nav class="lsidebar" role="navigation" style="min-height: 49751px;">
<ul>
    <li id="nav_menu-2" class="widget_nav_menu widget default" tabindex="-1"><div class="menu-principal-container"><ul id="menu-principal" class="menu ui-accordion ui-widget ui-helper-reset" role="tablist"><li id="menu-item-3131" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3131"><a href="http://www.prace-ri.eu/about-prace-ri" class="ui-accordion-header ui-state-default ui-corner-all ui-accordion-icons" role="tab" id="ui-id-1" aria-controls="ui-id-2" aria-selected="false" aria-expanded="false" tabindex="0"><span class="ui-accordion-header-icon ui-icon ui-icon-triangle-1-e"></span>About PRACE</a>
<ul class="sub-menu ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom" id="ui-id-2" aria-labelledby="ui-id-1" role="tabpanel" aria-hidden="true" style="display: none;">
	<li id="menu-item-3134" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3134"><a href="http://www.prace-ri.eu/prace-in-a-few-words/">PRACE in a few words</a></li>
	<li id="menu-item-3136" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3136"><a href="http://www.prace-ri.eu/organisation/">Organisation</a></li>
	<li id="menu-item-3140" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3140"><a href="http://www.prace-ri.eu/members-info/">Members</a>
	<ul class="sub-menu">
		<li id="menu-item-3143" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3143"><a href="http://www.prace-ri.eu/members/">Members</a></li>
		<li id="menu-item-3144" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3144"><a href="http://www.prace-ri.eu/member-systems/">Updates of Member Systems</a></li>
		<li id="menu-item-3142" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3142" style="border: none;"><a href="http://www.prace-ri.eu/how-to-become-a-member/">How to become a member</a></li>
	</ul>
</li>
	<li id="menu-item-3137" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3137"><a href="http://www.prace-ri.eu/statutes/">Statutes</a></li>
	<li id="menu-item-3139" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3139"><a href="http://www.prace-ri.eu/collaborations/">Collaborations</a></li>
	<li id="menu-item-3138" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3138"><a href="http://www.prace-ri.eu/statistics/">Statistics</a></li>
	<li id="menu-item-16356" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-16356"><a href="http://www.prace-ri.eu/prace-kpi/">PRACE KPI</a></li>
	<li id="menu-item-3135" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3135"><a href="http://www.prace-ri.eu/faq/">FAQ</a></li>
	<li id="menu-item-25706" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-25706" style="border: none;"><a href="http://www.prace-ri.eu/whpc-prace-partnership/">Women in HPC- PRACE Partnership</a></li>
</ul>
</li>
<li id="menu-item-3145" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3145"><a href="http://www.prace-ri.eu/hpc-access" class="ui-accordion-header ui-state-default ui-corner-all ui-accordion-icons" role="tab" id="ui-id-3" aria-controls="ui-id-4" aria-selected="false" aria-expanded="false" tabindex="-1"><span class="ui-accordion-header-icon ui-icon ui-icon-triangle-1-e"></span>HPC Access</a>
<ul class="sub-menu ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom" id="ui-id-4" aria-labelledby="ui-id-3" role="tabpanel" aria-hidden="true" style="display: none;">
	<li id="menu-item-3167" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3167"><a href="http://www.prace-ri.eu/call-announcements/">Call Announcements</a>
	<ul class="sub-menu">
		<li id="menu-item-3168" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3168"><a href="http://www.prace-ri.eu/prace-project-access/">Project Access</a></li>
		<li id="menu-item-3171" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3171" style="border: none;"><a href="http://www.prace-ri.eu/prace-preparatory-access/">Preparatory Access</a></li>
	</ul>
</li>
	<li id="menu-item-3149" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3149"><a href="http://www.prace-ri.eu/how-to-apply/">How to apply</a>
	<ul class="sub-menu">
		<li id="menu-item-3150" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3150"><a href="http://www.prace-ri.eu/application-guide/">Application Guide</a></li>
		<li id="menu-item-3151" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3151"><a href="http://www.prace-ri.eu/application-procedure/">Application Procedure</a></li>
		<li id="menu-item-3152" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3152"><a href="http://www.prace-ri.eu/expressions-of-interest/">Expressions of Interest</a></li>
		<li id="menu-item-3153" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3153" style="border: none;"><a href="http://www.prace-ri.eu/information-for-prace-awardees/">Information for PRACE Awardees</a></li>
	</ul>
</li>
	<li id="menu-item-3172" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3172"><a href="http://www.prace-ri.eu/prace-peer-review/">Peer Review</a>
	<ul class="sub-menu">
		<li id="menu-item-3173" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3173"><a href="http://www.prace-ri.eu/expert-registration/">Expert Registration</a></li>
		<li id="menu-item-3174" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3174"><a href="http://www.prace-ri.eu/peer-review/">Peer Review Process &amp; Principles</a></li>
		<li id="menu-item-3175" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3175"><a href="http://www.prace-ri.eu/resource-allocation/">Resource Allocation</a></li>
		<li id="menu-item-9439" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-9439"><a href="http://www.prace-ri.eu/aup/">PRACE Acceptable Use Policy</a></li>
		<li id="menu-item-3176" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3176"><a href="http://www.prace-ri.eu/confidentiality-and-conflict-of/">Confidentiality and Conflict of Interest Policy</a></li>
		<li id="menu-item-3177" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3177" style="border: none;"><a href="http://www.prace-ri.eu/information-for-reviewers/">Information for Reviewers</a></li>
	</ul>
</li>
	<li id="menu-item-9672" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-9672"><a href="http://www.prace-ri.eu/industry-access">Industry Access</a></li>
	<li id="menu-item-10680" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-10680"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/">SHAPE</a>
	<ul class="sub-menu">
		<li id="menu-item-22921" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-22921"><a href="http://www.prace-ri.eu/shape-white-papers/">SHAPE White Papers</a></li>
		<li id="menu-item-10673" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-10673"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/pilot-call/">SHAPE Pilot Call</a>
		<ul class="sub-menu">
			<li id="menu-item-10682" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-10682"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/pilot-call/pilot-press-releases-announcements/">SHAPE Pilot Call Press Releases &amp; Announcements</a></li>
			<li id="menu-item-10678" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-10678" style="border: none;"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/pilot-call/shape-pilot-highlights/">SHAPE Pilot Highlights</a></li>
		</ul>
</li>
		<li id="menu-item-10672" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-10672"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/second-call/">SHAPE 2nd Call</a>
		<ul class="sub-menu">
			<li id="menu-item-10676" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-10676"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/second-call/second-call-press-releases-announcements/">SHAPE 2nd Call Press Releases &amp; Announcements</a></li>
			<li id="menu-item-10675" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-10675" style="border: none;"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/second-call/second-call-highlights/">SHAPE 2nd Call Highlights</a></li>
		</ul>
</li>
		<li id="menu-item-17745" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-17745"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/third-call/">SHAPE 3rd Call</a>
		<ul class="sub-menu">
			<li id="menu-item-17746" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17746" style="border: none;"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/third-call/third-call-press-releases-announcements/">SHAPE 3rd Call Press Releases &amp; Announcements</a></li>
		</ul>
</li>
		<li id="menu-item-22121" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-22121"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/fourth-call/">SHAPE 4th Call</a>
		<ul class="sub-menu">
			<li id="menu-item-27543" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-27543" style="border: none;"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/fourth-call/fourth-call-press-releases-announcements/">SHAPE 4th Call Press Releases &amp; Announcements</a></li>
		</ul>
</li>
		<li id="menu-item-31880" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-31880"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/fifth-call/">SHAPE 5th Call</a></li>
		<li id="menu-item-10674" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-10674" style="border: none;"><a href="http://www.prace-ri.eu/hpc-access/shape-programme/shape-related-news/">SHAPE-Related News</a></li>
	</ul>
</li>
	<li id="menu-item-3154" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3154"><a href="http://www.prace-ri.eu/prace-awarded-projects/">Awarded Projects</a>
	<ul class="sub-menu">
		<li id="menu-item-3155" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3155"><a href="http://www.prace-ri.eu/hpc-access/prace-awarded-projects/preparatory-access/">Preparatory Access</a></li>
		<li id="menu-item-17629" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-17629"><a href="http://www.prace-ri.eu/projectaccessawarded/">Project Access</a>
		<ul class="sub-menu">
			<li id="menu-item-17631" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17631"><a href="http://www.prace-ri.eu/1st-project-call/">1st Call for Proposals</a></li>
			<li id="menu-item-17632" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17632"><a href="http://www.prace-ri.eu/2nd-project-call/">2nd Call for Proposals</a></li>
			<li id="menu-item-17633" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17633"><a href="http://www.prace-ri.eu/3rd-project-call/">3rd Call for Proposals</a></li>
			<li id="menu-item-17634" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17634"><a href="http://www.prace-ri.eu/4th-project-call/">4th Call for Proposals</a></li>
			<li id="menu-item-17635" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17635"><a href="http://www.prace-ri.eu/5th-project-call/">5th Call for Proposals</a></li>
			<li id="menu-item-17636" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17636"><a href="http://www.prace-ri.eu/6th-project-call/">6th Call for Proposals</a></li>
			<li id="menu-item-17637" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17637"><a href="http://www.prace-ri.eu/hpc-access/prace-awarded-projects/projectaccessawarded/7th-project-call/">7th Call for Proposals</a></li>
			<li id="menu-item-17638" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17638"><a href="http://www.prace-ri.eu/8th-project-call/">8th Call for Proposals</a></li>
			<li id="menu-item-17639" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17639"><a href="http://www.prace-ri.eu/9th-project-call/">9th Call for Proposals</a></li>
			<li id="menu-item-17640" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17640"><a href="http://www.prace-ri.eu/10th-project-call/">10th Call for Proposals</a></li>
			<li id="menu-item-17630" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17630"><a href="http://www.prace-ri.eu/11th-project-call/">11th Call for Proposals</a></li>
			<li id="menu-item-19571" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-19571"><a href="http://www.prace-ri.eu/12th-project-call/">12th Call for Proposals</a></li>
			<li id="menu-item-24096" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-24096"><a target="_blank" href="http://www.prace-ri.eu/13th-project-call/">13th Call for Proposals</a></li>
			<li id="menu-item-32003" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-32003" style="border: none;"><a href="http://www.prace-ri.eu/14th-project-call/">14th Call for Proposals</a></li>
		</ul>
</li>
		<li id="menu-item-3166" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3166" style="border: none;"><a href="http://www.prace-ri.eu/early-access/">Early Access</a>
		<ul class="sub-menu">
			<li id="menu-item-5954" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-5954"><a href="http://www.prace-ri.eu/prace-early-access-call/">Projects</a></li>
			<li id="menu-item-5946" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-5946" style="border: none;"><a href="http://www.prace-ri.eu/prace-early-access-call-publications/">Publications</a></li>
		</ul>
</li>
	</ul>
</li>
	<li id="menu-item-9669" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-9669"><a href="http://www.prace-ri.eu/success-stories">Success stories</a></li>
	<li id="menu-item-3147" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3147"><a href="http://www.prace-ri.eu/prace-prototypes/">Prototypes</a></li>
	<li id="menu-item-3148" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3148" style="border: none;"><a href="http://www.prace-ri.eu/prace-resources/">Resources</a></li>
</ul>
</li>
<li id="menu-item-3179" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3179"><a href="http://www.prace-ri.eu/prace-fp7-projects-and-outcomes/" class="ui-accordion-header ui-state-default ui-corner-all ui-accordion-icons" role="tab" id="ui-id-5" aria-controls="ui-id-6" aria-selected="false" aria-expanded="false" tabindex="-1"><span class="ui-accordion-header-icon ui-icon ui-icon-triangle-1-e"></span>PRACE Projects and Outcomes</a>
<ul class="sub-menu ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom" id="ui-id-6" aria-labelledby="ui-id-5" role="tabpanel" aria-hidden="true" style="display: none;">
	<li id="menu-item-9696" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-9696"><a href="http://www.prace-ri.eu/pcp/">PCP</a></li>
	<li id="menu-item-3189" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3189"><a href="http://www.prace-ri.eu/prace-implementation-phases/">PRACE Implementation Phases</a>
	<ul class="sub-menu">
		<li id="menu-item-28419" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-28419"><a href="http://www.prace-ri.eu/prace-5ip/">PRACE Fifth Implementation Phase (PRACE-5IP) project</a></li>
		<li id="menu-item-15001" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-15001"><a href="http://www.prace-ri.eu/prace-4ip/">PRACE Fourth Implementation Phase (PRACE-4IP) project</a></li>
		<li id="menu-item-3191" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3191"><a href="http://www.prace-ri.eu/prace-3ip/">PRACE Third Implementation Phase (PRACE-3IP) project</a></li>
		<li id="menu-item-3192" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3192"><a href="http://www.prace-ri.eu/prace-2ip/">PRACE Second Implementation Phase (PRACE-2IP) project</a></li>
		<li id="menu-item-3190" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3190"><a href="http://www.prace-ri.eu/prace-1ip/">PRACE First Implementation Phase (PRACE-1IP) project</a></li>
		<li id="menu-item-3193" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3193" style="border: none;"><a href="http://www.prace-ri.eu/prace-pp/">PRACE Preparatory Phase Project (PRACE-PP) project</a></li>
	</ul>
</li>
	<li id="menu-item-3194" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3194"><a href="http://www.prace-ri.eu/deci-projects/">DECI (Tier-1) Access</a>
	<ul class="sub-menu">
		<li id="menu-item-25662" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-25662"><a href="http://www.prace-ri.eu/deci-14-call/">Call for Proposals for DECI-14 (Tier-1)</a></li>
		<li id="menu-item-3195" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3195" style="border: none;"><a href="http://www.prace-ri.eu/awarded-projects/">Awarded projects</a>
		<ul class="sub-menu">
			<li id="menu-item-21486" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-21486"><a href="http://www.prace-ri.eu/deci-4th-call/">DECI 4th Call</a></li>
			<li id="menu-item-21479" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-21479"><a href="http://www.prace-ri.eu/deci-5th-call/">DECI 5th Call</a></li>
			<li id="menu-item-21474" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-21474"><a href="http://www.prace-ri.eu/deci-6th-call/">DECI 6th Call</a></li>
			<li id="menu-item-3197" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3197"><a href="http://www.prace-ri.eu/deci-7th-call/">DECI 7th Call</a></li>
			<li id="menu-item-3198" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3198"><a href="http://www.prace-ri.eu/deci-8th-call/">DECI 8th Call</a></li>
			<li id="menu-item-14434" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-14434"><a href="http://www.prace-ri.eu/deci-9th-call/">DECI 9th Call</a></li>
			<li id="menu-item-19244" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-19244"><a href="http://www.prace-ri.eu/deci-10th-call/">DECI 10th Call</a></li>
			<li id="menu-item-19246" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-19246"><a href="http://www.prace-ri.eu/deci-11th-call/">DECI 11th Call</a></li>
			<li id="menu-item-19245" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-19245" style="border: none;"><a href="http://www.prace-ri.eu/deci-12th-call/">DECI 12th Call</a></li>
		</ul>
</li>
	</ul>
</li>
	<li id="menu-item-3183" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3183"><a href="http://www.prace-ri.eu/public-deliverables/">Public Deliverables</a></li>
	<li id="menu-item-3182" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3182"><a href="http://www.prace-ri.eu/benchmark-suites/">Benchmark Suites</a></li>
	<li id="menu-item-3180" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3180" style="border: none;"><a href="http://www.prace-ri.eu/dttti/">Dare to Think the Impossible</a></li>
</ul>
</li>
<li id="menu-item-3201" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor menu-item-has-children menu-item-3201"><a href="http://www.prace-ri.eu/training-and-documentation/" class="ui-accordion-header ui-state-default ui-corner-all ui-accordion-icons" role="tab" id="ui-id-7" aria-controls="ui-id-8" aria-selected="false" aria-expanded="false" tabindex="-1"><span class="ui-accordion-header-icon ui-icon ui-icon-triangle-1-e"></span>Training and Documentation</a>
<ul class="sub-menu ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom" id="ui-id-8" aria-labelledby="ui-id-7" role="tabpanel" aria-hidden="true" style="display: none;">
	<li id="menu-item-3202" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3202"><a target="_blank" href="http://www.prace-ri.eu/trainings/">Training Portal</a></li>
	<li id="menu-item-19260" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-19260"><a href="http://www.prace-ri.eu/prace-codevault/">PRACE CodeVault</a></li>
	<li id="menu-item-28856" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-28856"><a href="http://www.prace-ri.eu/prace-moocs-via-future-learn/">PRACE MOOCs via Future Learn</a></li>
	<li id="menu-item-3205" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3205"><a href="http://www.prace-ri.eu/prace-advanced-training-centres/">PRACE Advanced Training Centres (PATCs)</a></li>
	<li id="menu-item-3210" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3210"><a href="http://www.prace-ri.eu/prace-training-events/">Training events</a>
	<ul class="sub-menu">
		<li id="menu-item-3211" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3211"><a href="http://www.prace-ri.eu/prace-seasonal-schools/">Seasonal Schools</a>
		<ul class="sub-menu">
			<li id="menu-item-6226" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6226"><a href="http://www.prace-ri.eu/upcoming-prace-seasonal-schools/">Upcoming</a></li>
			<li id="menu-item-6223" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6223" style="border: none;"><a href="http://www.prace-ri.eu/training-and-documentation/prace-training-events/prace-seasonal-schools/prace-seasonal-schools-past/">Past</a></li>
		</ul>
</li>
		<li id="menu-item-3212" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3212"><a href="http://www.prace-ri.eu/patcs-courses/">PATC Courses</a>
		<ul class="sub-menu">
			<li id="menu-item-8659" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-8659"><a href="http://www.prace-ri.eu/upcoming-patc-events/">Upcoming</a></li>
			<li id="menu-item-6222" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6222" style="border: none;"><a href="http://www.prace-ri.eu/patc-courses-past">Past</a></li>
		</ul>
</li>
		<li id="menu-item-3214" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3214" style="border: none;"><a href="http://www.prace-ri.eu/partners-trainings/">Partners Trainings</a></li>
	</ul>
</li>
	<li id="menu-item-3204" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3204"><a target="_blank" href="http://www.prace-ri.eu/summerofhpc/">Summer of HPC</a></li>
	<li id="menu-item-3215" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-3215"><a href="http://www.prace-ri.eu/best-practice-guides/">Best Practice Guides</a></li>
	<li id="menu-item-3216" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3216"><a href="http://www.prace-ri.eu/white-papers/">White Papers</a></li>
	<li id="menu-item-3217" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3217" style="border: none;"><a href="http://www.prace-ri.eu/user-documentation/">User Documentation</a></li>
</ul>
</li>
<li id="menu-item-3218" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3218"><a href="http://www.prace-ri.eu/events/" class="ui-accordion-header ui-state-default ui-corner-all ui-accordion-icons" role="tab" id="ui-id-9" aria-controls="ui-id-10" aria-selected="false" aria-expanded="false" tabindex="-1"><span class="ui-accordion-header-icon ui-icon ui-icon-triangle-1-e"></span>Events</a>
<ul class="sub-menu ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom" id="ui-id-10" aria-labelledby="ui-id-9" role="tabpanel" aria-hidden="true" style="display: none;">
	<li id="menu-item-17515" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-17515"><a href="http://www.prace-ri.eu/pracedays/">PRACEdays</a>
	<ul class="sub-menu">
		<li id="menu-item-25748" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-25748"><a href="http://www.prace-ri.eu/pracedays17">PRACEdays17</a></li>
		<li id="menu-item-17512" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17512"><a href="http://www.prace-ri.eu/pracedays16/">PRACEdays16</a></li>
		<li id="menu-item-17514" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17514"><a href="http://www.prace-ri.eu/pracedays15dublin/">PRACEdays15</a></li>
		<li id="menu-item-17513" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17513" style="border: none;"><a href="http://www.prace-ri.eu/pracedays14barcelona/">PRACEdays14</a></li>
	</ul>
</li>
	<li id="menu-item-20695" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-20695"><a href="http://www.prace-ri.eu/praceatisc/">PRACE@ISC</a></li>
	<li id="menu-item-20696" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-20696"><a href="http://www.prace-ri.eu/praceatsc/">PRACE@SC</a></li>
	<li id="menu-item-3219" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3219"><a href="http://www.prace-ri.eu/conferences-and-meetings/">Conferences and meetings</a></li>
	<li id="menu-item-3220" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3220"><a href="http://www.prace-ri.eu/prace-industrial-seminars">Industrial Seminars</a>
	<ul class="sub-menu">
		<li id="menu-item-3265" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3265"><a href="http://www.prace-ri.eu/prace-industrial-seminar-2013/">PRACE Industrial Seminar 2013</a></li>
		<li id="menu-item-3266" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3266"><a href="http://www.prace-ri.eu/prace-industrial-seminar-2012/">PRACE Industrial Seminar 2012</a></li>
		<li id="menu-item-3267" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3267"><a href="http://www.prace-ri.eu/prace-industrial-seminar-2011/">PRACE Industrial Seminar 2011</a></li>
		<li id="menu-item-3268" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3268"><a href="http://www.prace-ri.eu/prace-industrial-seminar-2009/">PRACE Industrial Seminar 2009</a></li>
		<li id="menu-item-3269" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3269" style="border: none;"><a href="http://www.prace-ri.eu/prace-industrial-seminar-2008/">PRACE Industrial Seminar 2008</a></li>
	</ul>
</li>
	<li id="menu-item-3270" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3270"><a href="http://www.prace-ri.eu/prace-outreach-events/">PRACE Outreach Events</a></li>
	<li id="menu-item-3271" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3271" style="border: none;"><a href="http://www.prace-ri.eu/hpc-related-events">HPC Related Events</a></li>
</ul>
</li>
<li id="menu-item-3273" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3273" style="border: none;"><a href="http://www.prace-ri.eu/media/" class="ui-accordion-header ui-state-default ui-corner-all ui-accordion-icons" role="tab" id="ui-id-11" aria-controls="ui-id-12" aria-selected="false" aria-expanded="false" tabindex="-1"><span class="ui-accordion-header-icon ui-icon ui-icon-triangle-1-e"></span>Media</a>
<ul class="sub-menu ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom" id="ui-id-12" aria-labelledby="ui-id-11" role="tabpanel" aria-hidden="true" style="display: none;">
	<li id="menu-item-3276" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3276"><a href="http://www.prace-ri.eu/news/">News</a>
	<ul class="sub-menu">
		<li id="menu-item-3277" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3277"><a href="http://www.prace-ri.eu/media/news/press-releases/">Press Releases</a></li>
		<li id="menu-item-3280" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3280"><a href="http://www.prace-ri.eu/newsletters/">Newsletters</a></li>
		<li id="menu-item-3281" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-3281" style="border: none;"><a href="http://www.prace-ri.eu/announcements/">Announcements</a></li>
	</ul>
</li>
	<li id="menu-item-3309" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-3309"><a href="http://www.prace-ri.eu/publications/">Publications</a>
	<ul class="sub-menu">
		<li id="menu-item-20828" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-20828"><a href="http://www.prace-ri.eu/praceannualreports/">PRACE Annual Reports</a></li>
		<li id="menu-item-3317" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3317"><a href="http://www.prace-ri.eu/sci-ar-2012/">Scientific Annual Report 2012</a></li>
		<li id="menu-item-3313" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3313"><a href="http://www.prace-ri.eu/prace-digest/">PRACE Digest</a></li>
		<li id="menu-item-3318" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3318"><a href="http://www.prace-ri.eu/prace-the-scientific-case-for-hpc/">Scientific Case for HPC in Europe 2012 – 2020</a></li>
		<li id="menu-item-17263" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-17263"><a href="http://www.prace-ri.eu/womeninhpc/">PRACE Women in HPC Magazine 2015</a></li>
		<li id="menu-item-3319" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3319"><a href="http://www.prace-ri.eu/special-report/">Special Report</a></li>
		<li id="menu-item-13893" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-13893" style="border: none;"><a href="http://www.prace-ri.eu/media/publications/partner-publications/">Partner Publications</a></li>
	</ul>
</li>
	<li id="menu-item-18545" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18545"><a href="http://www.prace-ri.eu/media/factsheets/">PRACE Fact Sheets</a></li>
	<li id="menu-item-12733" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-12733"><a href="http://www.prace-ri.eu/media/videos/">Videos</a>
	<ul class="sub-menu">
		<li id="menu-item-12734" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-12734"><a href="http://www.prace-ri.eu/?p=12718">Videos about PRACE</a></li>
		<li id="menu-item-12736" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-12736"><a href="http://www.prace-ri.eu/hpc-matters-sc14/">HPC Matters-SC’14</a></li>
		<li id="menu-item-12735" class="ppr-rewrite menu-item menu-item-type-post_type menu-item-object-post menu-item-12735" style="border: none;"><a href="http://www.prace-ri.eu/isc13-videos/">ISC’13 Videos</a></li>
	</ul>
</li>
	<li id="menu-item-3274" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-3274" style="border: none;"><a href="http://www.prace-ri.eu/press-materials">Press Materials</a></li>
</ul>
</li>
</ul></div></li></ul>
</nav>
    </div>
        </div>
    <footer id="ft" class="clear" role="contentinfo">
<!--footer-widget start-->
<div class="widget-wrapper clearfix">
        <br class="clear">
</div>
<!--footer-widget end-->
<address>
    <small>©PRACE 2017 &nbsp;* <a href="http://www.prace-ri.eu/feed/" class="entry-rss">Entries RSS</a> *&nbsp;</small>&nbsp;<small><a href="http://www.tenman.info/wp3/raindrops/">Child theme of Raindrops Theme</a></small>&nbsp;&nbsp;<small>*&nbsp;<a href="http://www.prace-ri.eu/data-protection-policy/">Legal Notice</a></small>&nbsp;&nbsp;</address>
    </footer>
    </div>
    <script type="text/javascript">
var colomatduration = 'fast';
var colomatslideEffect = 'slideFade';
</script><script type="text/javascript">
                    (function() {

                        jQuery(function() {
                                            jQuery('body').addClass('');
                

                                            var raindrops_ignore_template = false;
                                            if (raindrops_ignore_template == false) {
                                var raindrops_main_sidebar_height = jQuery('.lsidebar').height();
                                var raindrops_extra_sidebar_height = jQuery('.rsidebar').height();
                                var raindrops_container_height = jQuery('#container').height();
                                var raindrops_sticky_widget_height = jQuery('.topsidebar').height();

                                if (raindrops_main_sidebar_height > raindrops_container_height) {

                                    jQuery('#container').css({'min-height': raindrops_main_sidebar_height + 'px'});
                                    jQuery('.rsidebar').css({'min-height': raindrops_main_sidebar_height + 'px'});
                                } else {

                                    if (raindrops_sticky_widget_height > 0) {

                                        raindrops_left_sidebar_height = raindrops_container_height + raindrops_sticky_widget_height + 13;
                                        jQuery('.lsidebar').css({'min-height': raindrops_left_sidebar_height + 'px'});
                                    } else {

                                        jQuery('.lsidebar').css({'min-height': raindrops_container_height + 'px'});
                                    }
                                    jQuery('.rsidebar').css({'min-height': raindrops_container_height + 'px'});
                                }
                            }

                            if (navigator.userLanguage) {

                                baseLang = navigator.userLanguage.substring(0, 2).toLowerCase();
                            } else {

                                baseLang = navigator.language.substring(0, 2).toLowerCase();
                            }

                            jQuery('body').addClass('accept-lang-' + baseLang);


                            var userAgent = window.navigator.userAgent.toLowerCase();

                            if (userAgent.match(/msie/i)) {

                                var ie_num = userAgent.match(/MSIE (\d+\.\d+);/i);
                                var ieversion = parseInt(ie_num[1], 10);
                                jQuery('body').addClass('ie' + ieversion);
                            } else if (userAgent.indexOf('opera') != -1) {

                                jQuery('body').addClass('opera');
                            } else if (userAgent.indexOf('chrome') != -1) {

                                jQuery('body').addClass('chrome');
                            } else if (userAgent.indexOf('safari') != -1) {

                                jQuery('body').addClass('safari');
                            } else if (userAgent.indexOf('gecko') != -1) {

                                var match = userAgent.match(/(trident)(?:.*rv:([\w.]+))?/);
				if (match == null) 
                                  jQuery('body').addClass('gecko');
				else {
                                var version = parseInt(match[2], 10);

                                if (version == 11) {
                                    jQuery('body').addClass('ie11');
                                } else {
                                    jQuery('body').addClass('gecko');
                                }
                                }
                            } else if (userAgent.indexOf('iphone') != -1) {

                                jQuery('body').addClass('iphone');
                            } else if (userAgent.indexOf('Netscape') != -1) {

                                jQuery('body').addClass('netscape');
                            } else {

                                jQuery('body').addClass('unknown');
                            }

                        });
                    })(jQuery);
                </script>
                <script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/core.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/widget.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/accordion.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-blind.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-bounce.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-clip.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-drop.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-explode.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-fade.min.js"></script><div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div><iframe name="fb_xdm_frame_http" frameborder="0" allowtransparency="true" allowfullscreen="true" scrolling="no" id="fb_xdm_frame_http" aria-hidden="true" title="Facebook Cross Domain Communication Frame" tabindex="-1" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/0F7S7QWJ0Ac.html" style="border: none;"></iframe><iframe name="fb_xdm_frame_https" frameborder="0" allowtransparency="true" allowfullscreen="true" scrolling="no" id="fb_xdm_frame_https" aria-hidden="true" title="Facebook Cross Domain Communication Frame" tabindex="-1" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/0F7S7QWJ0Ac(1).html" style="border: none;"></iframe></div></div><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div></div></div></div>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-fold.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-highlight.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-pulsate.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-size.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-scale.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-shake.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-slide.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/effect-transfer.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/ssba.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/wp-embed.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/jquery.fancybox-1.3.8.min.js"></script>
<script type="text/javascript" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/jquery.mousewheel.min.js"></script>
<script type="text/javascript">
jQuery(document).on('ready post-load', function(){ jQuery('.nofancybox,a.pin-it-button,a[href*="pinterest.com/pin/create/button"]').addClass('nolightbox'); });
jQuery(document).on('ready post-load',easy_fancybox_handler);
jQuery(document).on('ready',easy_fancybox_auto);</script>

			<div id="cookie-notice" role="banner" class="cn-bottom bootstrap" style="color: rgb(0, 0, 0); background-color: rgb(30, 115, 190); display: block;"><div class="cookie-notice-container"><span id="cn-notice-text">We use cookies to ensure that we give you the best experience on our website. If you continue to use this site we will assume that you accept this behavior.</span><a href="http://www.prace-ri.eu/best-practice-guide-intel-xeon-phi-html/#" id="cn-accept-cookie" data-cookie-set="accept" class="cn-set-cookie button bootstrap">OK</a>
				</div>
			</div>
<div id="fancybox-tmp"></div><div id="fancybox-loading"><div></div></div><div id="fancybox-overlay"></div><div id="fancybox-wrap"><div id="fancybox-outer"><div class="fancybox-bg" id="fancybox-bg-n"></div><div class="fancybox-bg" id="fancybox-bg-ne"></div><div class="fancybox-bg" id="fancybox-bg-e"></div><div class="fancybox-bg" id="fancybox-bg-se"></div><div class="fancybox-bg" id="fancybox-bg-s"></div><div class="fancybox-bg" id="fancybox-bg-sw"></div><div class="fancybox-bg" id="fancybox-bg-w"></div><div class="fancybox-bg" id="fancybox-bg-nw"></div><div id="fancybox-content"></div><a id="fancybox-close"></a><div id="fancybox-title"></div><a href="javascript:;" id="fancybox-left"><span class="fancy-ico" id="fancybox-left-ico"></span></a><a href="javascript:;" id="fancybox-right"><span class="fancy-ico" id="fancybox-right-ico"></span></a></div></div><iframe frameborder="0" scrolling="no" style="background-color: transparent; border: 0px; display: none;" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/saved_resource.html"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="zh-t-i0-pinyin" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:true,&quot;ss&quot;:true}" style="display: none;"></div><iframe frameborder="0" scrolling="no" style="background-color: transparent; border: 0px; box-sizing: content-box; width: 152px; height: 29px; box-shadow: rgba(0, 0, 0, 0.2) 0px 2px 4px 0px; z-index: 2147483643; margin: 0px; position: fixed; left: 1498px; top: 866px;" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/saved_resource(1).html"></iframe><iframe frameborder="0" scrolling="no" style="background-color: transparent; border: 0px; box-sizing: content-box; width: 66px; height: 66px; box-shadow: rgba(0, 0, 0, 0.2) 0px 4px 16px 0px; z-index: 2147483644; margin: 0px; position: absolute; display: none;" src="./Best Practice Guide - Intel Xeon Phi - PRACE Research Infrastructure_files/saved_resource(2).html"></iframe><div class="ita-isv" style="display: none;"></div></body></html>