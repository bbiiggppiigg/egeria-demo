1	Performance optimization revolves around three basic strategies: Maximize parallel execution to achieve maximum utilization; Optimize memory usage to achieve maximum memory throughput; Optimize instruction usage to achieve maximum instruction throughput.
1	Which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion; optimizing instruction usage of a kernel that is mostly limited by memory accesses will not yield any significant performance gain, for example.
1	Optimization efforts should therefore be constantly directed by measuring and monitoring the performance limiters, for example using the CUDA profiler.
1	Also, comparing the floating-point operation throughput or memory throughput - whichever makes more sense - of a particular kernel to the corresponding peak theoretical throughput of the device indicates how much room for improvement there is for the kernel.
1	To maximize utilization the application should be structured in a way that it exposes as much parallelism as possible and efficiently maps this parallelism to the various components of the system to keep them busy most of the time.
1	At a high level, the application should maximize parallel execution between the host, the devices, and the bus connecting the host to the devices, by using asynchronous functions calls and streams as described in Asynchronous Concurrent Execution.
1	It should assign to each processor the type of work it does best: serial workloads to the host; parallel workloads to the devices.
1	For the parallel workloads, at points in the algorithm where parallelism is broken because some threads need to synchronize in order to share data with each other, there are two cases: Either these threads belong to the same block, in which case they should use __syncthreads() and share data through shared memory within the same kernel invocation, or they belong to different blocks, in which case they must share data through global memory using two separate kernel invocations, one for writing to and one for reading from global memory.
0	The second case is much less optimal since it adds the overhead of extra kernel invocations and global memory traffic.
1	Its occurrence should therefore be minimized by mapping the algorithm to the CUDA programming model in such a way that the computations that require inter-thread communication are performed within a single thread block as much as possible.
1	At a lower level, the application should maximize parallel execution between the multiprocessors of a device.
1	Multiple kernels can execute concurrently on a device, so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently as described in Asynchronous Concurrent Execution.
1	At an even lower level, the application should maximize parallel execution between the various functional units within a multiprocessor.
0	As described in Hardware Multithreading, a GPU multiprocessor relies on thread-level parallelism to maximize utilization of its functional units.
0	Utilization is therefore directly linked to the number of resident warps.
0	At every instruction issue time, a warp scheduler selects a warp that is ready to execute its next instruction, if any, and issues the instruction to the active threads of the warp.
1	The number of clock cycles it takes for a warp to be ready to execute its next instruction is called the latency, and full utilization is achieved when all warp schedulers always have some instruction to issue for some warp at every clock cycle during that latency period, or in other words, when latency is completely "hidden".
1	The number of instructions required to hide a latency of L clock cycles depends on the respective throughputs of these instructions (see Arithmetic Instructions for the throughputs of various arithmetic instructions); assuming maximum throughput for all instructions, it is: L for devices of compute capability 2.0 since a multiprocessor issues one instruction per warp over two clock cycles for two warps at a time, as mentioned in Compute Capability 2.x, 2L for devices of compute capability 2.1 since a multiprocessor issues a pair of instructions per warp over two clock cycles for two warps at a time, as mentioned in Compute Capability 2.x, 8L for devices of compute capability 3.x since a multiprocessor issues a pair of instructions per warp over one clock cycle for four warps at a time, as mentioned in Compute Capability 3.x.
0	For devices of compute capability 2.0, the two instructions issued every other cycle are for two different warps.
0	For devices of compute capability 2.1, the four instructions issued every other cycle are two pairs for two different warps, each pair being for the same warp.
0	For devices of compute capability 3.x, the eight instructions issued every cycle are four pairs for four different warps, each pair being for the same warp.
0	The most common reason a warp is not ready to execute its next instruction is that the instruction's input operands are not available yet.
0	If all input operands are registers, latency is caused by register dependencies, i.e., some of the input operands are written by some previous instruction(s) whose execution has not completed yet.
0	In the case of a back-to-back register dependency (i.e., some input operand is written by the previous instruction), the latency is equal to the execution time of the previous instruction and the warp schedulers must schedule instructions for different warps during that time.
0	Execution time varies depending on the instruction, but it is typically about 22 clock cycles for devices of compute capability 2.x and about 11 clock cycles for devices of compute capability 3.x, which translates to 22 warps for devices of compute capability 2.x and 44 warps for devices of compute capability 3.x and higher (still assuming that warps execute instructions with maximum throughput, otherwise fewer warps are needed).
0	For devices of compute capability 2.1 and higher, this is also assuming enough instruction-level parallelism so that schedulers are always able to issue pairs of instructions for each warp.
0	If some input operand resides in off-chip memory, the latency is much higher: 400 to 800 clock cycles for devices of compute capability 2.x and about 200 to 400 clock cycles for devices of compute capability 3.x.
1	The number of warps required to keep the warp schedulers busy during such high latency periods depends on the kernel code and its degree of instruction-level parallelism.
0	In general, more warps are required if the ratio of the number of instructions with no off-chip memory operands (i.e., arithmetic instructions most of the time) to the number of instructions with off-chip memory operands is low (this ratio is commonly called the arithmetic intensity of the program).
0	For example, assume this ratio is 30, also assume the latencies are 600 cycles on devices of compute capability 2.x and 300 cycles on devices of compute capability 3.x.
0	Then about 20 warps are required for devices of compute capability 2.x and about 40 for devices of compute capability 3.x (with the same assumptions as in the previous paragraph).
0	Another reason a warp is not ready to execute its next instruction is that it is waiting at some memory fence (Memory Fence Functions) or synchronization point (Memory Fence Functions).
0	A synchronization point can force the multiprocessor to idle as more and more warps wait for other warps in the same block to complete execution of instructions prior to the synchronization point.
1	Having multiple resident blocks per multiprocessor can help reduce idling in this case, as warps from different blocks do not need to wait for each other at synchronization points.
0	The number of blocks and warps residing on each multiprocessor for a given kernel call depends on the execution configuration of the call (Execution Configuration), the memory resources of the multiprocessor, and the resource requirements of the kernel as described in Hardware Multithreading.
0	Register and shared memory usage are reported by the compiler when compiling with the -ptxas-options=-v option.
0	The total amount of shared memory required for a block is equal to the sum of the amount of statically allocated shared memory and the amount of dynamically allocated shared memory.
0	The number of registers used by a kernel can have a significant impact on the number of resident warps.
0	For example, for devices of compute capability 2.x, if a kernel uses 32 registers and each block has 512 threads and requires very little shared memory, then two blocks (i.e., 32 warps) can reside on the multiprocessor since they require 2x512x32 registers, which exactly matches the number of registers available on the multiprocessor.
0	But as soon as the kernel uses one more register, only one block (i.e., 16 warps) can be resident since two blocks would require 2x512x17 registers, which are more registers than are available on the multiprocessor.
0	Therefore, the compiler attempts to minimize register usage while keeping register spilling (see Device Memory Accesses) and the number of instructions to a minimum.
0	Register usage can be controlled using the maxrregcount compiler option or launch bounds as described in Launch Bounds.
0	Each double variable and each long long variable uses two registers.
0	The effect of execution configuration on performance for a given kernel call generally depends on the kernel code.
1	Experimentation is therefore recommended.
1	Applications can also parameterize execution configurations based on register file size and shared memory size, which depends on the compute capability of the device, as well as on the number of multiprocessors and memory bandwidth of the device, all of which can be queried using the runtime (see reference manual).
1	The number of threads per block should be chosen as a multiple of the warp size to avoid wasting computing resources with under-populated warps as much as possible.
0	Several API functions exist to assist programmers in choosing thread block size based on register and shared memory requirements.
0	The occupancy calculator API, cudaOccupancyMaxActiveBlocksPerMultiprocessor, can provide an occupancy prediction based on the block size and shared memory usage of a kernel.
0	This function reports occupancy in terms of the number of concurrent thread blocks per multiprocessor.
0	Note that this value can be converted to other metrics.
0	Multiplying by the number of warps per block yields the number of concurrent warps per multiprocessor; further dividing concurrent warps by max warps per multiprocessor gives the occupancy as a percentage.
0	The occupancy-based launch configurator APIs, cudaOccupancyMaxPotentialBlockSize and cudaOccupancyMaxPotentialBlockSizeVariableSMem, heuristically calculate an execution configuration that achieves the maximum multiprocessor-level occupancy.
0	The following code sample calculates the occupancy of MyKernel.
0	It then reports the occupancy level with the ratio between concurrent warps versus maximum warps per multiprocessor.
0	The following code sample configures an occupancy-based kernel launch of MyKernel according to the user input.The CUDA Toolkit also provides a self-documenting, standalone occupancy calculator and launch configurator implementation in <CUDA_Toolkit_Path>/include/cuda_occupancy.h for any use cases that cannot depend on the CUDA software stack.
0	A spreadsheet version of the occupancy calculator is also provided.
0	The spreadsheet version is particularly useful as a learning tool that visualizes the impact of changes to the parameters that affect occupancy (block size, registers per thread, and shared memory per thread).
1	The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth.
1	That means minimizing data transfers between the host and the device, as detailed in Data Transfer between Host and Device, since these have much lower bandwidth than data transfers between global memory and the device.
1	That also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i.e., L1 cache available on devices of compute capability 2.x and 3.x, L2 cache available on devices of compute capability 2.x and higher, texture cache and constant cache available on all devices).
0	Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it.
0	As illustrated in CUDA C Runtime, a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each thread of a block: Load data from device memory to shared memory, Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were populated by different threads, Process the data in shared memory, Synchronize again if necessary to make sure that shared memory has been updated with the results, Write the results back to device memory.
1	For some applications (e.g., for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality.
0	As mentioned in Compute Capability 2.x and Compute Capability 3.x, for devices of compute capability 2.x and 3.x, the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus shared memory is configurable for each kernel call.
0	The throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory.
1	The next step in maximizing memory throughput is therefore to organize memory accesses as optimally as possible based on the optimal memory access patterns described in Device Memory Accesses.
1	This optimization is especially important for global memory accesses as global memory bandwidth is low, so non-optimal global memory accesses have a higher impact on performance.
1	Applications should strive to minimize data transfer between the host and the device.
1	One way to accomplish this is to move more code from the host to the device, even if that means running kernels with low parallelism computations.
1	Intermediate data structures may be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.
1	Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately.
1	On systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked host memory as described in Page-Locked Host Memory.
0	In addition, when using mapped page-locked memory (Mapped Memory), there is no need to allocate any device memory and explicitly copy data between device and host memory.
0	Data transfers are implicitly performed each time the kernel accesses the mapped memory.
1	For maximum performance, these memory accesses must be coalesced as with accesses to global memory (see Device Memory Accesses).
1	Assuming that they are and that the mapped memory is read or written only once, using mapped page-locked memory instead of explicit copies between device and host memory can be a win for performance.
1	On integrated systems where device memory and host memory are physically the same, any copy between host and device memory is superfluous and mapped page-locked memory should be used instead.
0	Applications may query a device is integrated by checking that the integrated device property (see Device Enumeration) is equal to 1.
0	An instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture memory) might need to be re-issued multiple times depending on the distribution of the memory addresses across the threads within the warp.
0	How the distribution affects the instruction throughput this way is specific to each type of memory and described in the following sections.
1	For example, for global memory, as a general rule, the more scattered the addresses are, the more reduced the throughput is.
0	Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions.
0	These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions.
0	When a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads.
1	In general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly.
0	For example, if a 32-byte memory transaction is generated for each thread's 4-byte access, throughput is divided by 8.
0	How many transactions are necessary and how much throughput is ultimately affected varies with the compute capability of the device.
0	Compute Capability 2.x, Compute Capability 3.x, and Compute Capability 5.x give more details on how global memory accesses are handled for various compute capabilities.
1	To maximize global memory throughput, it is therefore important to maximize coalescing by: Following the most optimal access patterns based on Compute Capability 2.x and Compute Capability 3.x, Using data types that meet the size and alignment requirement detailed in Device Memory Accesses, Padding data in some cases, for example, when accessing a two-dimensional array as described in Device Memory Accesses.
0	Global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes.
0	Any access (via a variable or a pointer) to data residing in global memory compiles to a single global memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is naturally aligned (i.e., its address is a multiple of that size).
0	If this size and alignment requirement is not fulfilled, the access compiles to multiple instructions with interleaved access patterns that prevent these instructions from fully coalescing.
1	It is therefore recommended to use types that meet this requirement for data that resides in global memory.
0	The alignment requirement is automatically fulfilled for the built-in types of char, short, int, long, longlong, float, double like float2 or float4.
0	For structures, the size and alignment requirements can be enforced by the compiler using the alignment specifiers __align__(8) or __align__(16), such as orAny address of a variable residing in global memory or returned by one of the memory allocation routines from the driver or runtime API is always aligned to at least 256 bytes.
1	Reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words), so special care must be taken to maintain alignment of the starting address of any value or array of values of these types.
0	A typical case where this might be easily overlooked is when using some custom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls to cudaMalloc() or cuMemAlloc()) is replaced by the allocation of a single large block of memory partitioned into multiple arrays, in which case the starting address of each array is offset from the block's starting address.
0	A common global memory access pattern is when each thread of index (tx,ty) uses the following address to access one element of a 2D array of width width, located at address BaseAddress of type type* (where type meets the requirement described in Maximize Utilization): For these accesses to be fully coalesced, both the width of the thread block and the width of the array must be a multiple of the warp size.
0	In particular, this means that an array whose width is not a multiple of this size will be accessed much more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size and its rows padded accordingly.
0	The cudaMallocPitch() and cuMemAllocPitch() functions and associated memory copy functions described in the reference manual enable programmers to write non-hardware-dependent code to allocate arrays that conform to these constraints.
0	Local memory accesses only occur for some automatic variables as mentioned in Variable Type Qualifiers.
0	Automatic variables that the compiler is likely to place in local memory are: Arrays for which it cannot determine that they are indexed with constant quantities, Large structures or arrays that would consume too much register space, Any variable if the kernel uses more registers than available (this is also known as register spilling).
0	Inspection of the PTX assembly code (obtained by compiling with the -ptx or-keep option) will tell if a variable has been placed in local memory during the first compilation phases as it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics.
0	Even if it has not, subsequent compilation phases might still decide otherwise though if they find it consumes too much register space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if this is the case.
0	Also, the compiler reports total local memory usage per kernel (lmem) when compiling with the --ptxas-options=-v option.
0	Note that some mathematical functions have implementation paths that might access local memory.
0	The local memory space resides in device memory, so local memory accesses have same high latency and low bandwidth as global memory accesses and are subject to the same requirements for memory coalescing as described in Device Memory Accesses.
0	Local memory is however organized such that consecutive 32-bit words are accessed by consecutive thread IDs.
0	Accesses are therefore fully coalesced as long as all threads in a warp access the same relative address (e.g., same index in an array variable, same member in a structure variable).
0	On devices of compute capability 2.x and 3.x, local memory accesses are always cached in L1 and L2 in the same way as global memory accesses (see Compute Capability 2.x and Compute Capability 3.x).
0	On devices of compute capability 5.x, local memory accesses are always cached in L2 in the same way as global memory accesses (see Compute Capability 5.x).
0	Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory.
0	To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously.
0	Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is n times as high as the bandwidth of a single module.
0	However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized.
0	The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests.
0	If the number of separate memory requests is n, the initial memory request is said to cause n-way bank conflicts.
1	To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts.
0	This is described in Compute Capability 2.x, Compute Capability 3.x, and Compute Capability 5.x for devices of compute capability 2.x, 3.x, and 5.x, respectively.
0	The constant memory space resides in device memory and is cached in the constant cache mentioned in Compute Capability 2.x.
0	A request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests.
0	The resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise.
0	The texture and surface memory spaces reside in device memory and are cached in texture cache, so a texture fetch or surface read costs one memory read from device memory only on a cache miss, otherwise it just costs one read from texture cache.
1	The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture or surface addresses that are close together in 2D will achieve best performance.
1	Also, it is designed for streaming fetches with a constant latency; a cache hit reduces DRAM bandwidth demand but not fetch latency.
1	Reading device memory through texture or surface fetching present some benefits that can make it an advantageous alternative to reading device memory from global or constant memory: If the memory reads do not follow the access patterns that global or constant memory reads must follow to get good performance, higher bandwidth can be achieved providing that there is locality in the texture fetches or surface reads; Addressing calculations are performed outside the kernel by dedicated units; Packed data may be broadcast to separate variables in a single operation; 8-bit and 16-bit integer input data may be optionally converted to 32 bit floating-point values in the range [0.0, 1.0] or [-1.0, 1.0] (see Texture Memory).
1	To maximize instruction throughput the application should: Minimize the use of arithmetic instructions with low throughput; this includes trading precision for speed when it does not affect the end result, such as using intrinsic instead of regular functions (intrinsic functions are listed in Intrinsic Functions), single-precision instead of double-precision, or flushing denormalized numbers to zero; Minimize divergent warps caused by control flow instructions as detailed in Control Flow Instructions Reduce the number of instructions, for example, by optimizing out synchronization points whenever possible as described in Synchronization Instruction or by using restricted pointers as described in __restrict__.
0	In this section, throughputs are given in number of operations per clock cycle per multiprocessor.
0	For a warp size of 32, one instruction corresponds to 32 operations, so if N is the number of operations per clock cycle, the instruction throughput is N/32 instructions per clock cycle.
0	All throughputs are for one multiprocessor.
0	They must be multiplied by the number of multiprocessors in the device to get throughput for the whole device.
0	Table 2 gives the throughputs of the arithmetic instructions that are natively supported in hardware for devices of various compute capabilities.
0	Other instructions and functions are implemented on top of the native instructions.
0	The implementation may be different for devices of different compute capabilities, and the number of native instructions after compilation may fluctuate with every compiler version.
0	For complicated functions, there can be multiple code paths depending on input.
1	cuobjdump can be used to inspect a particular implementation in a cubin object.
0	The implementation of some functions are readily available on the CUDA header files (math_functions.h, device_functions.h, ...).
1	In general, code compiled with -ftz=true (denormalized numbers are flushed to zero) tends to have higher performance than code compiled with -ftz=false.
1	Similarly, code compiled with -prec div=false (less precise division) tends to have higher performance code than code compiled with -prec div=true, and code compiled with -prec-sqrt=false (less precise square root) tends to have higher performance than code compiled with -prec-sqrt=true.
0	The nvcc user manual describes these compilation flags in more details.
0	__fdividef(x, y) (see Intrinsic Functions) provides faster single-precision floating-point division than the division operator.
0	To preserve IEEE-754 semantics the compiler can optimize 1.0/sqrtf() into rsqrtf() only when both reciprocal and square root are approximate, (i.e., with -prec-div=false and -prec-sqrt=false).
1	It is therefore recommended to invoke rsqrtf() directly where desired.
0	Single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity.
0	sinf(x), cosf(x), tanf(x), sincosf(x), and corresponding double-precision instructions are much more expensive and even more so if the argument x is large in magnitude.
0	More precisely, the argument reduction code (see Mathematical Functions for implementation) comprises two code paths referred to as the fast path and the slow path, respectively.
0	The fast path is used for arguments sufficiently small in magnitude and essentially consists of a few multiply-add operations.
0	The slow path is used for arguments large in magnitude and consists of lengthy computations required to achieve correct results over the entire argument range.
0	At present, the argument reduction code for the trigonometric functions selects the fast path for arguments whose magnitude is less than 105615.0f for the single-precision functions, and less than 2147483648.0 for the double-precision functions.
0	As the slow path requires more registers than the fast path, an attempt has been made to reduce register pressure in the slow path by storing some intermediate variables in local memory, which may affect performance because of local memory high latency and bandwidth (see Device Memory Accesses).
0	At present, 28 bytes of local memory are used by single-precision functions, and 44 bytes are used by double-precision functions.
0	However, the exact amount is subject to change.
0	Due to the lengthy computations and use of local memory in the slow path, the throughput of these trigonometric functions is lower by one order of magnitude when the slow path reduction is required as opposed to the fast path reduction.
0	Integer division and modulo operation are costly as they compiler to up to 20 instructions.
0	They can be replaced with bitwise operations in some cases: If n is a power of 2, (i/n) is equivalent to (i>>log2(n)) and (i%n) is equivalent to (i&(n-1)); the compiler will perform these conversions if n is literal.
0	__brev and __popc map to a single instruction and __brevll and __popcll to a few instructions.
0	__[u]mul24 are legacy intrinsic functions that have no longer any reason to be used.
0	Sometimes, the compiler must insert conversion instructions, introducing additional execution cycles.
0	This is the case for: Functions operating on variables of type char or short whose operands generally need to be converted to int, Double-precision floating-point constants (i.e., those constants defined without any type suffix) used as input to single-precision floating-point computations (as mandated by C/C++ standards).
1	This last case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3.141592653589793f, 1.0f, 0.5f.
0	Any flow control instruction (if, switch, do, for, while) can significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i.e., to follow different execution paths).
0	If this happens, the different executions paths have to be serialized, increasing the total number of instructions executed for this warp.
0	When all the different execution paths have completed, the threads converge back to the same execution path.
1	To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps.
0	This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture.
0	A trivial example is when the controlling condition only depends on (threadIdx / warpSize) where warpSize is the warp size.
0	In this case, no warp diverges since the controlling condition is perfectly aligned with the warps.
0	Sometimes, the compiler may unroll loops or it may optimize out if or switch statements by using branch predication instead, as detailed below.
0	In these cases, no warp can ever diverge.
1	The programmer can also control loop unrolling using the #pragma unroll directive (see #pragma unroll).
0	When using branch predication none of the instructions whose execution depends on the controlling condition gets skipped.
0	Instead, each of them is associated with a per-thread condition code or predicate that is set to true or false based on the controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true predicate are actually executed.
0	Instructions with a false predicate do not write results, and also do not evaluate addresses or read operands.
0	The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less or equal to a certain threshold: If the compiler determines that the condition is likely to produce many divergent warps, this threshold is 7, otherwise it is 4.
0	Throughput for __syncthreads() is 16 operations per clock cycle for devices of compute capability 2.x and 128 operations per clock cycle for devices of compute capability 3.x.
0	Note that __syncthreads() can impact performance by forcing the multiprocessor to idle as detailed in Device Memory Accesses.
