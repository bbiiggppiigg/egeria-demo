<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<META http-equiv="X-UA-Compatible" content="IE=8">
<TITLE>bcl_254167465.htm</TITLE>
<META name="generator" content="BCL easyConverter SDK 5.0.08">
<STYLE type="text/css">

body {margin-top: 0px;margin-left: 0px;}

#page_1 {position:relative; overflow: hidden;margin: 0px 0px 0px 0px;padding: 0px;border: none;width: 816px;height: 1056px;}
#page_1 #id_1 {border:none;margin: 196px 0px 0px 131px;padding: 0px;border:none;width: 685px;overflow: hidden;}
#page_1 #id_2 {border:none;margin: 577px 0px 0px 62px;padding: 0px;border:none;width: 754px;overflow: hidden;}

#page_1 #p1dimg1 {position:absolute;top:0px;left:0px;z-index:-1;width:816px;height:1056px;}
#page_1 #p1dimg1 #p1img1 {width:816px;height:1056px;}




#page_2 {position:relative; overflow: hidden;margin: 186px 0px 54px 96px;padding: 0px;border: none;width: 720px;}
#page_2 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_2 #id_2 {border:none;margin: 67px 0px 0px 309px;padding: 0px;border:none;width: 411px;overflow: hidden;}





#page_3 {position:relative; overflow: hidden;margin: 48px 0px 544px 96px;padding: 0px;border: none;width: 625px;}





#page_4 {position:relative; overflow: hidden;margin: 186px 0px 54px 96px;padding: 0px;border: none;width: 720px;}
#page_4 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_4 #id_2 {border:none;margin: 20px 0px 0px 309px;padding: 0px;border:none;width: 411px;overflow: hidden;}





#page_5 {position:relative; overflow: hidden;margin: 51px 0px 117px 96px;padding: 0px;border: none;width: 720px;}

#page_5 #p5dimg1 {position:absolute;top:750px;left:156px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_5 #p5dimg1 #p5img1 {width:312px;height:1px;}




#page_6 {position:relative; overflow: hidden;margin: 51px 0px 97px 96px;padding: 0px;border: none;width: 720px;}
#page_6 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_6 #id_2 {border:none;margin: 429px 0px 0px 160px;padding: 0px;border:none;width: 560px;overflow: hidden;}

#page_6 #p6dimg1 {position:absolute;top:470px;left:0px;z-index:-1;width:624px;height:401px;}
#page_6 #p6dimg1 #p6img1 {width:624px;height:401px;}




#page_7 {position:relative; overflow: hidden;margin: 51px 0px 93px 76px;padding: 0px;border: none;width: 740px;}

#page_7 #p7dimg1 {position:absolute;top:109px;left:176px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_7 #p7dimg1 #p7img1 {width:312px;height:1px;}




#page_8 {position:relative; overflow: hidden;margin: 51px 0px 97px 76px;padding: 0px;border: none;width: 740px;}





#page_9 {position:relative; overflow: hidden;margin: 51px 0px 96px 96px;padding: 0px;border: none;width: 720px;}

#page_9 #p9dimg1 {position:absolute;top:329px;left:156px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_9 #p9dimg1 #p9img1 {width:312px;height:1px;}




#page_10 {position:relative; overflow: hidden;margin: 51px 0px 102px 71px;padding: 0px;border: none;width: 745px;}
#page_10 #id_1 {float:left;border:none;margin: 135px 0px 0px 0px;padding: 0px;border:none;width: 25px;overflow: hidden;}
#page_10 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_10 #p10dimg1 {position:absolute;top:578px;left:181px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_10 #p10dimg1 #p10img1 {width:312px;height:1px;}




#page_11 {position:relative; overflow: hidden;margin: 51px 0px 559px 71px;padding: 0px;border: none;width: 745px;}
#page_11 #id_1 {border:none;margin: 0px 0px 0px 25px;padding: 0px;border:none;width: 624px;overflow: hidden;}
#page_11 #id_2 {border:none;margin: 22px 0px 0px 0px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_11 #id_2 #id_2_1 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 25px;overflow: hidden;}
#page_11 #id_2 #id_2_2 {float:left;border:none;margin: 10px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_12 {position:relative; overflow: hidden;margin: 185px 0px 54px 96px;padding: 0px;border: none;width: 720px;}
#page_12 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_12 #id_2 {border:none;margin: 15px 0px 0px 305px;padding: 0px;border:none;width: 415px;overflow: hidden;}





#page_13 {position:relative; overflow: hidden;margin: 51px 0px 97px 95px;padding: 0px;border: none;width: 721px;}





#page_14 {position:relative; overflow: hidden;margin: 51px 0px 91px 94px;padding: 0px;border: none;width: 722px;}

#page_14 #p14dimg1 {position:absolute;top:325px;left:2px;z-index:-1;width:624px;height:476px;}
#page_14 #p14dimg1 #p14img1 {width:624px;height:476px;}




#page_15 {position:relative; overflow: hidden;margin: 51px 0px 336px 95px;padding: 0px;border: none;width: 721px;}

#page_15 #p15dimg1 {position:absolute;top:125px;left:1px;z-index:-1;width:624px;height:475px;}
#page_15 #p15dimg1 #p15img1 {width:624px;height:475px;}




#page_16 {position:relative; overflow: hidden;margin: 185px 0px 54px 76px;padding: 0px;border: none;width: 740px;}
#page_16 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_16 #id_1 #id_1_1 {float:left;border:none;margin: 640px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_16 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_16 #id_2 {border:none;margin: 17px 0px 0px 325px;padding: 0px;border:none;width: 415px;overflow: hidden;}

#page_16 #p16dimg1 {position:absolute;top:393px;left:176px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_16 #p16dimg1 #p16img1 {width:312px;height:1px;}




#page_17 {position:relative; overflow: hidden;margin: 51px 0px 101px 0px;padding: 0px;border: none;width: 816px;}
#page_17 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_17 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_17 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_17 #id_2 {border:none;margin: 28px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_17 #id_2 #id_2_1 {float:left;border:none;margin: 184px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_17 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_17 #p17dimg1 {position:absolute;top:143px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_17 #p17dimg1 #p17img1 {width:312px;height:1px;}




#page_18 {position:relative; overflow: hidden;margin: 51px 0px 91px 0px;padding: 0px;border: none;width: 816px;}
#page_18 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_18 #id_1 #id_1_1 {float:left;border:none;margin: 86px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_18 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_18 #id_2 {border:none;margin: 27px 0px 0px 96px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_18 #p18dimg1 {position:absolute;top:58px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_18 #p18dimg1 #p18img1 {width:312px;height:1px;}




#page_19 {position:relative; overflow: hidden;margin: 51px 0px 94px 96px;padding: 0px;border: none;width: 720px;}

#page_19 #p19dimg1 {position:absolute;top:340px;left:156px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_19 #p19dimg1 #p19img1 {width:312px;height:1px;}




#page_20 {position:relative; overflow: hidden;margin: 51px 0px 102px 76px;padding: 0px;border: none;width: 740px;}
#page_20 #id_1 {border:none;margin: 0px 0px 0px 18px;padding: 0px;border:none;width: 722px;overflow: hidden;}
#page_20 #id_2 {border:none;margin: 22px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_20 #id_2 #id_2_1 {float:left;border:none;margin: 357px 0px 0px 0px;padding: 0px;border:none;width: 19px;overflow: hidden;}
#page_20 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_21 {position:relative; overflow: hidden;margin: 51px 0px 95px 0px;padding: 0px;border: none;width: 816px;}
#page_21 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_21 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_21 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_21 #id_2 {border:none;margin: 27px 0px 0px 71px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_21 #id_2 #id_2_1 {float:left;border:none;margin: 446px 0px 0px 0px;padding: 0px;border:none;width: 24px;overflow: hidden;}
#page_21 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 721px;overflow: hidden;}





#page_22 {position:relative; overflow: hidden;margin: 51px 0px 93px 71px;padding: 0px;border: none;width: 745px;}
#page_22 #id_1 {float:left;border:none;margin: 50px 0px 0px 0px;padding: 0px;border:none;width: 23px;overflow: hidden;}
#page_22 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 722px;overflow: hidden;}

#page_22 #p22dimg1 {position:absolute;top:205px;left:181px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_22 #p22dimg1 #p22img1 {width:312px;height:1px;}




#page_23 {position:relative; overflow: hidden;margin: 51px 0px 93px 71px;padding: 0px;border: none;width: 745px;}
#page_23 #id_1 {float:left;border:none;margin: 454px 0px 0px 0px;padding: 0px;border:none;width: 24px;overflow: hidden;}
#page_23 #id_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_24 {position:relative; overflow: hidden;margin: 51px 0px 107px 71px;padding: 0px;border: none;width: 745px;}
#page_24 #id_1 {float:left;border:none;margin: 87px 0px 0px 0px;padding: 0px;border:none;width: 25px;overflow: hidden;}
#page_24 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_24 #p24dimg1 {position:absolute;top:58px;left:181px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_24 #p24dimg1 #p24img1 {width:312px;height:1px;}




#page_25 {position:relative; overflow: hidden;margin: 51px 0px 108px 96px;padding: 0px;border: none;width: 720px;}

#page_25 #p25dimg1 {position:absolute;top:222px;left:62px;z-index:-1;width:501px;height:438px;}
#page_25 #p25dimg1 #p25img1 {width:501px;height:438px;}




#page_26 {position:relative; overflow: hidden;margin: 51px 0px 267px 96px;padding: 0px;border: none;width: 625px;}

#page_26 #p26dimg1 {position:absolute;top:217px;left:0px;z-index:-1;width:624px;height:472px;}
#page_26 #p26dimg1 #p26img1 {width:624px;height:472px;}




#page_27 {position:relative; overflow: hidden;margin: 51px 0px 103px 76px;padding: 0px;border: none;width: 740px;}
#page_27 #id_1 {float:left;border:none;margin: 796px 0px 0px 0px;padding: 0px;border:none;width: 19px;overflow: hidden;}
#page_27 #id_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_28 {position:relative; overflow: hidden;margin: 51px 0px 330px 76px;padding: 0px;border: none;width: 740px;}
#page_28 #id_1 {float:left;border:none;margin: 391px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_28 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_28 #p28dimg1 {position:absolute;top:45px;left:174px;z-index:-1;width:316px;height:407px;}
#page_28 #p28dimg1 #p28img1 {width:316px;height:407px;}




#page_29 {position:relative; overflow: hidden;margin: 186px 0px 54px 96px;padding: 0px;border: none;width: 720px;}
#page_29 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_29 #id_2 {border:none;margin: 21px 0px 0px 305px;padding: 0px;border:none;width: 415px;overflow: hidden;}

#page_29 #p29dimg1 {position:absolute;top:362px;left:156px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_29 #p29dimg1 #p29img1 {width:312px;height:1px;}




#page_30 {position:relative; overflow: hidden;margin: 51px 0px 106px 0px;padding: 0px;border: none;width: 816px;}
#page_30 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_30 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_30 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 721px;overflow: hidden;}
#page_30 #id_2 {border:none;margin: 29px 0px 0px 96px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_30 #p30dimg1 {position:absolute;top:318px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_30 #p30dimg1 #p30img1 {width:312px;height:1px;}




#page_31 {position:relative; overflow: hidden;margin: 51px 0px 103px 0px;padding: 0px;border: none;width: 816px;}
#page_31 #id_1 {float:left;border:none;margin: 700px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_31 #id_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_32 {position:relative; overflow: hidden;margin: 51px 0px 97px 0px;padding: 0px;border: none;width: 816px;}
#page_32 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_32 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_32 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_32 #id_2 {border:none;margin: 56px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_32 #id_2 #id_2_1 {float:left;border:none;margin: 125px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_32 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_32 #p32dimg1 {position:absolute;top:111px;left:252px;z-index:-1;width:312px;height:665px;}
#page_32 #p32dimg1 #p32img1 {width:312px;height:665px;}




#page_33 {position:relative; overflow: hidden;margin: 51px 0px 103px 0px;padding: 0px;border: none;width: 816px;}
#page_33 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_33 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_33 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_33 #id_2 {border:none;margin: 27px 0px 0px 96px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_33 #id_3 {border:none;margin: 22px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_33 #id_3 #id_3_1 {float:left;border:none;margin: 489px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_33 #id_3 #id_3_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_34 {position:relative; overflow: hidden;margin: 51px 0px 124px 71px;padding: 0px;border: none;width: 745px;}
#page_34 #id_1 {float:left;border:none;margin: 50px 0px 0px 0px;padding: 0px;border:none;width: 25px;overflow: hidden;}
#page_34 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_34 #p34dimg1 {position:absolute;top:762px;left:181px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_34 #p34dimg1 #p34img1 {width:312px;height:1px;}




#page_35 {position:relative; overflow: hidden;margin: 51px 0px 94px 0px;padding: 0px;border: none;width: 816px;}
#page_35 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_35 #id_1 #id_1_1 {float:left;border:none;margin: 277px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_35 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_35 #id_2 {border:none;margin: 20px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_35 #id_2 #id_2_1 {float:left;border:none;margin: 228px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_35 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_36 {position:relative; overflow: hidden;margin: 51px 0px 99px 0px;padding: 0px;border: none;width: 816px;}
#page_36 #id_1 {border:none;margin: 0px 0px 0px 96px;padding: 0px;border:none;width: 624px;overflow: hidden;}
#page_36 #id_2 {border:none;margin: 22px 0px 0px 71px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_36 #id_2 #id_2_1 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 53px;overflow: hidden;}
#page_36 #id_2 #id_2_2 {float:left;border:none;margin: 9px 0px 0px 0px;padding: 0px;border:none;width: 692px;overflow: hidden;}
#page_36 #id_3 {border:none;margin: 28px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_36 #id_3 #id_3_1 {float:left;border:none;margin: 224px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_36 #id_3 #id_3_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_36 #p36dimg1 {position:absolute;top:627px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_36 #p36dimg1 #p36img1 {width:312px;height:1px;}




#page_37 {position:relative; overflow: hidden;margin: 51px 0px 121px 71px;padding: 0px;border: none;width: 745px;height: 884px;}
#page_37 #id_1 {border:none;margin: 0px 0px 0px 24px;padding: 0px;border:none;width: 625px;overflow: hidden;}
#page_37 #id_2 {border:none;margin: 26px 0px 0px 0px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_37 #id_2 #id_2_1 {float:left;border:none;margin: 255px 0px 0px 0px;padding: 0px;border:none;width: 24px;overflow: hidden;}
#page_37 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_37 #p37dimg1 {position:absolute;top:883px;left:181px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_37 #p37dimg1 #p37img1 {width:312px;height:1px;}




#page_38 {position:relative; overflow: hidden;margin: 48px 0px 334px 71px;padding: 0px;border: none;width: 745px;}
#page_38 #id_1 {float:left;border:none;margin: 53px 0px 0px 0px;padding: 0px;border:none;width: 24px;overflow: hidden;}
#page_38 #id_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_39 {position:relative; overflow: hidden;margin: 51px 0px 101px 96px;padding: 0px;border: none;width: 720px;}
#page_39 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 624px;overflow: hidden;}
#page_39 #id_2 {border:none;margin: 317px 0px 0px 92px;padding: 0px;border:none;width: 628px;overflow: hidden;}

#page_39 #p39dimg1 {position:absolute;top:48px;left:0px;z-index:-1;width:624px;height:826px;}
#page_39 #p39dimg1 #p39img1 {width:624px;height:826px;}




#page_40 {position:relative; overflow: hidden;margin: 186px 0px 54px 96px;padding: 0px;border: none;width: 720px;}
#page_40 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_40 #id_2 {border:none;margin: 30px 0px 0px 305px;padding: 0px;border:none;width: 415px;overflow: hidden;}





#page_41 {position:relative; overflow: hidden;margin: 51px 0px 95px 96px;padding: 0px;border: none;width: 720px;}

#page_41 #p41dimg1 {position:absolute;top:140px;left:0px;z-index:-1;width:624px;height:455px;}
#page_41 #p41dimg1 #p41img1 {width:624px;height:455px;}




#page_42 {position:relative; overflow: hidden;margin: 51px 0px 135px 76px;padding: 0px;border: none;width: 740px;}
#page_42 #id_1 {border:none;margin: 0px 0px 0px 20px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_42 #id_2 {border:none;margin: 15px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_42 #id_2 #id_2_1 {float:left;border:none;margin: 92px 0px 0px 0px;padding: 0px;border:none;width: 17px;overflow: hidden;}
#page_42 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 723px;overflow: hidden;}
#page_42 #id_3 {border:none;margin: 16px 0px 0px 0px;padding: 0px;border:none;width: 215px;overflow: hidden;}
#page_42 #id_3 #id_3_1 {float:left;border:none;margin: 6px 0px 0px 0px;padding: 0px;border:none;width: 48px;overflow: hidden;}
#page_42 #id_3 #id_3_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 167px;overflow: hidden;}
#page_42 #id_4 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_42 #id_5 {border:none;margin: 15px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_42 #id_5 #id_5_1 {float:left;border:none;margin: 129px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_42 #id_5 #id_5_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_42 #id_6 {border:none;margin: 19px 0px 0px 20px;padding: 0px;border:none;width: 624px;overflow: hidden;}

#page_42 #p42dimg1 {position:absolute;top:669px;left:176px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_42 #p42dimg1 #p42img1 {width:312px;height:1px;}




#page_43 {position:relative; overflow: hidden;margin: 51px 0px 95px 76px;padding: 0px;border: none;width: 740px;}
#page_43 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_43 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_43 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_43 #id_2 {border:none;margin: 15px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_43 #id_2 #id_2_1 {float:left;border:none;margin: 159px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_43 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_43 #id_3 {border:none;margin: 28px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_43 #id_3 #id_3_1 {float:left;border:none;margin: 216px 0px 0px 0px;padding: 0px;border:none;width: 19px;overflow: hidden;}
#page_43 #id_3 #id_3_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_43 #p43dimg1 {position:absolute;top:159px;left:176px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_43 #p43dimg1 #p43img1 {width:312px;height:1px;}




#page_44 {position:relative; overflow: hidden;margin: 51px 0px 94px 71px;padding: 0px;border: none;width: 745px;}
#page_44 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_44 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 24px;overflow: hidden;}
#page_44 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 721px;overflow: hidden;}
#page_44 #id_2 {border:none;margin: 26px 0px 0px 24px;padding: 0px;border:none;width: 721px;overflow: hidden;}
#page_44 #id_3 {border:none;margin: 26px 0px 0px 5px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_44 #id_3 #id_3_1 {float:left;border:none;margin: 287px 0px 0px 0px;padding: 0px;border:none;width: 18px;overflow: hidden;}
#page_44 #id_3 #id_3_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 722px;overflow: hidden;}





#page_45 {position:relative; overflow: hidden;margin: 51px 0px 94px 0px;padding: 0px;border: none;width: 816px;}
#page_45 #id_1 {float:left;border:none;margin: 50px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_45 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_45 #p45dimg1 {position:absolute;top:190px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_45 #p45dimg1 #p45img1 {width:312px;height:1px;}




#page_46 {position:relative; overflow: hidden;margin: 51px 0px 95px 0px;padding: 0px;border: none;width: 816px;}
#page_46 #id_1 {float:left;border:none;margin: 119px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_46 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 721px;overflow: hidden;}

#page_46 #p46dimg1 {position:absolute;top:371px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_46 #p46dimg1 #p46img1 {width:312px;height:1px;}




#page_47 {position:relative; overflow: hidden;margin: 51px 0px 93px 0px;padding: 0px;border: none;width: 816px;}
#page_47 #id_1 {float:left;border:none;margin: 396px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_47 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_47 #p47dimg1 {position:absolute;top:45px;left:168px;z-index:-1;width:480px;height:524px;}
#page_47 #p47dimg1 #p47img1 {width:480px;height:524px;}




#page_48 {position:relative; overflow: hidden;margin: 51px 0px 468px 96px;padding: 0px;border: none;width: 720px;}

#page_48 #p48dimg1 {position:absolute;top:45px;left:71px;z-index:-1;width:482px;height:288px;}
#page_48 #p48dimg1 #p48img1 {width:482px;height:288px;}




#page_49 {position:relative; overflow: hidden;margin: 186px 0px 61px 0px;padding: 0px;border: none;width: 816px;}
#page_49 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_49 #id_1 #id_1_1 {float:left;border:none;margin: 548px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_49 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_49 #id_2 {border:none;margin: 28px 0px 0px 401px;padding: 0px;border:none;width: 415px;overflow: hidden;}





#page_50 {position:relative; overflow: hidden;margin: 51px 0px 95px 0px;padding: 0px;border: none;width: 816px;}
#page_50 #id_1 {border:none;margin: 0px 0px 0px 71px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_50 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 25px;overflow: hidden;}
#page_50 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_50 #id_2 {border:none;margin: 30px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_50 #id_2 #id_2_1 {float:left;border:none;margin: 200px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_50 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_51 {position:relative; overflow: hidden;margin: 51px 0px 94px 71px;padding: 0px;border: none;width: 745px;}
#page_51 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 745px;overflow: hidden;}
#page_51 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 25px;overflow: hidden;}
#page_51 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_51 #id_2 {border:none;margin: 27px 0px 0px 25px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_52 {position:relative; overflow: hidden;margin: 51px 0px 540px 0px;padding: 0px;border: none;width: 816px;}
#page_52 #id_1 {float:left;border:none;margin: 199px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_52 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}





#page_53 {position:relative; overflow: hidden;margin: 186px 0px 61px 76px;padding: 0px;border: none;width: 740px;}
#page_53 #id_1 {border:none;margin: 0px 0px 0px 20px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_53 #id_2 {border:none;margin: 26px 0px 0px 0px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_53 #id_2 #id_2_1 {float:left;border:none;margin: 248px 0px 0px 0px;padding: 0px;border:none;width: 19px;overflow: hidden;}
#page_53 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 721px;overflow: hidden;}
#page_53 #id_3 {border:none;margin: 17px 0px 0px 325px;padding: 0px;border:none;width: 415px;overflow: hidden;}

#page_53 #p53dimg1 {position:absolute;top:616px;left:176px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_53 #p53dimg1 #p53img1 {width:312px;height:1px;}




#page_54 {position:relative; overflow: hidden;margin: 51px 0px 93px 0px;padding: 0px;border: none;width: 816px;}
#page_54 #id_1 {border:none;margin: 0px 0px 0px 76px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_54 #id_1 #id_1_1 {float:left;border:none;margin: 149px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_54 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_54 #id_2 {border:none;margin: 14px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_54 #id_2 #id_2_1 {float:left;border:none;margin: 213px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_54 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_54 #p54dimg1 {position:absolute;top:287px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_54 #p54dimg1 #p54img1 {width:312px;height:1px;}




#page_55 {position:relative; overflow: hidden;margin: 51px 0px 109px 0px;padding: 0px;border: none;width: 816px;}
#page_55 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_55 #id_1 #id_1_1 {float:left;border:none;margin: 88px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_55 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_55 #id_2 {border:none;margin: 20px 0px 0px 96px;padding: 0px;border:none;width: 627px;overflow: hidden;}
#page_55 #id_3 {border:none;margin: 28px 0px 0px 76px;padding: 0px;border:none;width: 740px;overflow: hidden;}
#page_55 #id_3 #id_3_1 {float:left;border:none;margin: 192px 0px 0px 0px;padding: 0px;border:none;width: 19px;overflow: hidden;}
#page_55 #id_3 #id_3_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 721px;overflow: hidden;}

#page_55 #p55dimg1 {position:absolute;top:58px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_55 #p55dimg1 #p55img1 {width:312px;height:1px;}




#page_56 {position:relative; overflow: hidden;margin: 51px 0px 150px 76px;padding: 0px;border: none;width: 740px;}
#page_56 #id_1 {float:left;border:none;margin: 546px 0px 0px 0px;padding: 0px;border:none;width: 20px;overflow: hidden;}
#page_56 #id_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_56 #p56dimg1 {position:absolute;top:45px;left:132px;z-index:-1;width:400px;height:472px;}
#page_56 #p56dimg1 #p56img1 {width:400px;height:472px;}




#page_57 {position:relative; overflow: hidden;margin: 51px 0px 93px 0px;padding: 0px;border: none;width: 816px;}
#page_57 #id_1 {border:none;margin: 0px 0px 0px 96px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_57 #id_2 {border:none;margin: 26px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_57 #id_2 #id_2_1 {float:left;border:none;margin: 155px 0px 0px 0px;padding: 0px;border:none;width: 96px;overflow: hidden;}
#page_57 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_57 #p57dimg1 {position:absolute;top:45px;left:96px;z-index:-1;width:624px;height:716px;}
#page_57 #p57dimg1 #p57img1 {width:624px;height:716px;}




#page_58 {position:relative; overflow: hidden;margin: 51px 0px 112px 0px;padding: 0px;border: none;width: 816px;}
#page_58 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_58 #id_1 #id_1_1 {float:left;border:none;margin: 51px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_58 #id_1 #id_1_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_58 #id_2 {border:none;margin: 21px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_58 #id_2 #id_2_1 {float:left;border:none;margin: 8px 0px 0px 0px;padding: 0px;border:none;width: 124px;overflow: hidden;}
#page_58 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 692px;overflow: hidden;}
#page_58 #id_3 {border:none;margin: 0px 0px 0px 71px;padding: 0px;border:none;width: 745px;overflow: hidden;}

#page_58 #p58dimg1 {position:absolute;top:611px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_58 #p58dimg1 #p58img1 {width:312px;height:1px;}




#page_59 {position:relative; overflow: hidden;margin: 51px 0px 99px 0px;padding: 0px;border: none;width: 816px;}
#page_59 #id_1 {border:none;margin: 0px 0px 0px 96px;padding: 0px;border:none;width: 625px;overflow: hidden;}
#page_59 #id_2 {border:none;margin: 29px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_59 #id_2 #id_2_1 {float:left;border:none;margin: 173px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_59 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_59 #p59dimg1 {position:absolute;top:542px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_59 #p59dimg1 #p59img1 {width:312px;height:1px;}




#page_60 {position:relative; overflow: hidden;margin: 51px 0px 92px 96px;padding: 0px;border: none;width: 720px;}
#page_60 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_60 #id_2 {border:none;margin: 11px 0px 0px 0px;padding: 0px;border:none;width: 624px;overflow: hidden;}

#page_60 #p60dimg1 {position:absolute;top:290px;left:0px;z-index:-1;width:624px;height:355px;}
#page_60 #p60dimg1 #p60img1 {width:624px;height:355px;}




#page_61 {position:relative; overflow: hidden;margin: 51px 0px 95px 0px;padding: 0px;border: none;width: 816px;}
#page_61 #id_1 {border:none;margin: 0px 0px 0px 96px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_61 #id_2 {border:none;margin: 19px 0px 0px 0px;padding: 0px;border:none;width: 816px;overflow: hidden;}
#page_61 #id_2 #id_2_1 {float:left;border:none;margin: 576px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_61 #id_2 #id_2_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_61 #p61dimg1 {position:absolute;top:330px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_61 #p61dimg1 #p61img1 {width:312px;height:1px;}




#page_62 {position:relative; overflow: hidden;margin: 51px 0px 212px 0px;padding: 0px;border: none;width: 816px;}
#page_62 #id_1 {float:left;border:none;margin: 50px 0px 0px 0px;padding: 0px;border:none;width: 95px;overflow: hidden;}
#page_62 #id_2 {float:left;border:none;margin: 0px 0px 0px 1px;padding: 0px;border:none;width: 720px;overflow: hidden;}

#page_62 #p62dimg1 {position:absolute;top:286px;left:252px;z-index:-1;width:312px;height:1px;font-size: 1px;line-height:nHeight;}
#page_62 #p62dimg1 #p62img1 {width:312px;height:1px;}




#page_63 {position:relative; overflow: hidden;margin: 51px 0px 333px 96px;padding: 0px;border: none;width: 720px;}
#page_63 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 624px;overflow: hidden;}
#page_63 #id_2 {border:none;margin: 631px 0px 0px 100px;padding: 0px;border:none;width: 620px;overflow: hidden;}

#page_63 #p63dimg1 {position:absolute;top:280px;left:0px;z-index:-1;width:624px;height:362px;}
#page_63 #p63dimg1 #p63img1 {width:624px;height:362px;}




#page_64 {position:relative; overflow: hidden;margin: 186px 0px 61px 96px;padding: 0px;border: none;width: 720px;}
#page_64 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_64 #id_2 {border:none;margin: 541px 0px 0px 305px;padding: 0px;border:none;width: 415px;overflow: hidden;}





.dclr {clear:both;float:none;height:1px;margin:0px;padding:0px;overflow:hidden;}

.ft0{font: bold 44px 'Arial';color: #000049;line-height: 75px;}
.ft1{font: 28px 'Arial';color: #3986b5;line-height: 32px;}
.ft2{font: 11px 'Arial';color: #000049;line-height: 14px;}
.ft3{font: bold 33px 'Arial';line-height: 38px;}
.ft4{font: bold 13px 'Arial';line-height: 16px;}
.ft5{font: 1px 'Gabriola';line-height: 1px;}
.ft6{font: 13px 'Gabriola';line-height: 21px;}
.ft7{font: 13px 'Gabriola';line-height: 22px;}
.ft8{font: 13px 'Gabriola';line-height: 23px;}
.ft9{font: 13px 'Arial';line-height: 16px;}
.ft10{font: bold 28px 'Arial';line-height: 33px;}
.ft11{font: bold 19px 'Arial';line-height: 22px;}
.ft12{font: bold 16px 'Arial';line-height: 19px;}
.ft13{font: 13px 'Gabriola';line-height: 18px;}
.ft14{font: 13px 'Gabriola';line-height: 20px;}
.ft15{font: italic 13px 'Arial';line-height: 16px;}
.ft16{font: italic bold 13px 'Arial';line-height: 16px;}
.ft17{font: 7px 'Arial';line-height: 7px;}
.ft18{font: 13px 'Arial';color: #007021;margin-left: 44px;line-height: 16px;}
.ft19{font: 13px 'Arial';color: #007021;line-height: 16px;}
.ft20{font: italic 13px 'Arial';color: #61a1b0;margin-left: 44px;line-height: 16px;}
.ft21{font: italic 13px 'Arial';color: #61a1b0;line-height: 16px;}
.ft22{font: 13px 'Gabriola';line-height: 17px;}
.ft23{font: 13px 'Arial';line-height: 17px;}
.ft24{font: bold 13px 'Arial';color: #007021;line-height: 16px;}
.ft25{font: 13px 'Arial';color: #40a170;line-height: 16px;}
.ft26{font: 13px 'Arial';color: #8f2100;line-height: 16px;}
.ft27{font: 12px 'Arial';line-height: 15px;}
.ft28{font: 13px 'Arial';margin-left: 7px;line-height: 16px;}
.ft29{font: 13px 'Arial';color: #4070a1;line-height: 16px;}
.ft30{font: bold 13px 'Arial';color: #007021;line-height: 17px;}
.ft31{font: 13px 'Arial';color: #40a170;line-height: 17px;}
.ft32{font: 13px 'Arial';color: #05297d;line-height: 16px;}
.ft33{font: 13px 'Gabriola';margin-left: 6px;line-height: 23px;}
.ft34{font: 13px 'Gabriola';margin-left: 6px;line-height: 20px;}
.ft35{font: 13px 'Arial';line-height: 18px;}
.ft36{font: italic 13px 'Arial';color: #61a1b0;line-height: 17px;}
.ft37{font: 13px 'Arial';color: #007021;line-height: 17px;}
.ft38{font: bold 19px 'Gabriola';line-height: 32px;}
.ft39{font: 13px 'Gabriola';line-height: 19px;}
.ft40{font: 13px 'Arial';color: #8f2100;line-height: 17px;}
.ft41{font: 13px 'Arial';margin-left: 8px;line-height: 16px;}
.ft42{font: italic 13px 'Arial';color: #61a1b0;line-height: 14px;}
.ft43{font: 13px 'Arial';margin-left: 6px;line-height: 17px;}
.ft44{font: 13px 'Arial';margin-left: 6px;line-height: 16px;}
.ft45{font: 13px 'Arial';margin-left: 9px;line-height: 16px;}
.ft46{font: 13px 'Arial';margin-left: 9px;line-height: 17px;}
.ft47{font: bold 19px 'Gabriola';line-height: 35px;}
.ft48{font: italic 13px 'Arial';color: #61a1b0;line-height: 15px;}
.ft49{font: italic 13px 'Arial';color: #61a1b0;margin-left: 7px;line-height: 15px;}
.ft50{font: 13px 'Arial';line-height: 15px;}
.ft51{font: 13px 'Arial';color: #40a170;line-height: 15px;}
.ft52{font: italic 13px 'Arial';color: #61a1b0;margin-left: 7px;line-height: 17px;}
.ft53{font: italic 13px 'Arial';line-height: 18px;}
.ft54{font: 13px 'Arial';margin-left: 7px;line-height: 15px;}
.ft55{font: 1px 'Arial';line-height: 1px;}
.ft56{font: italic 13px 'Arial';color: #61a1b0;margin-left: 58px;line-height: 14px;}
.ft57{font: bold 13px 'Arial';color: #007021;margin-left: 58px;line-height: 16px;}
.ft58{font: italic 13px 'Arial';color: #61a1b0;margin-left: 7px;line-height: 16px;}
.ft59{font: bold 13px 'Arial';color: #007021;line-height: 15px;}
.ft60{font: 13px 'Arial';color: #4070a1;line-height: 17px;}
.ft61{font: bold 13px 'Arial';color: #007021;line-height: 14px;}
.ft62{font: 13px 'Arial';line-height: 14px;}
.ft63{font: 13px 'Arial';color: #40a170;line-height: 14px;}
.ft64{font: bold 13px 'Arial';color: #007021;line-height: 13px;}
.ft65{font: 13px 'Arial';color: #0000ff;margin-left: 6px;line-height: 16px;}
.ft66{font: 13px 'Arial';color: #0000ff;line-height: 16px;}

.p0{text-align: left;padding-right: 150px;margin-top: 0px;margin-bottom: 0px;}
.p1{text-align: left;padding-left: 3px;margin-top: 35px;margin-bottom: 0px;}
.p2{text-align: left;margin-top: 0px;margin-bottom: 0px;}
.p3{text-align: right;padding-right: 6px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p4{text-align: left;padding-left: 6px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p5{text-align: right;padding-right: 2px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p6{text-align: left;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p7{text-align: right;padding-right: 1px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p8{text-align: right;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p9{text-align: left;margin-top: 35px;margin-bottom: 0px;}
.p10{text-align: justify;padding-right: 96px;margin-top: 57px;margin-bottom: 0px;}
.p11{text-align: left;margin-top: 25px;margin-bottom: 0px;}
.p12{text-align: justify;padding-right: 93px;margin-top: 22px;margin-bottom: 0px;}
.p13{text-align: justify;padding-right: 96px;margin-top: 3px;margin-bottom: 0px;}
.p14{text-align: left;margin-top: 21px;margin-bottom: 0px;}
.p15{text-align: justify;padding-right: 94px;margin-top: 11px;margin-bottom: 0px;}
.p16{text-align: justify;padding-right: 95px;margin-top: 9px;margin-bottom: 0px;}
.p17{text-align: justify;padding-right: 94px;margin-top: 5px;margin-bottom: 0px;}
.p18{text-align: justify;padding-right: 93px;margin-top: 17px;margin-bottom: 0px;}
.p19{text-align: left;margin-top: 29px;margin-bottom: 0px;}
.p20{text-align: justify;padding-right: 95px;margin-top: 16px;margin-bottom: 0px;}
.p21{text-align: left;margin-top: 26px;margin-bottom: 0px;}
.p22{text-align: justify;padding-right: 95px;margin-top: 17px;margin-bottom: 0px;}
.p23{text-align: justify;padding-right: 93px;margin-top: 55px;margin-bottom: 0px;}
.p24{text-align: left;margin-top: 8px;margin-bottom: 0px;}
.p25{text-align: justify;padding-right: 93px;margin-top: 16px;margin-bottom: 0px;}
.p26{text-align: left;margin-top: 27px;margin-bottom: 0px;}
.p27{text-align: justify;padding-right: 96px;margin-top: 12px;margin-bottom: 0px;}
.p28{text-align: justify;padding-right: 95px;margin-top: 8px;margin-bottom: 0px;}
.p29{text-align: justify;padding-left: 20px;padding-right: 95px;margin-top: 14px;margin-bottom: 0px;}
.p30{text-align: justify;padding-left: 18px;padding-right: 96px;margin-top: 64px;margin-bottom: 0px;text-indent: 2px;}
.p31{text-align: left;padding-left: 20px;margin-top: 17px;margin-bottom: 0px;}
.p32{text-align: justify;padding-left: 20px;padding-right: 93px;margin-top: 12px;margin-bottom: 0px;}
.p33{text-align: left;padding-left: 20px;margin-top: 31px;margin-bottom: 0px;}
.p34{text-align: justify;padding-left: 20px;padding-right: 96px;margin-top: 21px;margin-bottom: 0px;}
.p35{text-align: left;padding-left: 20px;margin-top: 21px;margin-bottom: 0px;}
.p36{text-align: justify;padding-left: 19px;padding-right: 93px;margin-top: 16px;margin-bottom: 0px;}
.p37{text-align: left;padding-left: 20px;padding-right: 96px;margin-top: 2px;margin-bottom: 0px;}
.p38{text-align: left;margin-top: 31px;margin-bottom: 0px;}
.p39{text-align: left;padding-left: 20px;margin-top: 14px;margin-bottom: 0px;}
.p40{text-align: left;margin-top: 16px;margin-bottom: 0px;}
.p41{text-align: justify;padding-left: 20px;padding-right: 96px;margin-top: 12px;margin-bottom: 0px;}
.p42{text-align: left;padding-left: 20px;margin-top: 5px;margin-bottom: 0px;}
.p43{text-align: left;padding-left: 20px;margin-top: 19px;margin-bottom: 0px;}
.p44{text-align: justify;padding-left: 20px;padding-right: 95px;margin-top: 16px;margin-bottom: 0px;}
.p45{text-align: justify;padding-left: 20px;padding-right: 94px;margin-top: 5px;margin-bottom: 0px;}
.p46{text-align: left;padding-left: 20px;margin-top: 32px;margin-bottom: 0px;}
.p47{text-align: justify;padding-left: 20px;padding-right: 93px;margin-top: 16px;margin-bottom: 0px;}
.p48{text-align: left;padding-left: 20px;margin-top: 23px;margin-bottom: 0px;}
.p49{text-align: justify;padding-left: 20px;padding-right: 95px;margin-top: 17px;margin-bottom: 0px;}
.p50{text-align: justify;padding-left: 20px;padding-right: 94px;margin-top: 12px;margin-bottom: 0px;}
.p51{text-align: justify;padding-right: 96px;margin-top: 10px;margin-bottom: 0px;}
.p52{text-align: left;margin-top: 23px;margin-bottom: 0px;}
.p53{text-align: justify;padding-right: 95px;margin-top: 55px;margin-bottom: 0px;}
.p54{text-align: left;margin-top: 28px;margin-bottom: 0px;}
.p55{text-align: justify;padding-right: 93px;margin-top: 3px;margin-bottom: 0px;}
.p56{text-align: left;margin-top: 9px;margin-bottom: 0px;}
.p57{text-align: left;margin-top: 68px;margin-bottom: 0px;}
.p58{text-align: justify;padding-right: 95px;margin-top: 18px;margin-bottom: 0px;}
.p59{text-align: left;padding-left: 28px;margin-top: 18px;margin-bottom: 0px;}
.p60{text-align: left;padding-left: 28px;margin-top: 0px;margin-bottom: 0px;}
.p61{text-align: left;padding-left: 56px;margin-top: 1px;margin-bottom: 0px;}
.p62{text-align: left;padding-left: 56px;margin-top: 14px;margin-bottom: 0px;}
.p63{text-align: left;padding-left: 56px;margin-top: 0px;margin-bottom: 0px;}
.p64{text-align: left;padding-left: 84px;margin-top: 0px;margin-bottom: 0px;}
.p65{text-align: left;padding-left: 112px;margin-top: 0px;margin-bottom: 0px;}
.p66{text-align: left;padding-left: 251px;margin-top: 0px;margin-bottom: 0px;}
.p67{text-align: left;padding-left: 84px;margin-top: 1px;margin-bottom: 0px;}
.p68{text-align: left;padding-left: 56px;margin-top: 15px;margin-bottom: 0px;}
.p69{text-align: left;padding-left: 28px;margin-top: 2px;margin-bottom: 0px;}
.p70{text-align: left;padding-left: 42px;padding-right: 329px;margin-top: 58px;margin-bottom: 0px;text-indent: -13px;}
.p71{text-align: left;padding-left: 56px;padding-right: 594px;margin-top: 12px;margin-bottom: 0px;text-indent: -13px;}
.p72{text-align: left;padding-left: 272px;padding-right: 260px;margin-top: 0px;margin-bottom: 0px;text-indent: -201px;}
.p73{text-align: left;padding-left: 70px;margin-top: 0px;margin-bottom: 0px;}
.p74{text-align: left;padding-left: 42px;padding-right: 617px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p75{text-align: left;padding-left: 42px;padding-right: 274px;margin-top: 15px;margin-bottom: 0px;}
.p76{text-align: justify;padding-right: 95px;margin-top: 15px;margin-bottom: 0px;}
.p77{text-align: justify;padding-right: 94px;margin-top: 4px;margin-bottom: 0px;}
.p78{text-align: justify;padding-right: 94px;margin-top: 6px;margin-bottom: 0px;}
.p79{text-align: left;margin-top: 3px;margin-bottom: 0px;}
.p80{text-align: left;margin-top: 34px;margin-bottom: 0px;}
.p81{text-align: justify;padding-right: 96px;margin-top: 61px;margin-bottom: 0px;}
.p82{text-align: justify;padding-right: 96px;margin-top: 18px;margin-bottom: 0px;}
.p83{text-align: left;margin-top: 30px;margin-bottom: 0px;}
.p84{text-align: justify;padding-right: 96px;margin-top: 21px;margin-bottom: 0px;}
.p85{text-align: left;padding-left: 33px;padding-right: 96px;margin-top: 11px;margin-bottom: 0px;text-indent: -17px;}
.p86{text-align: justify;padding-left: 33px;padding-right: 96px;margin-top: 11px;margin-bottom: 0px;text-indent: -17px;}
.p87{text-align: justify;padding-left: 32px;padding-right: 93px;margin-top: 0px;margin-bottom: 0px;text-indent: -16px;}
.p88{text-align: justify;padding-left: 33px;padding-right: 96px;margin-top: 6px;margin-bottom: 0px;text-indent: -17px;}
.p89{text-align: left;padding-right: 95px;margin-top: 6px;margin-bottom: 0px;}
.p90{text-align: justify;padding-left: 1px;padding-right: 93px;margin-top: 10px;margin-bottom: 0px;}
.p91{text-align: left;padding-left: 1px;margin-top: 25px;margin-bottom: 0px;}
.p92{text-align: justify;padding-right: 94px;margin-top: 22px;margin-bottom: 0px;}
.p93{text-align: left;padding-left: 1px;margin-top: 28px;margin-bottom: 0px;}
.p94{text-align: justify;padding-left: 1px;padding-right: 95px;margin-top: 16px;margin-bottom: 0px;}
.p95{text-align: left;padding-left: 29px;padding-right: 364px;margin-top: 23px;margin-bottom: 0px;}
.p96{text-align: left;padding-left: 92px;padding-right: 281px;margin-top: 16px;margin-bottom: 0px;text-indent: -27px;}
.p97{text-align: left;padding-left: 64px;padding-right: 336px;margin-top: 15px;margin-bottom: 0px;text-indent: 28px;}
.p98{text-align: left;padding-left: 92px;padding-right: 309px;margin-top: 0px;margin-bottom: 0px;}
.p99{text-align: left;padding-left: 64px;margin-top: 1px;margin-bottom: 0px;}
.p100{text-align: left;padding-left: 1px;padding-right: 96px;margin-top: 15px;margin-bottom: 0px;}
.p101{text-align: left;padding-left: 29px;margin-top: 19px;margin-bottom: 0px;}
.p102{text-align: left;padding-left: 57px;padding-right: 364px;margin-top: 0px;margin-bottom: 0px;text-indent: -27px;}
.p103{text-align: justify;padding-left: 43px;padding-right: 587px;margin-top: 0px;margin-bottom: 0px;}
.p104{text-align: left;padding-left: 37px;padding-right: 560px;margin-top: 18px;margin-bottom: 0px;text-indent: 7px;}
.p105{text-align: left;padding-left: 30px;margin-top: 1px;margin-bottom: 0px;}
.p106{text-align: left;padding-left: 2px;padding-right: 96px;margin-top: 18px;margin-bottom: 0px;}
.p107{text-align: left;padding-left: 30px;margin-top: 17px;margin-bottom: 0px;}
.p108{text-align: justify;padding-right: 96px;margin-top: 18px;margin-bottom: 0px;text-indent: 1px;}
.p109{text-align: left;padding-left: 2px;margin-top: 506px;margin-bottom: 0px;}
.p110{text-align: justify;padding-left: 2px;padding-right: 94px;margin-top: 15px;margin-bottom: 0px;}
.p111{text-align: justify;padding-left: 1px;padding-right: 96px;margin-top: 14px;margin-bottom: 0px;text-indent: -1px;}
.p112{text-align: left;padding-left: 72px;margin-top: 502px;margin-bottom: 0px;}
.p113{text-align: left;padding-left: 1px;padding-right: 95px;margin-top: 9px;margin-bottom: 0px;}
.p114{text-align: justify;padding-right: 95px;margin-top: 56px;margin-bottom: 0px;}
.p115{text-align: justify;padding-right: 96px;margin-top: 48px;margin-bottom: 0px;}
.p116{text-align: justify;padding-right: 93px;margin-top: 21px;margin-bottom: 0px;}
.p117{text-align: left;padding-left: 28px;margin-top: 23px;margin-bottom: 0px;}
.p118{text-align: left;padding-left: 42px;margin-top: 1px;margin-bottom: 0px;}
.p119{text-align: left;padding-left: 42px;margin-top: 0px;margin-bottom: 0px;}
.p120{text-align: left;padding-left: 42px;margin-top: 15px;margin-bottom: 0px;}
.p121{text-align: left;padding-left: 71px;margin-top: 0px;margin-bottom: 0px;}
.p122{text-align: left;padding-left: 71px;margin-top: 9px;margin-bottom: 0px;}
.p123{text-align: right;padding-right: 16px;margin-top: 67px;margin-bottom: 0px;}
.p124{text-align: right;padding-right: 16px;margin-top: 9px;margin-bottom: 0px;}
.p125{text-align: left;padding-left: 28px;margin-top: 1px;margin-bottom: 0px;}
.p126{text-align: left;padding-left: 28px;padding-right: 601px;margin-top: 56px;margin-bottom: 0px;}
.p127{text-align: left;padding-left: 42px;padding-right: 622px;margin-top: 0px;margin-bottom: 0px;}
.p128{text-align: left;padding-left: 28px;margin-top: 14px;margin-bottom: 0px;}
.p129{text-align: justify;padding-right: 94px;margin-top: 19px;margin-bottom: 0px;}
.p130{text-align: right;padding-right: 16px;margin-top: 0px;margin-bottom: 0px;}
.p131{text-align: left;padding-left: 71px;margin-top: 8px;margin-bottom: 0px;}
.p132{text-align: left;padding-left: 42px;padding-right: 518px;margin-top: 18px;margin-bottom: 0px;text-indent: -13px;}
.p133{text-align: left;padding-left: 42px;padding-right: 518px;margin-top: 13px;margin-bottom: 0px;text-indent: -13px;}
.p134{text-align: left;padding-left: 28px;padding-right: 560px;margin-top: 49px;margin-bottom: 0px;}
.p135{text-align: left;padding-left: 28px;padding-right: 560px;margin-top: 14px;margin-bottom: 0px;}
.p136{text-align: left;padding-left: 28px;padding-right: 511px;margin-top: 1px;margin-bottom: 0px;text-indent: 14px;}
.p137{text-align: justify;padding-right: 95px;margin-top: 29px;margin-bottom: 0px;}
.p138{text-align: justify;padding-right: 96px;margin-top: 6px;margin-bottom: 0px;}
.p139{text-align: justify;padding-right: 94px;margin-top: 16px;margin-bottom: 0px;}
.p140{text-align: justify;padding-right: 96px;margin-top: 14px;margin-bottom: 0px;}
.p141{text-align: justify;padding-right: 96px;margin-top: 17px;margin-bottom: 0px;}
.p142{text-align: justify;padding-right: 95px;margin-top: 7px;margin-bottom: 0px;}
.p143{text-align: justify;padding-right: 96px;margin-top: 1px;margin-bottom: 0px;}
.p144{text-align: justify;padding-right: 95px;margin-top: 50px;margin-bottom: 0px;}
.p145{text-align: justify;padding-right: 94px;margin-top: 12px;margin-bottom: 0px;}
.p146{text-align: left;margin-top: 18px;margin-bottom: 0px;}
.p147{text-align: justify;padding-right: 94px;margin-top: 13px;margin-bottom: 0px;}
.p148{text-align: justify;padding-left: 2px;padding-right: 96px;margin-top: 14px;margin-bottom: 0px;}
.p149{text-align: justify;padding-left: 2px;padding-right: 96px;margin-top: 10px;margin-bottom: 0px;}
.p150{text-align: left;padding-left: 2px;margin-top: 35px;margin-bottom: 0px;}
.p151{text-align: justify;padding-right: 96px;margin-top: 14px;margin-bottom: 0px;text-indent: 1px;}
.p152{text-align: left;padding-left: 2px;margin-top: 21px;margin-bottom: 0px;}
.p153{text-align: left;padding-left: 1px;margin-top: 20px;margin-bottom: 0px;}
.p154{text-align: justify;padding-right: 95px;margin-top: 19px;margin-bottom: 0px;}
.p155{text-align: left;margin-top: 24px;margin-bottom: 0px;}
.p156{text-align: left;padding-left: 28px;margin-top: 24px;margin-bottom: 0px;}
.p157{text-align: left;padding-left: 56px;padding-right: 581px;margin-top: 2px;margin-bottom: 0px;text-indent: -6px;}
.p158{text-align: left;padding-left: 49px;padding-right: 483px;margin-top: 13px;margin-bottom: 0px;}
.p159{text-align: left;padding-left: 56px;margin-top: 17px;margin-bottom: 0px;}
.p160{text-align: left;padding-left: 49px;margin-top: 0px;margin-bottom: 0px;}
.p161{text-align: left;padding-left: 1px;margin-top: 0px;margin-bottom: 0px;}
.p162{text-align: justify;padding-left: 1px;padding-right: 94px;margin-top: 22px;margin-bottom: 0px;}
.p163{text-align: left;padding-left: 1px;margin-top: 30px;margin-bottom: 0px;}
.p164{text-align: justify;padding-left: 1px;padding-right: 94px;margin-top: 12px;margin-bottom: 0px;}
.p165{text-align: justify;padding-left: 1px;padding-right: 95px;margin-top: 8px;margin-bottom: 0px;}
.p166{text-align: justify;padding-left: 1px;padding-right: 96px;margin-top: 0px;margin-bottom: 0px;}
.p167{text-align: left;padding-left: 29px;margin-top: 13px;margin-bottom: 0px;}
.p168{text-align: left;padding-left: 29px;margin-top: 0px;margin-bottom: 0px;}
.p169{text-align: left;padding-left: 43px;margin-top: 2px;margin-bottom: 0px;}
.p170{text-align: left;padding-left: 43px;padding-right: 357px;margin-top: 14px;margin-bottom: 0px;}
.p171{text-align: left;padding-left: 43px;margin-top: 0px;margin-bottom: 0px;}
.p172{text-align: left;padding-left: 57px;padding-right: 406px;margin-top: 0px;margin-bottom: 0px;}
.p173{text-align: left;padding-left: 57px;margin-top: 0px;margin-bottom: 0px;}
.p174{text-align: left;padding-left: 71px;padding-right: 309px;margin-top: 0px;margin-bottom: 0px;text-indent: 118px;}
.p175{text-align: left;padding-left: 43px;padding-right: 476px;margin-top: 13px;margin-bottom: 0px;}
.p176{text-align: left;margin-top: 64px;margin-bottom: 0px;}
.p177{text-align: left;padding-left: 72px;margin-top: 19px;margin-bottom: 0px;}
.p178{text-align: left;padding-left: 58px;margin-top: 0px;margin-bottom: 0px;}
.p179{text-align: left;padding-left: 44px;margin-top: 1px;margin-bottom: 0px;}
.p180{text-align: left;padding-left: 44px;margin-top: 15px;margin-bottom: 0px;}
.p181{text-align: left;padding-left: 44px;margin-top: 14px;margin-bottom: 0px;}
.p182{text-align: left;padding-left: 30px;margin-top: 2px;margin-bottom: 0px;}
.p183{text-align: left;padding-left: 44px;padding-right: 329px;margin-top: 55px;margin-bottom: 0px;text-indent: -13px;}
.p184{text-align: left;padding-left: 44px;margin-top: 12px;margin-bottom: 0px;}
.p185{text-align: left;padding-left: 44px;margin-top: 2px;margin-bottom: 0px;}
.p186{text-align: left;padding-left: 253px;padding-right: 239px;margin-top: 0px;margin-bottom: 0px;text-indent: -180px;}
.p187{text-align: left;padding-left: 72px;margin-top: 0px;margin-bottom: 0px;}
.p188{text-align: left;padding-left: 44px;padding-right: 617px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p189{text-align: left;padding-left: 44px;padding-right: 274px;margin-top: 14px;margin-bottom: 0px;}
.p190{text-align: left;padding-left: 30px;margin-top: 16px;margin-bottom: 0px;}
.p191{text-align: justify;padding-left: 2px;padding-right: 95px;margin-top: 18px;margin-bottom: 0px;}
.p192{text-align: left;padding-right: 95px;margin-top: 7px;margin-bottom: 0px;text-indent: 2px;}
.p193{text-align: left;padding-left: 2px;padding-right: 336px;margin-top: 16px;margin-bottom: 0px;}
.p194{text-align: left;padding-left: 37px;margin-top: 16px;margin-bottom: 0px;}
.p195{text-align: left;padding-left: 65px;margin-top: 0px;margin-bottom: 0px;}
.p196{text-align: left;padding-left: 93px;padding-right: 309px;margin-top: 0px;margin-bottom: 0px;text-indent: -27px;}
.p197{text-align: left;padding-left: 37px;margin-top: 1px;margin-bottom: 0px;}
.p198{text-align: left;padding-left: 63px;padding-right: 476px;margin-top: 19px;margin-bottom: 0px;}
.p199{text-align: left;padding-left: 35px;margin-top: 0px;margin-bottom: 0px;}
.p200{text-align: left;padding-left: 63px;margin-top: 0px;margin-bottom: 0px;}
.p201{text-align: left;padding-left: 35px;padding-right: 309px;margin-top: 0px;margin-bottom: 0px;text-indent: 28px;}
.p202{text-align: left;padding-left: 63px;padding-right: 399px;margin-top: 0px;margin-bottom: 0px;}
.p203{text-align: justify;padding-right: 88px;margin-top: 16px;margin-bottom: 0px;}
.p204{text-align: justify;padding-right: 96px;margin-top: 7px;margin-bottom: 0px;}
.p205{text-align: justify;padding-right: 96px;margin-top: 16px;margin-bottom: 0px;}
.p206{text-align: left;padding-left: 28px;margin-top: 13px;margin-bottom: 0px;}
.p207{text-align: left;padding-left: 42px;margin-top: 2px;margin-bottom: 0px;}
.p208{text-align: left;padding-left: 42px;margin-top: 14px;margin-bottom: 0px;}
.p209{text-align: left;padding-left: 84px;padding-right: 295px;margin-top: 0px;margin-bottom: 0px;text-indent: 118px;}
.p210{text-align: left;padding-left: 42px;margin-top: 16px;margin-bottom: 0px;}
.p211{text-align: left;padding-left: 42px;padding-right: 329px;margin-top: 53px;margin-bottom: 0px;text-indent: -13px;}
.p212{text-align: left;padding-left: 42px;padding-right: 587px;margin-top: 13px;margin-bottom: 0px;}
.p213{text-align: left;padding-left: 251px;padding-right: 239px;margin-top: 0px;margin-bottom: 0px;text-indent: -180px;}
.p214{text-align: left;padding-left: 42px;padding-right: 274px;margin-top: 14px;margin-bottom: 0px;}
.p215{text-align: left;padding-left: 28px;margin-top: 16px;margin-bottom: 0px;}
.p216{text-align: left;margin-top: 6px;margin-bottom: 0px;}
.p217{text-align: left;padding-right: 343px;margin-top: 14px;margin-bottom: 0px;}
.p218{text-align: left;padding-left: 63px;padding-right: 378px;margin-top: 15px;margin-bottom: 0px;text-indent: -27px;}
.p219{text-align: justify;padding-left: 35px;padding-right: 504px;margin-top: 0px;margin-bottom: 0px;}
.p220{text-align: left;padding-left: 63px;padding-right: 190px;margin-top: 0px;margin-bottom: 0px;}
.p221{text-align: justify;padding-right: 95px;margin-top: 14px;margin-bottom: 0px;}
.p222{text-align: justify;padding-right: 93px;margin-top: 64px;margin-bottom: 0px;}
.p223{text-align: left;padding-left: 168px;margin-top: 314px;margin-bottom: 0px;}
.p224{text-align: justify;padding-right: 93px;margin-top: 11px;margin-bottom: 0px;}
.p225{text-align: justify;padding-right: 96px;margin-top: 5px;margin-bottom: 0px;}
.p226{text-align: justify;margin-top: 679px;margin-bottom: 0px;}
.p227{text-align: justify;padding-right: 95px;margin-top: 12px;margin-bottom: 0px;}
.p228{text-align: justify;padding-right: 95px;margin-top: 6px;margin-bottom: 0px;}
.p229{text-align: justify;padding-right: 95px;margin-top: 21px;margin-bottom: 0px;}
.p230{text-align: justify;padding-right: 93px;margin-top: 14px;margin-bottom: 0px;}
.p231{text-align: left;padding-left: 28px;padding-right: 518px;margin-top: 25px;margin-bottom: 0px;}
.p232{text-align: left;padding-left: 28px;padding-right: 518px;margin-top: 14px;margin-bottom: 0px;}
.p233{text-align: left;margin-top: 67px;margin-bottom: 0px;}
.p234{text-align: left;padding-left: 172px;margin-top: 317px;margin-bottom: 0px;}
.p235{text-align: left;padding-left: 42px;margin-top: 27px;margin-bottom: 0px;}
.p236{text-align: left;padding-left: 28px;padding-right: 601px;margin-top: 58px;margin-bottom: 0px;}
.p237{text-align: left;padding-left: 28px;padding-right: 560px;margin-top: 0px;margin-bottom: 0px;}
.p238{text-align: left;padding-right: 93px;margin-top: 19px;margin-bottom: 0px;}
.p239{text-align: justify;padding-right: 95px;margin-top: 61px;margin-bottom: 0px;}
.p240{text-align: justify;padding-right: 95px;margin-top: 54px;margin-bottom: 0px;}
.p241{text-align: right;padding-right: 15px;margin-top: 0px;margin-bottom: 0px;}
.p242{text-align: right;padding-right: 15px;margin-top: 9px;margin-bottom: 0px;}
.p243{text-align: right;padding-right: 15px;margin-top: 8px;margin-bottom: 0px;}
.p244{text-align: right;padding-right: 15px;margin-top: 67px;margin-bottom: 0px;}
.p245{text-align: left;padding-left: 57px;padding-right: 504px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p246{text-align: left;padding-left: 57px;padding-right: 504px;margin-top: 14px;margin-bottom: 0px;text-indent: -13px;}
.p247{text-align: left;padding-left: 29px;margin-top: 58px;margin-bottom: 0px;}
.p248{text-align: left;padding-left: 43px;padding-right: 622px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p249{text-align: left;padding-left: 29px;margin-top: 11px;margin-bottom: 0px;}
.p250{text-align: left;padding-left: 29px;margin-top: 1px;margin-bottom: 0px;}
.p251{text-align: left;padding-left: 43px;margin-top: 1px;margin-bottom: 0px;}
.p252{text-align: justify;padding-left: 1px;padding-right: 94px;margin-top: 19px;margin-bottom: 0px;}
.p253{text-align: left;margin-top: 4px;margin-bottom: 0px;}
.p254{text-align: justify;padding-right: 95px;margin-top: 22px;margin-bottom: 0px;}
.p255{text-align: justify;padding-left: 33px;padding-right: 96px;margin-top: 15px;margin-bottom: 0px;text-indent: -17px;}
.p256{text-align: justify;padding-left: 33px;padding-right: 95px;margin-top: 0px;margin-bottom: 0px;text-indent: -17px;}
.p257{text-align: left;padding-left: 33px;padding-right: 96px;margin-top: 8px;margin-bottom: 0px;text-indent: -17px;}
.p258{text-align: left;padding-left: 33px;padding-right: 96px;margin-top: 5px;margin-bottom: 0px;text-indent: -17px;}
.p259{text-align: left;padding-left: 33px;padding-right: 93px;margin-top: 5px;margin-bottom: 0px;text-indent: -17px;}
.p260{text-align: justify;padding-left: 33px;padding-right: 94px;margin-top: 5px;margin-bottom: 0px;text-indent: -17px;}
.p261{text-align: justify;padding-right: 96px;margin-top: 0px;margin-bottom: 0px;}
.p262{text-align: justify;padding-right: 95px;margin-top: 10px;margin-bottom: 0px;}
.p263{text-align: justify;padding-right: 96px;margin-top: 9px;margin-bottom: 0px;}
.p264{text-align: left;padding-left: 28px;margin-top: 30px;margin-bottom: 0px;}
.p265{text-align: left;padding-left: 56px;padding-right: 504px;margin-top: 1px;margin-bottom: 0px;text-indent: -13px;}
.p266{text-align: left;padding-left: 56px;padding-right: 504px;margin-top: 13px;margin-bottom: 0px;text-indent: -13px;}
.p267{text-align: left;padding-left: 56px;margin-top: 19px;margin-bottom: 0px;}
.p268{text-align: left;padding-left: 28px;margin-top: 58px;margin-bottom: 0px;}
.p269{text-align: left;padding-left: 42px;padding-right: 622px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p270{text-align: left;padding-left: 28px;margin-top: 12px;margin-bottom: 0px;}
.p271{text-align: justify;padding-right: 95px;margin-top: 0px;margin-bottom: 0px;}
.p272{text-align: left;padding-left: 28px;margin-top: 22px;margin-bottom: 0px;}
.p273{text-align: left;padding-left: 56px;padding-right: 504px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p274{text-align: left;padding-left: 56px;padding-right: 504px;margin-top: 14px;margin-bottom: 0px;text-indent: -13px;}
.p275{text-align: left;padding-left: 28px;padding-right: 560px;margin-top: 15px;margin-bottom: 0px;}
.p276{text-align: left;padding-left: 28px;padding-right: 511px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p277{text-align: left;padding-left: 28px;margin-top: 17px;margin-bottom: 0px;}
.p278{text-align: left;padding-right: 93px;margin-top: 0px;margin-bottom: 0px;}
.p279{text-align: left;padding-left: 56px;padding-right: 467px;margin-top: 0px;margin-bottom: 0px;}
.p280{text-align: left;padding-left: 42px;margin-top: 13px;margin-bottom: 0px;}
.p281{text-align: left;padding-left: 56px;padding-right: 515px;margin-top: 0px;margin-bottom: 0px;}
.p282{text-align: left;padding-left: 28px;padding-right: 406px;margin-top: 19px;margin-bottom: 0px;}
.p283{text-align: left;padding-left: 56px;padding-right: 483px;margin-top: 14px;margin-bottom: 0px;}
.p284{text-align: left;padding-left: 28px;padding-right: 364px;margin-top: 0px;margin-bottom: 0px;}
.p285{text-align: left;padding-left: 84px;padding-right: 455px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p286{text-align: left;padding-left: 56px;padding-right: 522px;margin-top: 15px;margin-bottom: 0px;}
.p287{text-align: left;padding-left: 28px;padding-right: 455px;margin-top: 0px;margin-bottom: 0px;}
.p288{text-align: justify;padding-right: 96px;margin-top: 20px;margin-bottom: 0px;}
.p289{text-align: justify;padding-right: 93px;margin-top: 58px;margin-bottom: 0px;}
.p290{text-align: right;padding-right: 16px;margin-top: 8px;margin-bottom: 0px;}
.p291{text-align: left;padding-left: 28px;margin-top: 19px;margin-bottom: 0px;}
.p292{text-align: left;padding-left: 42px;padding-right: 615px;margin-top: 0px;margin-bottom: 0px;}
.p293{text-align: left;padding-right: 476px;margin-top: 0px;margin-bottom: 0px;}
.p294{text-align: left;padding-left: 14px;margin-top: 0px;margin-bottom: 0px;}
.p295{text-align: justify;padding-right: 95px;margin-top: 33px;margin-bottom: 0px;}
.p296{text-align: left;padding-left: 28px;padding-right: 518px;margin-top: 34px;margin-bottom: 0px;}
.p297{text-align: left;padding-left: 42px;padding-right: 483px;margin-top: 0px;margin-bottom: 0px;}
.p298{text-align: left;padding-left: 28px;padding-right: 560px;margin-top: 58px;margin-bottom: 0px;}
.p299{text-align: left;padding-left: 42px;padding-right: 490px;margin-top: 0px;margin-bottom: 0px;}
.p300{text-align: left;padding-left: 42px;padding-right: 594px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p301{text-align: justify;margin-top: 18px;margin-bottom: 0px;}
.p302{text-align: left;padding-right: 96px;margin-top: 8px;margin-bottom: 0px;}
.p303{text-align: left;padding-left: 56px;padding-right: 322px;margin-top: 16px;margin-bottom: 0px;text-indent: -27px;}
.p304{text-align: left;padding-left: 84px;padding-right: 316px;margin-top: 14px;margin-bottom: 0px;}
.p305{text-align: left;padding-left: 112px;padding-right: 350px;margin-top: 0px;margin-bottom: 0px;}
.p306{text-align: left;padding-left: 139px;margin-top: 0px;margin-bottom: 0px;}
.p307{text-align: left;padding-left: 279px;margin-top: 0px;margin-bottom: 0px;}
.p308{text-align: left;padding-left: 77px;margin-top: 14px;margin-bottom: 0px;}
.p309{text-align: left;padding-left: 112px;margin-top: 1px;margin-bottom: 0px;}
.p310{text-align: left;padding-left: 84px;margin-top: 15px;margin-bottom: 0px;}
.p311{text-align: left;padding-left: 28px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p312{text-align: left;padding-left: 42px;margin-top: 12px;margin-bottom: 0px;}
.p313{text-align: left;padding-left: 42px;padding-right: 274px;margin-top: 13px;margin-bottom: 0px;}
.p314{text-align: justify;padding-right: 89px;margin-top: 20px;margin-bottom: 0px;}
.p315{text-align: justify;padding-right: 95px;margin-top: 4px;margin-bottom: 0px;}
.p316{text-align: justify;padding-right: 95px;margin-top: 5px;margin-bottom: 0px;}
.p317{text-align: justify;margin-top: 510px;margin-bottom: 0px;}
.p318{text-align: left;padding-left: 162px;margin-top: 486px;margin-bottom: 0px;}
.p319{text-align: left;margin-top: 37px;margin-bottom: 0px;}
.p320{text-align: left;padding-left: 16px;margin-top: 14px;margin-bottom: 0px;}
.p321{text-align: left;padding-left: 16px;margin-top: 2px;margin-bottom: 0px;}
.p322{text-align: justify;padding-left: 3px;padding-right: 96px;margin-top: 0px;margin-bottom: 0px;}
.p323{text-align: left;padding-left: 31px;padding-right: 483px;margin-top: 15px;margin-bottom: 0px;}
.p324{text-align: left;padding-left: 45px;padding-right: 518px;margin-top: 0px;margin-bottom: 0px;}
.p325{text-align: left;padding-left: 59px;margin-top: 0px;margin-bottom: 0px;}
.p326{text-align: left;margin-top: 19px;margin-bottom: 0px;}
.p327{text-align: left;padding-left: 28px;margin-top: 21px;margin-bottom: 0px;}
.p328{text-align: left;padding-left: 28px;padding-right: 546px;margin-top: 0px;margin-bottom: 0px;}
.p329{text-align: left;padding-left: 28px;padding-right: 587px;margin-top: 0px;margin-bottom: 0px;}
.p330{text-align: left;padding-left: 42px;padding-right: 525px;margin-top: 0px;margin-bottom: 0px;}
.p331{text-align: justify;margin-top: 0px;margin-bottom: 0px;}
.p332{text-align: left;padding-left: 28px;padding-right: 350px;margin-top: 15px;margin-bottom: 0px;}
.p333{text-align: left;padding-left: 42px;padding-right: 518px;margin-top: 0px;margin-bottom: 0px;}
.p334{text-align: left;padding-left: 28px;padding-right: 392px;margin-top: 58px;margin-bottom: 0px;}
.p335{text-align: justify;padding-right: 94px;margin-top: 0px;margin-bottom: 0px;}
.p336{text-align: left;padding-left: 112px;padding-right: 253px;margin-top: 20px;margin-bottom: 0px;text-indent: -83px;}
.p337{text-align: left;padding-left: 28px;padding-right: 427px;margin-top: 26px;margin-bottom: 0px;}
.p338{text-align: left;padding-left: 56px;padding-right: 448px;margin-top: 0px;margin-bottom: 0px;}
.p339{text-align: left;padding-left: 84px;padding-right: 546px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p340{text-align: left;padding-left: 98px;margin-top: 0px;margin-bottom: 0px;}
.p341{text-align: left;padding-left: 4px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p342{text-align: left;padding-left: 71px;padding-right: 596px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p343{text-align: left;padding-left: 43px;padding-right: 624px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p344{text-align: justify;padding-right: 93px;margin-top: 15px;margin-bottom: 0px;}
.p345{text-align: left;padding-left: 2px;margin-top: 0px;margin-bottom: 0px;}
.p346{text-align: justify;padding-left: 2px;padding-right: 95px;margin-top: 21px;margin-bottom: 0px;}
.p347{text-align: justify;padding-right: 93px;margin-top: 12px;margin-bottom: 0px;text-indent: 1px;}
.p348{text-align: left;padding-left: 30px;padding-right: 490px;margin-top: 17px;margin-bottom: 0px;}
.p349{text-align: left;padding-left: 44px;margin-top: 0px;margin-bottom: 0px;}
.p350{text-align: left;padding-left: 44px;padding-right: 448px;margin-top: 0px;margin-bottom: 0px;}
.p351{text-align: right;padding-right: 16px;margin-top: 65px;margin-bottom: 0px;}
.p352{text-align: left;padding-left: 56px;padding-right: 483px;margin-top: 15px;margin-bottom: 0px;}
.p353{text-align: center;padding-left: 28px;padding-right: 560px;margin-top: 55px;margin-bottom: 0px;}
.p354{text-align: left;padding-left: 42px;padding-right: 462px;margin-top: 2px;margin-bottom: 0px;}
.p355{text-align: left;padding-left: 42px;padding-right: 469px;margin-top: 0px;margin-bottom: 0px;}
.p356{text-align: left;padding-left: 56px;padding-right: 546px;margin-top: 0px;margin-bottom: 0px;}
.p357{text-align: left;padding-left: 56px;margin-top: 2px;margin-bottom: 0px;}
.p358{text-align: justify;padding-right: 96px;margin-top: 2px;margin-bottom: 0px;}
.p359{text-align: justify;padding-right: 96px;margin-top: 24px;margin-bottom: 0px;}
.p360{text-align: left;padding-left: 70px;padding-right: 309px;margin-top: 23px;margin-bottom: 0px;text-indent: -41px;}
.p361{text-align: left;padding-left: 126px;padding-right: 92px;margin-top: 0px;margin-bottom: 0px;text-indent: -27px;}
.p362{text-align: left;padding-left: 126px;margin-top: 0px;margin-bottom: 0px;}
.p363{text-align: left;padding-left: 153px;padding-right: 246px;margin-top: 0px;margin-bottom: 0px;text-indent: -27px;}
.p364{text-align: left;padding-left: 98px;padding-right: 441px;margin-top: 0px;margin-bottom: 0px;}
.p365{text-align: right;padding-right: 15px;margin-top: 68px;margin-bottom: 0px;}
.p366{text-align: right;padding-right: 15px;margin-top: 219px;margin-bottom: 0px;}
.p367{text-align: justify;padding-right: 96px;margin-top: 9px;margin-bottom: 0px;text-indent: 1px;}
.p368{text-align: left;padding-left: 29px;padding-right: 392px;margin-top: 21px;margin-bottom: 0px;}
.p369{text-align: left;padding-left: 43px;padding-right: 476px;margin-top: 0px;margin-bottom: 0px;}
.p370{text-align: left;padding-left: 43px;padding-right: 399px;margin-top: 0px;margin-bottom: 0px;}
.p371{text-align: left;padding-left: 57px;padding-right: 483px;margin-top: 0px;margin-bottom: 0px;}
.p372{text-align: left;padding-left: 29px;padding-right: 434px;margin-top: 57px;margin-bottom: 0px;}
.p373{text-align: left;padding-left: 43px;padding-right: 462px;margin-top: 1px;margin-bottom: 0px;}
.p374{text-align: left;padding-left: 43px;padding-right: 420px;margin-top: 0px;margin-bottom: 0px;}
.p375{text-align: left;padding-left: 57px;padding-right: 546px;margin-top: 0px;margin-bottom: 0px;}
.p376{text-align: justify;padding-left: 1px;padding-right: 96px;margin-top: 19px;margin-bottom: 0px;}
.p377{text-align: justify;padding-left: 1px;padding-right: 93px;margin-top: 0px;margin-bottom: 0px;}
.p378{text-align: left;padding-left: 29px;padding-right: 197px;margin-top: 20px;margin-bottom: 0px;}
.p379{text-align: left;padding-left: 43px;padding-right: 476px;margin-top: 1px;margin-bottom: 0px;}
.p380{text-align: left;padding-left: 57px;margin-top: 322px;margin-bottom: 0px;}
.p381{text-align: left;padding-left: 42px;margin-top: 26px;margin-bottom: 0px;}
.p382{text-align: left;padding-left: 56px;padding-right: 427px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p383{text-align: left;padding-left: 56px;padding-right: 483px;margin-top: 1px;margin-bottom: 0px;}
.p384{text-align: left;padding-left: 28px;padding-right: 239px;margin-top: 56px;margin-bottom: 0px;}
.p385{text-align: left;padding-left: 42px;padding-right: 462px;margin-top: 0px;margin-bottom: 0px;}
.p386{text-align: left;padding-left: 42px;padding-right: 420px;margin-top: 0px;margin-bottom: 0px;}
.p387{text-align: left;padding-right: 96px;margin-top: 20px;margin-bottom: 0px;}
.p388{text-align: left;padding-right: 96px;margin-top: 4px;margin-bottom: 0px;}
.p389{text-align: left;padding-left: 81px;margin-top: 318px;margin-bottom: 0px;}
.p390{text-align: justify;padding-right: 95px;margin-top: 20px;margin-bottom: 0px;}
.p391{text-align: justify;padding-right: 93px;margin-top: 61px;margin-bottom: 0px;}
.p392{text-align: left;padding-left: 28px;margin-top: 20px;margin-bottom: 0px;}
.p393{text-align: left;padding-left: 70px;padding-right: 567px;margin-top: 0px;margin-bottom: 0px;}
.p394{text-align: left;padding-left: 42px;margin-top: 19px;margin-bottom: 0px;}
.p395{text-align: left;padding-left: 42px;padding-right: 553px;margin-top: 0px;margin-bottom: 0px;}
.p396{text-align: left;padding-left: 42px;padding-right: 399px;margin-top: 14px;margin-bottom: 0px;}
.p397{text-align: left;padding-left: 42px;padding-right: 581px;margin-top: 14px;margin-bottom: 0px;}
.p398{text-align: left;padding-left: 42px;padding-right: 309px;margin-top: 0px;margin-bottom: 0px;}
.p399{text-align: left;padding-left: 42px;padding-right: 594px;margin-top: 1px;margin-bottom: 0px;}
.p400{text-align: justify;padding-right: 95px;margin-top: 13px;margin-bottom: 0px;}
.p401{text-align: justify;padding-right: 93px;margin-top: 20px;margin-bottom: 0px;}
.p402{text-align: justify;padding-right: 94px;margin-top: 3px;margin-bottom: 0px;}
.p403{text-align: left;padding-right: 96px;margin-top: 3px;margin-bottom: 0px;}
.p404{text-align: left;margin-top: 20px;margin-bottom: 0px;}
.p405{text-align: left;margin-top: 15px;margin-bottom: 0px;}
.p406{text-align: justify;padding-left: 28px;padding-right: 511px;margin-top: 20px;margin-bottom: 0px;}
.p407{text-align: left;padding-left: 42px;padding-right: 448px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p408{text-align: left;padding-left: 28px;margin-top: 15px;margin-bottom: 0px;}
.p409{text-align: left;padding-left: 28px;padding-right: 336px;margin-top: 0px;margin-bottom: 0px;}
.p410{text-align: left;padding-left: 42px;padding-right: 336px;margin-top: 0px;margin-bottom: 0px;}
.p411{text-align: justify;padding-right: 93px;margin-top: 22px;margin-bottom: 0px;text-indent: 1px;}
.p412{text-align: left;padding-left: 29px;padding-right: 476px;margin-top: 20px;margin-bottom: 0px;}
.p413{text-align: left;padding-left: 29px;margin-top: 56px;margin-bottom: 0px;}
.p414{text-align: left;padding-left: 1px;padding-right: 94px;margin-top: 20px;margin-bottom: 0px;}
.p415{text-align: left;margin-top: 63px;margin-bottom: 0px;}
.p416{text-align: justify;padding-right: 93px;margin-top: 10px;margin-bottom: 0px;}
.p417{text-align: left;padding-left: 28px;padding-right: 476px;margin-top: 14px;margin-bottom: 0px;}
.p418{text-align: left;padding-left: 28px;padding-right: 434px;margin-top: 0px;margin-bottom: 0px;}
.p419{text-align: left;padding-left: 28px;padding-right: 518px;margin-top: 49px;margin-bottom: 0px;}
.p420{text-align: left;padding-left: 28px;padding-right: 455px;margin-top: 18px;margin-bottom: 0px;}
.p421{text-align: left;padding-left: 28px;padding-right: 413px;margin-top: 1px;margin-bottom: 0px;}
.p422{text-align: left;padding-left: 28px;padding-right: 497px;margin-top: 51px;margin-bottom: 0px;}
.p423{text-align: left;padding-left: 28px;padding-right: 497px;margin-top: 0px;margin-bottom: 0px;}
.p424{text-align: left;padding-left: 28px;padding-right: 581px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p425{text-align: justify;padding-right: 2px;margin-top: 6px;margin-bottom: 0px;}
.p426{text-align: justify;padding-left: 1px;padding-right: 95px;margin-top: 12px;margin-bottom: 0px;}
.p427{text-align: left;padding-left: 29px;padding-right: 504px;margin-top: 18px;margin-bottom: 0px;}
.p428{text-align: left;padding-left: 57px;padding-right: 434px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p429{text-align: left;padding-left: 209px;margin-top: 434px;margin-bottom: 0px;}
.p430{text-align: left;padding-left: 28px;margin-top: 63px;margin-bottom: 0px;}
.p431{text-align: left;padding-left: 42px;padding-right: 581px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p432{text-align: left;padding-left: 28px;padding-right: 638px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p433{text-align: left;padding-left: 77px;margin-top: 336px;margin-bottom: 0px;}
.p434{text-align: left;padding-left: 42px;padding-right: 343px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p435{text-align: left;padding-left: 28px;padding-right: 385px;margin-top: 0px;margin-bottom: 0px;text-indent: 42px;}
.p436{text-align: left;padding-left: 70px;padding-right: 420px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p437{text-align: left;padding-left: 28px;margin-top: 54px;margin-bottom: 0px;}
.p438{text-align: left;padding-left: 28px;padding-right: 497px;margin-top: 1px;margin-bottom: 0px;}
.p439{text-align: left;padding-left: 42px;padding-right: 455px;margin-top: 1px;margin-bottom: 0px;}
.p440{text-align: center;padding-left: 42px;padding-right: 546px;margin-top: 0px;margin-bottom: 0px;}
.p441{text-align: left;padding-left: 70px;margin-top: 20px;margin-bottom: 0px;}
.p442{text-align: left;padding-left: 42px;padding-right: 624px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p443{text-align: right;padding-right: 44px;margin-top: 0px;margin-bottom: 0px;}
.p444{text-align: right;padding-right: 44px;margin-top: 9px;margin-bottom: 0px;}
.p445{text-align: right;padding-right: 44px;margin-top: 8px;margin-bottom: 0px;}
.p446{text-align: right;padding-right: 44px;margin-top: 67px;margin-bottom: 0px;}
.p447{text-align: left;padding-right: 281px;margin-top: 0px;margin-bottom: 0px;}
.p448{text-align: left;padding-left: 14px;padding-right: 343px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p449{text-align: left;padding-right: 385px;margin-top: 0px;margin-bottom: 0px;text-indent: 42px;}
.p450{text-align: left;padding-left: 42px;padding-right: 420px;margin-top: 0px;margin-bottom: 0px;text-indent: -13px;}
.p451{text-align: left;margin-top: 1px;margin-bottom: 0px;}
.p452{text-align: left;margin-top: 56px;margin-bottom: 0px;}
.p453{text-align: left;padding-right: 497px;margin-top: 2px;margin-bottom: 0px;}
.p454{text-align: left;padding-right: 525px;margin-top: 0px;margin-bottom: 0px;}
.p455{text-align: left;padding-left: 14px;padding-right: 455px;margin-top: 0px;margin-bottom: 0px;}
.p456{text-align: center;padding-left: 14px;padding-right: 546px;margin-top: 0px;margin-bottom: 0px;}
.p457{text-align: left;padding-left: 14px;padding-right: 624px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p458{text-align: justify;margin-top: 14px;margin-bottom: 0px;}
.p459{text-align: left;padding-left: 28px;padding-right: 281px;margin-top: 17px;margin-bottom: 0px;}
.p460{text-align: left;padding-left: 28px;margin-top: 56px;margin-bottom: 0px;}
.p461{text-align: left;padding-left: 28px;padding-right: 497px;margin-top: 2px;margin-bottom: 0px;}
.p462{text-align: left;padding-left: 28px;padding-right: 525px;margin-top: 0px;margin-bottom: 0px;}
.p463{text-align: left;padding-left: 42px;padding-right: 455px;margin-top: 0px;margin-bottom: 0px;}
.p464{text-align: left;padding-left: 42px;padding-right: 476px;margin-top: 0px;margin-bottom: 0px;}
.p465{text-align: justify;padding-right: 95px;margin-top: 380px;margin-bottom: 0px;}
.p466{text-align: left;margin-top: 45px;margin-bottom: 0px;}
.p467{text-align: left;margin-top: 14px;margin-bottom: 0px;}
.p468{text-align: justify;padding-right: 95px;margin-top: 11px;margin-bottom: 0px;}
.p469{text-align: left;padding-left: 28px;padding-right: 455px;margin-top: 20px;margin-bottom: 0px;}
.p470{text-align: justify;padding-left: 28px;padding-right: 385px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p471{text-align: left;padding-left: 28px;padding-right: 357px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p472{text-align: left;padding-left: 70px;margin-top: 19px;margin-bottom: 0px;}
.p473{text-align: left;padding-left: 28px;padding-right: 267px;margin-top: 14px;margin-bottom: 0px;}
.p474{text-align: left;padding-left: 28px;padding-right: 385px;margin-top: 0px;margin-bottom: 0px;text-indent: 14px;}
.p475{text-align: left;padding-left: 28px;padding-right: 497px;margin-top: 56px;margin-bottom: 0px;}
.p476{text-align: left;padding-left: 42px;padding-right: 316px;margin-top: 0px;margin-bottom: 0px;}
.p477{text-align: left;padding-left: 16px;margin-top: 60px;margin-bottom: 0px;}
.p478{text-align: left;padding-left: 16px;margin-top: 0px;margin-bottom: 0px;}
.p479{text-align: left;padding-left: 16px;margin-top: 1px;margin-bottom: 0px;}
.p480{text-align: left;padding-left: 16px;margin-top: 3px;margin-bottom: 0px;}

.td0{padding: 0px;margin: 0px;width: 14px;vertical-align: bottom;}
.td1{padding: 0px;margin: 0px;width: 588px;vertical-align: bottom;}
.td2{padding: 0px;margin: 0px;width: 24px;vertical-align: bottom;}
.td3{padding: 0px;margin: 0px;width: 602px;vertical-align: bottom;}
.td4{padding: 0px;margin: 0px;width: 23px;vertical-align: bottom;}
.td5{padding: 0px;margin: 0px;width: 20px;vertical-align: bottom;}
.td6{padding: 0px;margin: 0px;width: 582px;vertical-align: bottom;}
.td7{padding: 0px;margin: 0px;width: 409px;vertical-align: bottom;}
.td8{padding: 0px;margin: 0px;width: 215px;vertical-align: bottom;}
.td9{padding: 0px;margin: 0px;width: 406px;vertical-align: bottom;}
.td10{padding: 0px;margin: 0px;width: 218px;vertical-align: bottom;}
.td11{padding: 0px;margin: 0px;width: 481px;vertical-align: bottom;}
.td12{padding: 0px;margin: 0px;width: 143px;vertical-align: bottom;}
.td13{padding: 0px;margin: 0px;width: 423px;vertical-align: bottom;}
.td14{padding: 0px;margin: 0px;width: 201px;vertical-align: bottom;}
.td15{padding: 0px;margin: 0px;width: 444px;vertical-align: bottom;}
.td16{padding: 0px;margin: 0px;width: 180px;vertical-align: bottom;}
.td17{padding: 0px;margin: 0px;width: 501px;vertical-align: bottom;}
.td18{padding: 0px;margin: 0px;width: 123px;vertical-align: bottom;}
.td19{padding: 0px;margin: 0px;width: 105px;vertical-align: bottom;}
.td20{padding: 0px;margin: 0px;width: 396px;vertical-align: bottom;}
.td21{padding: 0px;margin: 0px;width: 410px;vertical-align: bottom;}
.td22{padding: 0px;margin: 0px;width: 214px;vertical-align: bottom;}
.td23{padding: 0px;margin: 0px;width: 205px;vertical-align: bottom;}
.td24{padding: 0px;margin: 0px;width: 234px;vertical-align: bottom;}
.td25{padding: 0px;margin: 0px;width: 126px;vertical-align: bottom;}
.td26{padding: 0px;margin: 0px;width: 21px;vertical-align: bottom;}
.td27{padding: 0px;margin: 0px;width: 60px;vertical-align: bottom;}
.td28{padding: 0px;margin: 0px;width: 153px;vertical-align: bottom;}
.td29{padding: 0px;margin: 0px;width: 456px;vertical-align: bottom;}
.td30{padding: 0px;margin: 0px;width: 168px;vertical-align: bottom;}
.td31{padding: 0px;margin: 0px;width: 464px;vertical-align: bottom;}
.td32{padding: 0px;margin: 0px;width: 160px;vertical-align: bottom;}

.tr0{height: 25px;}
.tr1{height: 21px;}
.tr2{height: 24px;}
.tr3{height: 38px;}
.tr4{height: 23px;}
.tr5{height: 37px;}
.tr6{height: 29px;}
.tr7{height: 28px;}
.tr8{height: 36px;}
.tr9{height: 16px;}
.tr10{height: 27px;}
.tr11{height: 18px;}

.t0{width: 626px;margin-top: 71px;font: 13px 'Gabriola';}
.t1{width: 625px;font: 13px 'Gabriola';}
.t2{width: 624px;font: 13px 'Arial';}
.t3{width: 624px;margin-left: 20px;font: 13px 'Arial';}
.t4{width: 624px;margin-left: 1px;font: 13px 'Arial';}
.t5{width: 624px;margin-left: 2px;font: 13px 'Arial';}
.t6{width: 565px;margin-left: 99px;margin-top: 5px;font: 13px 'Arial';}

</STYLE>
</HEAD>

<BODY>
<DIV id="page_1">

<DIV class="dclr"></DIV>
<DIV id="id_1">
<P class="p0 ft0">OpenACC Programming and Best Practices Guide</P>
<P class="p1 ft1">June 2015</P>
</DIV>
<DIV id="id_2">
<P class="p2 ft2">© 2015 <NOBR>openacc-standard.org.</NOBR> All Rights Reserved.</P>
</DIV>
</DIV>
<DIV id="page_2">


<DIV id="id_1">
<P class="p2 ft3">Contents</P>
<TABLE cellpadding=0 cellspacing=0 class="t0">
<TR>
	<TD class="tr0 td0"><P class="p3 ft4"><A href="#page_4">1</A></P></TD>
	<TD class="tr0 td1"><P class="p4 ft4"><A href="#page_4">Introduction</A></P></TD>
	<TD class="tr0 td2"><P class="p5 ft4">3</P></TD>
</TR>
<TR>
	<TD class="tr1 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr1 td1"><P class="p4 ft6"><A href="#page_4">Writing Portable Code </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr1 td2"><P class="p5 ft6">3</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_6">What is OpenACC? </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft7">5</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_7">Accelerating an Application with OpenACC </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft7">6</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_9">Case Study - Jacobi Iteration </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft7">8</P></TD>
</TR>
<TR>
	<TD class="tr3 td0"><P class="p3 ft4"><A href="#page_12">2</A></P></TD>
	<TD class="tr3 td1"><P class="p4 ft4"><A href="#page_12">Assess Application Performance</A></P></TD>
	<TD class="tr3 td2"><P class="p5 ft4">11</P></TD>
</TR>
<TR>
	<TD class="tr4 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr4 td1"><P class="p4 ft7"><A href="#page_12">Baseline Profiling </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr4 td2"><P class="p8 ft7">11</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_13">Additional Profiling </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft7">12</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_13">Case Study - Analysis </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft8">12</P></TD>
</TR>
<TR>
	<TD class="tr5 td0"><P class="p3 ft4"><A href="#page_16">3</A></P></TD>
	<TD class="tr5 td1"><P class="p4 ft4"><A href="#page_16">Parallelize Loops</A></P></TD>
	<TD class="tr5 td2"><P class="p5 ft4">15</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_16">The Kernels Construct </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft7">15</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_17">The Parallel Construct </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft8">16</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_18">Diﬀerences Between Parallel and Kernels </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft8">17</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_19">The Loop Construct </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft8">18</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_20">Routine Directive </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft8">19</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_21">Case Study - Parallelize </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft8">20</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_27">Atomic Operations </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft7">26</P></TD>
</TR>
<TR>
	<TD class="tr5 td0"><P class="p3 ft4"><A href="#page_29">4</A></P></TD>
	<TD class="tr5 td1"><P class="p4 ft4"><A href="#page_29">Optimize Data Locality</A></P></TD>
	<TD class="tr5 td2"><P class="p5 ft4">28</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft8"><A href="#page_29">Data Regions </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft8">28</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_30">Data Clauses </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p5 ft7">29</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_33">Unstructured Data Lifetimes </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft7">32</P></TD>
</TR>
<TR>
	<TD class="tr2 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td1"><P class="p4 ft7"><A href="#page_35">Update Directive </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td2"><P class="p7 ft7">34</P></TD>
</TR>
<TR>
	<TD class="tr4 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr4 td1"><P class="p4 ft7"><A href="#page_36">Best Practice: Offload Inefficient Operations to Maintain Data Locality </A>. . . . . . . . . . . . . . .</P></TD>
	<TD class="tr4 td2"><P class="p7 ft7">35</P></TD>
</TR>
<TR>
	<TD class="tr6 td0"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr6 td1"><P class="p4 ft8"><A href="#page_37">Case Study - Optimize Data Locality </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr6 td2"><P class="p5 ft8">36</P></TD>
</TR>
</TABLE>
</DIV>
<DIV id="id_2">
<P class="p2 ft8">1</P>
</DIV>
</DIV>
<DIV id="page_3">


<TABLE cellpadding=0 cellspacing=0 class="t1">
<TR>
	<TD colspan=2 class="tr7 td3"><P class="p6 ft9">CONTENTS</P></TD>
	<TD class="tr7 td4"><P class="p7 ft7">2</P></TD>
</TR>
<TR>
	<TD class="tr5 td5"><P class="p6 ft4"><A href="#page_40">5</A></P></TD>
	<TD class="tr5 td6"><P class="p6 ft4"><A href="#page_40">Optimize Loops</A></P></TD>
	<TD class="tr5 td4"><P class="p7 ft4">39</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft8"><A href="#page_40">Eﬃcient Loop Ordering </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p8 ft8">39</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft8"><A href="#page_40">OpenACC’s 3 Levels of Parallelism </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p8 ft8">39</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><A href="#page_41">Mapping Parallelism to the Hardware </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p8 ft7">40</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><A href="#page_43">Collapse Clause </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p7 ft7">42</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><A href="#page_44">Routine Parallelism </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p7 ft7">43</P></TD>
</TR>
<TR>
	<TD class="tr4 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr4 td6"><P class="p6 ft7"><A href="#page_44">Case Study - Optimize Loops </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr4 td4"><P class="p7 ft7">43</P></TD>
</TR>
<TR>
	<TD class="tr3 td5"><P class="p6 ft4"><A href="#page_49">6</A></P></TD>
	<TD class="tr3 td6"><P class="p6 ft4"><A href="#page_49">OpenACC Interoperability</A></P></TD>
	<TD class="tr3 td4"><P class="p7 ft4">48</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><A href="#page_49">The Host Data Region </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p7 ft7">48</P></TD>
</TR>
<TR>
	<TD class="tr4 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr4 td6"><P class="p6 ft7"><A href="#page_50">Using Device Pointers </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr4 td4"><P class="p8 ft7">49</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><A href="#page_51">Obtaining Device and Host Pointer Addresses </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p8 ft7">50</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><A href="#page_51">Additional Vendor-Specific</A></NOBR><A href="#page_51"> Interoperability Features </A>. . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p8 ft7">50</P></TD>
</TR>
<TR>
	<TD class="tr3 td5"><P class="p6 ft4"><A href="#page_53">7</A></P></TD>
	<TD class="tr3 td6"><P class="p6 ft4"><A href="#page_53">Advanced OpenACC Features</A></P></TD>
	<TD class="tr3 td4"><P class="p7 ft4">52</P></TD>
</TR>
<TR>
	<TD class="tr4 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr4 td6"><P class="p6 ft7"><A href="#page_53">Asynchronous Operation </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr4 td4"><P class="p8 ft7">52</P></TD>
</TR>
<TR>
	<TD class="tr2 td5"><P class="p6 ft5">&nbsp;</P></TD>
	<TD class="tr2 td6"><P class="p6 ft7"><NOBR><A href="#page_60">Multi-device Programming </A>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</P></TD>
	<TD class="tr2 td4"><P class="p7 ft7">59</P></TD>
</TR>
<TR>
	<TD class="tr3 td5"><P class="p6 ft4"><A href="#page_64">A</A></P></TD>
	<TD class="tr3 td6"><P class="p6 ft4"><A href="#page_64">References</A></P></TD>
	<TD class="tr3 td4"><P class="p7 ft4">63</P></TD>
</TR>
</TABLE>
</DIV>
<DIV id="page_4">


<DIV id="id_1">
<P class="p2 ft10">Chapter 1</P>
<P class="p9 ft3">Introduction</P>
<P class="p10 ft7">This guide presents methods and best practices for accelerating applications in an incremental, performance portable way. Although some of the examples may show results using a given compiler or accelerator, the information presented in this document is intended to address all architectures both available at publication time and well into the future. Readers should be comfortable with C, C++, or Fortran, but do not need experience with parallel programming or accelerated computing, although such experience will be helpful.</P>
<P class="p11 ft11">Writing Portable Code</P>
<P class="p12 ft8">The current computing landscape is spotted with a variety of computing architectures: <NOBR>multi-core</NOBR> CPUs, GPUs, <NOBR>many-core</NOBR> devices, DSPs, and FPGAs, to name a few. It is now commonplace to find not just one, but several of these diﬀering architectures within the same machine. Programmers must make portability of their code a forethought, otherwise they risk locking their application to a single architecture, which may limit the ability to run on future architectures. Although the variety of architectures may seem daunting to the programmer, closer analysis reveals trends that show a lot in common between them. The ﬁrst thing to note is that all of these architectures are moving in the direction of more parallelism. CPUs are not only adding CPU cores but also expanding the length of their SIMD operations. GPUs have grown to require a high degree of block and SIMT parallelism. It is clear that going forward all architectures will require a signiﬁcant degree of parallelism in order to achieve high performance. Modern processors need not only a large amount of parallelism, but frequently expose multiple levels of parallelism with varying degrees of coarseness. The next thing to notice is that all of these architectures have exposed hierarchies of memory. CPUs have the main system memory, typically DDR, and multiple layers of cache memory. GPUs have the main CPU memory, the main GPU memory, and various degrees of cache or scratchpad memory. Additionally on hybrid architectures, which include two or more diﬀerent architectures, there exist machines where the two architectures have completely separate memories, some with physically separate but logically the same memory, and some with fully shared memory.</P>
<P class="p13 ft8">Because of these complexities, it’s important that developers choose a programming model that balances the need for portability with the need for performance. Below are four programming models of varying degrees of both portability and performance. In a real application it’s frequently best to use a mixture of approaches to ensure a good balance between high portability and performance.</P>

<P class="p15 ft13">Standard (and defacto standard) libraries provide the highest degree of portability because the programmer can frequently replace only the library used without even changing the source code itself when changing compute architectures. Since many hardware vendors provide <NOBR>highly-tuned</NOBR> versions of common libraries,</P>
</DIV>

</DIV>
<DIV id="page_5">


<P class="p16 ft14">using libraries can also result in very high performance. Although libraries can provide both high portability and high performance, few applications are able to use only libraries because of their limited scope.</P>
<P class="p17 ft8">Some vendors provide additional libraries as a <NOBR>value-added</NOBR> for their platform, but which implement non- standard APIs. These libraries provide high performance, but little portability. Fortunately because libraries provide modular APIs, the impact of using <NOBR>non-portable</NOBR> libraries can be isolated to limit the impact on overall application portability.</P>

<P class="p18 ft8">Many standard programming languages either have or are beginning to adopt features for parallel programming. For example, Fortran 2008 added support for <SPAN class="ft9">do concurrent</SPAN>, which exposes the parallelism within that loop. Adoption of these language features is often slow, however, and many standard languages are only now beginning to discuss parallel programming features for future language releases. When these features become commonplace, they will provide high portability, since they are part of a standard language, and if <NOBR>well-designed</NOBR> can provide high performance as well.</P>

<P class="p20 ft7">When standard programming languages lack support for necessary features compiler directives can provide additional functionality. Directives, in the form of pragmas in C/C++ and comments in Fortran, provide additional information to compilers on how to build and/or optimize the code. Most compilers support their own directives, but also directives such as OpenACC and OpenMP, which are backed by industry groups and implemented by a range of compilers. When using <NOBR>industry-backed</NOBR> compiler directives the programmer can write code with a high degree of portability across compilers and architectures. Frequently, however, these compiler directives are written to remain very high level, both for simplicity and portability, meaning that performance may lag <NOBR>lower-level</NOBR> programming paradigms. Many developers are willing to give up <NOBR>10-20%</NOBR> of <NOBR>hand-tuned</NOBR> performance in order to get a high degree of portability to other architectures and to enhance programmer productivity. The tolerance for this portability/performance <NOBR>trade-oﬀ</NOBR> will vary according to the needs of the programmer and application.</P>

<P class="p22 ft8">CUDA and OpenCL are examples of extensions to existing programming languages to give additional parallel programming capabilities. Code written in these languages is frequently at a lower level than that of other options, but as a result can frequently achieve higher performance. Lower level architectural details are exposed and the way that a problem is decomposed to the hardware must be explicitly managed with these languages. This is the best option when performance goals outweigh portability, as the <NOBR>low-level</NOBR> nature of these programming languages frequently makes the resulting code less portable. Good software engineering practices can reduce the impact these languages have on portability.</P>
<P class="p23 ft8">There is no one programming model that ﬁts all needs. An application developer needs to evaluate the priorities of the project and make decisions accordingly. A best practice is to begin with the most portable and productive programming models and move to lower level programming models only as needed and in a modular fashion. In doing so the programmer can accelerate much of the application very quickly, which is often more beneﬁcial than attempting to get the absolute highest performance out of a particular routine before moving to the next. When development time is limited, focusing on accelerating as much of the application as possible is generally more productive than focusing solely on the top time consuming routine.</P>
</DIV>
<DIV id="page_6">


<DIV id="id_1">
<P class="p24 ft11">What is OpenACC?</P>
<P class="p25 ft14">With the emergence of GPU and <NOBR>many-core</NOBR> architectures in high performance computing, programmers desire the ability to program using a familiar, high level programming model that provides both high performance and portability to a wide range of computing architectures. OpenACC emerged in 2011 as a programming model that uses <NOBR>high-level</NOBR> compiler directives to expose parallelism in the code and parallelizing compilers to build the code for a variety of parallel accelerators. This document is intended as a best practices guide for accelerating an application using OpenACC to give both good performance and portability to other devices.</P>

<P class="p27 ft14">In order to ensure that OpenACC would be portable to all computing architectures available at the time of its inception and into the future, OpenACC deﬁnes an abstract model for accelerated computing. This model exposes multiple levels of parallelism that may appear on a processor as well as a hierarchy of memories with varying degrees of speed and addressibility. The goal of this model is to ensure that OpenACC will be applicable to more than just a particular architecture or even just the architectures in wide availability at the time, but to ensure that OpenACC could be used on future devices as well.</P>
<P class="p28 ft8">At its core OpenACC supports oﬄoading of both computation and data from a <SPAN class="ft15">host </SPAN>device to an <SPAN class="ft15">accelerator </SPAN>device. In fact, these devices may be the same or may be completely diﬀerent architectures, such as the case of a CPU host and GPU accelerator. The two devices may also have separate memory spaces or a single memory space. In the case that the two devices have diﬀerent memories the OpenACC compiler and runtime will analyze the code and handle any accelerator memory management and the transfer of data between host and device memory. Figure 1.1 shows a high level diagram of the OpenACC abstract accelerator, but remember that the devices and memories may be physically the same on some architectures.</P>
</DIV>

</DIV>
<DIV id="page_7">
<DIV id="p7dimg1">


<P class="p29 ft8">More details of OpenACC’s abstract accelerator model will be presented throughout this guide when they are pertinent.</P>
<P class="p30 ft8"><SPAN class="ft16">Best Practice: </SPAN>For developers coming to OpenACC from other accelerator programming models, such as CUDA or OpenCL, where host and accelerator memory is frequently represented by two distinct variables (<SPAN class="ft9">host_A[] </SPAN>and <SPAN class="ft9">device_A[]</SPAN>, for instance), it’s important to remember that when using OpenACC a variable should be thought of as a single object, regardless of whether the it’s backed by memory in one or more memory spaces. If one assumes that a variable represents two separate memories, depending on where it is used in the program, then it is possible to write programs that access the variable in unsafe ways, resulting in code that would not be portable to devices that share a single memory between the host and device. As with any parallel or asynchronous programming paradigm, accessing the same variable from two sections of code simultaneously could result in a race condition that produces inconsistent results. By assuming that you are always accessing a single variable, regardless of how it is stored in memory, the programmer will avoid making mistakes that could cost a signiﬁcant amount of eﬀort to debug.</P>

<P class="p32 ft6">OpenACC is designed to be a <NOBR>high-level,</NOBR> platform independent language for programming accelerators. As such, one can develop a single source code that can be run on a range of devices and achieve good performance. The simplicity and portability that OpenACC’s programming model provides sometimes comes at a cost to performance. The OpenACC abstract accelerator model deﬁnes a least common denominator for accelerator devices, but cannot represent architectural speciﬁcs of these devices without making the language less portable. There will always be some optimizations that are possible in a <NOBR>lower-level</NOBR> programming model, such as CUDA or OpenCL, that cannot be represented at a high level. For instance, although OpenACC has the <SPAN class="ft9">cache </SPAN>directive, some uses of <SPAN class="ft15">shared memory </SPAN>on NVIDIA GPUs are more easily represented using CUDA. The same is true for any host or device: certain optimizations are too <NOBR>low-level</NOBR> for a <NOBR>high-level</NOBR> approach like OpenACC. It is up to the developers to determine the cost and beneﬁt of selectively using a lower level programming language for performance critical sections of code. In cases where performance is too critical to take a <NOBR>high-level</NOBR> approach, it’s still possible to use OpenACC for much of the application, while using another approach in certain places, as will be discussed in a later chapter on interoperability.</P>



<P class="p33 ft11">Accelerating an Application with OpenACC</P>
<P class="p34 ft8">This section will detail an incremental approach to accelerating an application using OpenACC. When taking this approach it is beneﬁcial to revisit each step multiple times, checking the results of each step for correctness. Working incrementally will limit the scope of each change for improved productivity and debugging.</P>

<P class="p36 ft8">This guide will introduce OpenACC directives incrementally, as they become useful for the porting process. All OpenACC directives have a common syntax, however, with the <SPAN class="ft9">acc </SPAN>sentinal, designating to the compiler that the text that follows will be OpenACC, a directive, and clauses to that directive, many of which are optional but provide the compiler with additional information.</P>
<P class="p37 ft8">In C and C++, these directives take the form of a pragma. The example code below shows the OpenACC <SPAN class="ft9">kernels </SPAN>directive without any additional clauses.</P>
<!-- <P class="p38 ft19"><SPAN class="ft17">1</SPAN><SPAN class="ft18">#pragma acc kernels</SPAN></P> -->
</DIV>
<DIV id="page_8">


<P class="p39 ft8">In Fortran, the directives take the form of a special comment, as demonstrated below.</P>

<P class="p41 ft9"><SPAN class="ft22">Some OpenACC directives apply to structured blocks of code, while others are executable statements. In C and C++ a block of code can be represented by curly braces (</SPAN>{ <SPAN class="ft22">and </SPAN>}<SPAN class="ft22">). In Fortran a block of code will begin with an OpenACC directive (</SPAN>!$acc kernels<SPAN class="ft22">) and end with a matching ending directive (</SPAN>!$acc end</P>
<P class="p42 ft9">kernels<SPAN class="ft8">).</SPAN></P>

<P class="p44 ft8">Programmers should take an incremental approach to accelerating applications using OpenACC to ensure correctness. This guide will follow the approach of ﬁrst assessing application performance, then using OpenACC to parallelize important loops in the code, next optimizing data locality to remove unnecessary data migrations between the host and accelerator, and ﬁnally optimizing loops within the code to maximize performance on a given architecture. This approach has been successful in many applications because it prioritizes changes that are likely to provide the greatest returns so that the programmer can quickly and productively achieve the acceleration.</P>
<P class="p45 ft8">There are two important things to note before detailing each step. First, at times during this process application performance may actually slow down. Developers should not become frustrated if their initial eﬀorts result in a loss of performance. As will be explained later, this is generally the result of implicit data movement between the host and accelerator, which will be optimized as a part of the porting cycle. Second, it is critical that developers check the program results for correctness after each change. Frequent correctness checks will save a lot of debugging eﬀort, since errors can be found and ﬁxed immediately, before they have the chance to compound. Some developers may find it beneﬁcial to use a source version control tool to snapshot the code after each successful change so that any breaking changes can be quickly thrown away and the code returned to a known good state.</P>

<P class="p47 ft8">Before one can begin to accelerate an application it is important to understand in which routines and loops an application is spending the bulk of its time and why. It is critical to understand the most <NOBR>time-consuming</NOBR> parts of the application to maximize the beneﬁt of acceleration. Amdahl’s Law informs us that the <NOBR>speed-up</NOBR> achievable from running an application on a parallel accelerator will be limited by the remaining serial code. In other words, the application will see the most beneﬁt by accelerating as much of the code as possible and by prioritizing the most <NOBR>time-consuming</NOBR> parts. A variety of tools may be used to identify important parts of the code, including simple application timers.</P>

<P class="p49 ft8">Once important regions of the code have been identiﬁed, OpenACC directives should be used to accelerate these regions on the target device. Parallel loops within the code should be decorated with OpenACC directives to provide OpenACC compilers the information necessary to parallelize the code for the target architecture.</P>

<P class="p50 ft13">Because many accelerated architectures, such as CPU + GPU architectures, use distinct memory spaces for the <SPAN class="ft15">host </SPAN>and <SPAN class="ft15">device </SPAN>it is necessary for the compiler to manage data in both memories and move the data between the two memories to ensure correct results. Compilers rarely have full knowledge of the application,</P>
</DIV>
<DIV id="page_9">


<P class="p51 ft7">so they must be cautious in order to ensure correctness, which often involves copying data to and from the accelerator more often than is actually necessary. The programmer can give the compiler additional information about how to manage the memory so that it remains local to the accelerator as long as possible and is only moved between the two memories when absolutely necessary. Programmers will often realize the largest performance gains after optimizing data movement during this step.</P>

<P class="p20 ft8">Compilers will make decisions about how to map the parallelism in the code to the target accelerator based on internal heuristics and the limited knowledge it has about the application. Sometimes additional performance can be gained by providing the compiler with more information so that it can make better decisions on how to map the parallelism to the accelerator. When coming from a traditional CPU architecture to a more parallel architecture, such as a GPU, it may also be necessary to restructure loops to expose additional parallelism for the accelerator or to reduce the frequency of data movement. Frequently code refactoring that was motivated by improving performance on parallel accelerators is beneﬁcial to traditional CPUs as well.</P>
<P class="p53 ft8">This process is by no means the only way to accelerate using OpenACC, but it has been proven successful in numerous applications. Doing the same steps in diﬀerent orders may cause both frustration and diﬃculty debugging, so it’s advisable to perform each step of the process in the order shown above.</P>

<P class="p22 ft8">Many applications have been written with little or even no parallelism exposed in the code. The applications that do expose parallelism frequently do so in a <NOBR>coarse-grained</NOBR> manner, where a small number of threads or processes execute for a long time and compute a signiﬁcant amount work each. Modern GPUs and <NOBR>many-core</NOBR> processors, however, are designed to execute <NOBR>ﬁne-grained</NOBR> threads, which are <NOBR>short-lived</NOBR> and execute a minimal amount of work each. These parallel architectures achieve high throughput by trading <NOBR>single-threaded</NOBR> performance in favor of several orders in magnitude more parallelism. This means that when accelerating an application with OpenACC, which was designed in light of increased hardware parallelism, it may be necessary to refactor the code to favor <NOBR>tightly-nested</NOBR> loops with a signiﬁcant amount of data reuse. In many cases these same code changes also beneﬁt more traditional CPU architectures as well by improving cache use and vectorization.</P>
<P class="p55 ft7">OpenACC may be used to accelerate applications on devices that have a discrete memory or that have a memory space that’s shared with the host. Even on devices that utilize a shared memory there is frequently still a hierarchy of a fast, close memory for the accelerator and a larger, slower memory used by the host. For this reason it is important to structure the application code to maximize reuse of arrays regardless of whether the underlying architecture uses discrete or uniﬁed memories. When refactoring the code for use with OpenACC it is frequently beneﬁcial to assume a discrete memory, even if the device you are developing on has a uniﬁed memory. This forces data locality to be a primary consideration in the refactoring and will ensure that the resulting code exploits hierarchical memories and is portable to a wide range of devices.</P>


<P class="p54 ft11">Case Study - Jacobi Iteration</P>
<P class="p18 ft6">Throughout this guide we will use simple applications to demonstrate each step of the acceleration process. The ﬁrst such application will solve the <NOBR>2D-Laplace</NOBR> equation with the iterative Jacobi solver. Iterative methods are a common technique to approximate the solution of elliptic PDEs, like the <NOBR>2D-Laplace</NOBR> equation, within some allowable tolerance. In the case of our example we will perform a simple stencil calculation where each point calculates it value as the mean of its neighbors’ values. The calculation will continue to</P>
</DIV>
<DIV id="page_10">


<P class="p58 ft23">iterate until either the maximum change in value between two iterations drops below some tolerance level or a maximum number of iterations is reached. For the sake of consistent comparison through the document the examples will always iterate 1000 times. The main iteration loop for both C/C++ and Fortran appears below.</P>
<!-- </DIV>
</DIV> -->
</DIV>
<DIV id="page_11">

<DIV id="id_2">
<DIV id="id_2_2">

<P class="p76 ft7">The outermost loop in each example will be referred to as the <SPAN class="ft15">convergence loop</SPAN>, since it loops until the answer has converged by reaching some maximum error tolerance or number of iterations. Notice that whether or not a loop iteration occurs depends on the error value of the previous iteration. Also, the values for each element of <SPAN class="ft9">A </SPAN>is calculated based on the values of the previous iteration, known as a data dependency. These two facts mean that this loop cannot be run in parallel.</P>
<P class="p77 ft8">The ﬁrst loop nest within the convergence loop calculates the new value for each element based on the current values of its neighbors. Notice that it is necessary to store this new value into a diﬀerent array. If each iteration stored the new value back into itself then a data dependency would exist between the data elements, as the order each element is calculated would aﬀect the ﬁnal answer. By storing into a temporary array we ensure that all values are calculated using the current state of <SPAN class="ft9">A </SPAN>before <SPAN class="ft9">A </SPAN>is updated. As a result, each loop iteration is completely independent of each other iteration. These loop iterations may safely be run in any order or in parallel and the ﬁnal result would be the same. This loop also calculates a maximum error value. The error value is the diﬀerence between the new value and the old. If the maximum amount of change between two iterations is within some tolerance, the problem is considered converged and the outer loop will exit.</P>
<P class="p78 ft8">The second loop nest simply updates the value of <SPAN class="ft9">A </SPAN>with the values calculated into <SPAN class="ft9">Anew</SPAN>. If this is the last iteration of the convergence loop, <SPAN class="ft9">A </SPAN>will be the ﬁnal, converged value. If the problem has not yet converged, then <SPAN class="ft9">A </SPAN>will serve as the input for the next iteration. As with the above loop nest, each iteration of this loop nest is independent of each other and is safe to parallelize.</P>
<P class="p79 ft8">In the coming sections we will accelerate this simple application using the method described in this document.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_12">


<DIV id="id_1">
<!-- <P class="p2 ft10">Chapter 2</P> -->
<P class="p80 ft3">Assess Application Performance</P>
<P class="p81 ft8">A variety of tools can be used to evaluate application performance and which are available will depend on your development environment. From simple application timers to graphical performance analyzers, the choice of performance analysis tool is outside of the scope of this document. The purpose of this section is to provide guidance on choosing important sections of code for acceleration, which is independent of the profiling tools available.</P>
<P class="p82 ft8">Because this document is focused on OpenACC, the PGProf tool, which is provided with the PGI OpenACC compiler will be used for CPU profiling. When accelerator profiling is needed, the application will be run on an Nvidia GPU and the Nvidia Visual Proﬁler will be used.</P>
<P class="p83 ft11">Baseline Profiling</P>
<P class="p84 ft8">Before parallelizing an application with OpenACC the programmer must ﬁrst understand where time is currently being spent in the code. Routines and loops that take up a signiﬁcant percentage of the runtime are frequently referred to as <SPAN class="ft15">hot spots </SPAN>and will be the starting point for accelerating the application. A variety of tools exist for generating application proﬁles, such as gprof, pgprof, Vampir, and TAU. Selecting the speciﬁc tool that works best for a given application is outside of the scope of this document, but regardless of which tool or tools are used below are some important pieces of information that will help guide the next steps in parallelizing the application.</P>
<P class="p85 ft8"><SPAN class="ft9">•</SPAN><SPAN class="ft33">Application performance - How much time does the application take to run? How eﬃciently does the program use the computing resources?</SPAN></P>
<P class="p86 ft8"><SPAN class="ft9">•</SPAN><SPAN class="ft33">Program hotspots - In which routines is the program spending most of its time? What is being done within these important routines? Focusing on the most time consuming parts of the application will yield the greatest results.</SPAN></P>
<P class="p87 ft14"><SPAN class="ft9">•</SPAN><SPAN class="ft34">Performance limiters - Within the identiﬁed hotspots, what’s currently limiting the application perfor- mance? Some common limiters may be I/O, memory bandwidth, cache reuse, ﬂoating point performance, communication, etc. One way to evaluate the performance limiters of a given loop nest is to evaluate its </SPAN><SPAN class="ft15">computational intensity</SPAN>, which is a measure of how many operations are performed on a data element per load or store from memory.</P>
<P class="p88 ft8"><SPAN class="ft9">•</SPAN><SPAN class="ft33">Available parallelism - Examine the loops within the hotspots to understand how much work each loop nest performs. Do the loops iterate 10’s, 100’s, 1000’s of times (or more)? Do the loop iterations operate independently of each other? Look not only at the individual loops, but look a nest of loops to understand the bigger picture of the entire nest.</SPAN></P>
<P class="p89 ft14">Gathering baseline data like the above both helps inform the developer where to focus eﬀorts for the best results and provides a basis for comparing performance throughout the rest of the process. It’s important to</P>
</DIV>
</DIV>
<DIV id="page_13">


<P class="p90 ft7">choose input that will realistically reﬂect how the application will be used once it has been accelerated. It’s tempting to use a known benchmark problem for profiling, but frequently these benchmark problems use a reduced problem size or reduced I/O, which may lead to incorrect assumptions about program performance. Many developers also use the baseline proﬁle to gather the expected output of the application to use for verifying the correctness of the application as it is accelerated.</P>
<P class="p91 ft11">Additional Profiling</P>
<P class="p92 ft8">Through the process of porting and optimizing an application with OpenACC it’s necessary to gather additional proﬁle data to guide the next steps in the process. Some profiling tools, such as pgprof and Vampir, support profiling on CPUs and GPUs, while other tools, such as gprof and NVIDIA Visual Proﬁler, may only support profiling on a particular platform. Additionally, some compilers build their own profiling into the application, such is the case with the PGI compiler, which supports setting the PGI_ACC_TIME environment variable for gathering runtime information about the application. When developing on oﬄoading platforms, such as CPU + GPU platforms, it’s generally important to use a profiling tool throughout the development process that can evaluate both time spent in computation and time spent performing PCIe data transfers. This document will use NVIDIA Visual Proﬁler for performing this analysis, although it is only available on NVIDIA platforms.</P>
<P class="p93 ft11">Case Study - Analysis</P>
<P class="p94 ft14">To get a better understanding of the case study program we will use the PGProf utility that comes as a part of the PGI Workstation package. First, it’s necessary to build the executable to embed the compiler feedback into the executable using the <SPAN class="ft15">common compiler feedback framework </SPAN>(CCFF) feature of the PGI compiler. This feature is enabled with the <NOBR><SPAN class="ft9">-Mprof=ccff</SPAN></NOBR><SPAN class="ft9"> </SPAN>compiler ﬂag and embeds additional information into the executable that can then be used by the PGProf utility to display additional information about how the compiler optimized the code. The executable is built with the following command. Once the executable has been build, the pgcollect command will run the executable and gather information that can be used by PGProf to profile the executable. </P>


<P class="p106 ft35">Once the data has been collected, it can be visualized using the pgprof command, which will open a PGProf window.</P>

<P class="p108 ft9">When PGPROG opens we see that the vast majority of the time is spent in two routines: main and __c_mcopy8. A screenshot of the initial screen for PGProf is show in ﬁgure 2.1. Since the code for this case study is completely within the main function of the program, it’s not surprising that nearly all of the time is spent in main, but in larger applications it’s likely that the time will be spent in several other routines.</P>

<P class="p110 ft23">Clicking into the main function we can see that nearly all of the runtime within main comes from the loop that calculates the next value for A. This is shown in ﬁgure 2.2. What is not obvious from the proﬁler output, however, is that the time spent in the memory copy routine shown in the initial screen is actually the second loop nest, which performs the array swap at the end of each iteration. The compiler output shows above</P>
</DIV>
<DIV id="page_15">


<P class="p111 ft8">(and is reiterated in PGProf) that the loop at line 68 was replaced by a memory copy, because doing so is more eﬃcient than copying each element individually. So what the proﬁler is really showing us is that the major hotspots for our application are the loop nest that calculate <SPAN class="ft9">Anew </SPAN>from <SPAN class="ft9">A </SPAN>and the loop nest that copies from <SPAN class="ft9">Anew </SPAN>to <SPAN class="ft9">A </SPAN>for the next iteration, so we’ll concentrate our eﬀorts on these two loop nests.</P>

<P class="p113 ft8">In the chapters that follow, we will optimize the loops identiﬁed in this chapter as the hotspots within our example application.</P>
</DIV>
<DIV id="page_16">

<!-- <DIV id="id_1">
<DIV id="id_1_1">
<P class="p2 ft17">1</P>
<P class="p56 ft17">2</P>
<P class="p56 ft17">3</P>
<P class="p56 ft17">4</P>
<P class="p56 ft17">5</P>
<P class="p56 ft17">6</P>
<P class="p56 ft17">7</P>
<P class="p56 ft17">8</P>
<P class="p56 ft17">9</P>
</DIV> -->
<DIV id="id_1_2">
<!-- <P class="p2 ft10">Chapter 3</P> -->
<P class="p9 ft3">Parallelize Loops</P>
<P class="p114 ft7">Now that the important hotspots in the application have been identiﬁed, the programmer should incrementally accelerate these hotspots by adding OpenACC directives to the important loops within those routines. There is no reason to think about the movement of data at this point in the process, the OpenACC compiler will analyze the data needed in the identiﬁed region and automatically ensure that the data is available on the accelerator. By focusing solely on the parallelism during this step, the programmer can move as much computation to the device as possible and ensure that the program is still giving correct results before optimizing away data motion in the next step. During this step in the process it is common for the overall runtime of the application to increase, even if the execution of the individual loops is faster using the accelerator. This is because the compiler must take a cautious approach to data movement, frequently copying more data to and from the accelerator than is actually necessary. Even if overall execution time increases during this step, the developer should focus on expressing a signiﬁcant amount of parallelism in the code before moving on to the next step and realizing a beneﬁt from the directives.</P>
<P class="p115 ft14">OpenACC provides two diﬀerent approaches for exposing parallelism in the code: <SPAN class="ft9">parallel </SPAN>and <SPAN class="ft9">kernels </SPAN>regions. Each of these directives will be detailed in the sections that follow.</P>
<P class="p21 ft11">The Kernels Construct</P>
<P class="p116 ft8">The <SPAN class="ft9">kernels </SPAN>construct identiﬁes a region of code that may contain parallelism, but relies on the automatic parallelization capabilities of the compiler to analyze the region, identify which loops are safe to parallelize, and then accelerate those loops. Developers will little or no parallel programming experience, or those working on functions containing many loop nests that might be parallelized will find the kernels directive a good starting place for OpenACC acceleration. The code below demonstrates the use of <SPAN class="ft9">kernels </SPAN>in both C/C++ and Fortran.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_17">



<DIV id="id_1">

<DIV id="id_1_2">

<P class="p129 ft8">In this example the code is initializing two arrays and then performing a simple calculation on them. Notice that we have identiﬁed a block of code, using curly braces in C and starting and ending directives in Fortran, that contains two candidate loops for acceleration. The compiler will analyze these loops for data independence and parallelize both loops by generating an accelerator <SPAN class="ft15">kernel </SPAN>for each. The compiler is given complete freedom to determine how best to map the parallelism available in these loops to the hardware, meaning that we will be able to use this same code regardless of the accelerator we are building for. The compiler will use its own knowledge of the target accelerator to choose the best path for acceleration. One caution about the <SPAN class="ft9">kernels </SPAN>directive, however, is that if the compiler cannot be certain that a loop is data independent, it will not parallelize the loop. Common reasons for why a compiler may misidentify a loop as <NOBR>non-parallel</NOBR> will be discussed in a later section.</P>
</DIV>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<P class="p2 ft11">The Parallel Construct</P>
<P class="p92 ft8">The <SPAN class="ft9">parallel </SPAN>construct identiﬁes a region of code that will be parallelized across OpenACC <SPAN class="ft15">gangs</SPAN>. By itself a <SPAN class="ft9">parallel </SPAN>region is of limited use, but when paired with the <SPAN class="ft9">loop </SPAN>directive (discussed in more detail later) the compiler will generate a parallel version of the loop for the accelerator. These two directives can, and most often are, combined into a single <SPAN class="ft9">parallel loop </SPAN>directive. By placing this directive on a loop the programmer asserts that the aﬀected loop is safe to parallelize and allows the compiler to select how to schedule the loop iterations on the target accelerator. The code below demonstrates the use of the <SPAN class="ft9">parallel loop </SPAN>combined directive in both C/C++ and Fortran.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_18">


<DIV id="id_1">

<DIV id="id_1_2">

<P class="p137 ft6">Notice that, unlike the <SPAN class="ft9">kernels </SPAN>directive, each loop needs to be explicitly decorated with <SPAN class="ft9">parallel loop </SPAN>directives. This is because the <SPAN class="ft9">parallel </SPAN>construct relies on the programmer to identify the parallelism in the code rather than performing its own compiler analysis of the loops. In this case, the programmer is only identifying the availability of parallelism, but still leaving the decision of how to map that parallelism to the accelerator to the compiler’s knowledge about the device. This is a key feature that diﬀerentiates OpenACC from other, similar programming models. The programmer identiﬁes the parallelism without dictating to the compiler how to exploit that parallelism. This means that OpenACC code will be portable to devices other than the device on which the code is being developed, because details about how to parallelize the code are left to compiler knowledge rather than being <NOBR>hard-coded</NOBR> into the source.</P>
</DIV>
</DIV>
<DIV id="id_2">
<P class="p2 ft38">Diﬀerences Between Parallel and Kernels</P>
<P class="p76 ft8">One of the biggest points of confusion for new OpenACC programmers is why the speciﬁcation has both the <SPAN class="ft9">parallel </SPAN>and <SPAN class="ft9">kernels </SPAN>directives, which appear to do the same thing. While they are very closely related there are subtle diﬀerences between them. The <SPAN class="ft9">kernels </SPAN>construct gives the compiler maximum leeway to parallelize and optimize the code how it sees ﬁt for the target accelerator, but also relies most heavily on the compiler’s ability to automatically parallelize the code. As a result, the programmer may see diﬀerences in what diﬀerent compilers are able to parallelize and how they do so. The <SPAN class="ft9">parallel loop </SPAN>directive is an assertion by the programmer that it is both safe and desirable to parallelize the aﬀected loop. This relies on the programmer to have correctly identiﬁed parallelism in the code and remove anything in the code that may be unsafe to parallelize. If the programmer asserts incorrectly that the loop may be parallelized then the resulting application may produce incorrect results.</P>
<P class="p138 ft8">To put things another way: the <SPAN class="ft9">kernels </SPAN>construct may be thought of as a hint to the compiler of where it should look for parallelism while the <SPAN class="ft9">parallel </SPAN>directive is an assertion to the compiler of where there is parallelism.</P>
<P class="p16 ft8">An important thing to note about the <SPAN class="ft9">kernels </SPAN>construct is that the compiler will analyze the code and only parallelize when it is certain that it is safe to do so. In some cases the compiler may not have enough information at compile time to determine whether a loop is safe the parallelize, in which case it will not parallelize the loop, even if the programmer can clearly see that the loop is safely parallel. For example, in the case of C/C++ code, where arrays are passed into functions as pointers, the compiler may not always be able to determine that two arrays do not share the same memory, otherwise known as <SPAN class="ft15">pointer aliasing</SPAN>. If the compiler cannot know that two pointers are not aliased it will not be able to parallelize a loop that accesses those arrays.</P>
<P class="p139 ft7"><SPAN class="ft16">Best Practice: </SPAN>C programmers should use the <SPAN class="ft9">restrict </SPAN>keyword (or the <SPAN class="ft9">__restrict </SPAN>decorator in C++) whenever possible to inform the compiler that the pointers are not aliased, which will frequently give the compiler enough information to then parallelize loops that it would not have otherwise. In addition to the <SPAN class="ft9">restrict </SPAN>keyword, declaring constant variables using the <SPAN class="ft9">const </SPAN>keyword may allow the compiler to use a <NOBR>read-only</NOBR> memory for that variable if such a memory exists on the accelerator. Use of <SPAN class="ft9">const </SPAN>and <SPAN class="ft9">restrict</SPAN></P>
</DIV>
</DIV>
<DIV id="page_19">



<P class="p140 ft8">is a good programming practice in general, as it gives the compiler additional information that can be used when optimizing the code.</P>
<P class="p141 ft8">Fortran programmers should also note that an OpenACC compiler will parallelize Fortran array syntax that is contained in a <SPAN class="ft9">kernels </SPAN>construct. When using <SPAN class="ft9">parallel </SPAN>instead, it will be necessary to explicitly introduce loops over the elements of the arrays.</P>
<P class="p142 ft7">One more notable beneﬁt that the <SPAN class="ft9">kernels </SPAN>construct provides is that if data is moved to the device for use in loops contained in the region, that data will remain on the device for the full extent of the region, or until it is needed again on the host within that region. This means that if multiple loops access the same data it will only be copied to the accelerator once. When <SPAN class="ft9">parallel loop </SPAN>is used on two subsequent loops that access the same data a compiler may or may not copy the data back and forth between the host and the device between the two loops. In the examples shown in the previous section the compiler generates implicit data movement for both parallel loops, but only generates data movement once for the <SPAN class="ft9">kernels </SPAN>approach, which may result in less data motion by default. This diﬀerence will be revisited in the case study later in this chapter.</P>
<P class="p143 ft39">For more information on the diﬀerences between the <SPAN class="ft9">kernels </SPAN>and <SPAN class="ft9">parallel </SPAN>directives, please see [http://www.pgroup.com/lit/articles/insider/v4n2a1.htm].</P>
<P class="p144 ft39">At this point many programmers will be left wondering which directive they should use in their code. More experienced parallel programmers, who may have already identiﬁed parallel loops within their code, will likely find the <SPAN class="ft9">parallel loop </SPAN>approach more desirable. Programmers with less parallel programming experience or whose code contains a large number of loops that need to be analyzed may find the <SPAN class="ft9">kernels </SPAN>approach much simpler, as it puts more of the burden on the compiler. Both approaches have advantages, so new OpenACC programmers should determine for themselves which approach is a better ﬁt for them. A programmer may even choose to use <SPAN class="ft9">kernels </SPAN>in one part of the code, but <SPAN class="ft9">parallel </SPAN>in another if it makes sense to do so.</P>
<P class="p51 ft8"><SPAN class="ft4">Note: </SPAN>For the remainder of the document the phrase <SPAN class="ft15">parallel region </SPAN>will be used to describe either a <SPAN class="ft9">parallel </SPAN>or <SPAN class="ft9">kernels </SPAN>region. When refering to the <SPAN class="ft9">parallel </SPAN>construct, a terminal font will be used, as shown in this sentence.</P>
<P class="p38 ft11">The Loop Construct</P>
<P class="p116 ft8">The <SPAN class="ft9">loop </SPAN>construct gives the compiler additional information about the very next loop in the source code. The <SPAN class="ft9">loop </SPAN>directive was shown above in connection with the <SPAN class="ft9">parallel </SPAN>directive, although it is also valid with <SPAN class="ft9">kernels</SPAN>. Loop clauses come in two forms: clauses for correctness and clauses for optimization. This chapter will only discuss the two correctness clauses and a later chapter will discuss optimization clauses.</P>
<!-- <P class="p14 ft12">private</P> -->
<P class="p145 ft7">The private clause speciﬁes that each loop iteration requires its own copy of the listed variables. For example, if each loop contains a small, temporary array named <SPAN class="ft9">tmp </SPAN>that it uses during its calculation, then this variable must be made private to each loop iteration in order to ensure correct results. If <SPAN class="ft9">tmp </SPAN>is not declared private, then threads executing diﬀerent loop iterations may access this shared <SPAN class="ft9">tmp </SPAN>variable in unpredictable ways, resulting in a race condition and potentially incorrect results. Below is the synax for the <SPAN class="ft9">private </SPAN>clause.</P>
<!-- <P class="p146 ft9">private(variable)</P> -->
<P class="p147 ft13">There are a few special cases that must be understood about scalar variables within loops. First, loop iterators will be privatized by default, so they do not need to be listed as private. Second, unless otherwise speciﬁed, any scalar accessed within a parallel loop will be made <SPAN class="ft15">ﬁrst private </SPAN>by default, meaning a private copy will</P>
</DIV>
<DIV id="page_20">


<DIV id="id_1">

<P class="p148 ft8">be made of the variable for each loop iteration and it will be initialized with the value of that scalar upon entering the region. Finally, any variables (scalar or not) that are declared within a loop in C or C++ will be made private to the iterations of that loop by default.</P>
<P class="p149 ft8">Note: The <SPAN class="ft9">parallel </SPAN>construct also has a <SPAN class="ft9">private </SPAN>clause which will privatize the listed variables for each gang in the parallel region.</P>
<!-- <P class="p150 ft12">reduction</P> -->
<P class="p151 ft7">The <SPAN class="ft9">reduction </SPAN>clause works similarly to the <SPAN class="ft9">private </SPAN>clause in that a private copy of the aﬀected variable is generated for each loop iteration, but <SPAN class="ft9">reduction </SPAN>goes a step further to reduce all of those private copies into one ﬁnal result, which is returned from the region. For example, the maximum of all private copies of the variable may be required or perhaps the sum. A reduction may only be speciﬁed on a scalar variable and only common, speciﬁed operations can be performed, such as <SPAN class="ft9">+</SPAN>, <SPAN class="ft9">*</SPAN>, <SPAN class="ft9">min</SPAN>, <SPAN class="ft9">max</SPAN>, and various bitwise operations (see the OpenACC speciﬁcation for a complete list). The format of the reduction clause is as follows, where <SPAN class="ft15">operator </SPAN>should be replaced with the operation of interest and <SPAN class="ft15">variable </SPAN>should be replaced with the variable being reduced:</P>
<P class="p152 ft9">reduction(operator:variable).</P>
<P class="p153 ft8">An example of using the <SPAN class="ft9">reduction </SPAN>clause will come in the case study below.</P>
</DIV>
<DIV id="id_2">
<!-- <DIV id="id_2_1">
<P class="p2 ft17">1</P>
<P class="p56 ft17">2</P>
<P class="p56 ft17">3</P>
<P class="p56 ft17">4</P>
<P class="p56 ft17">5</P>
<P class="p56 ft17">6</P>
<P class="p56 ft17">7</P>
<P class="p56 ft17">8</P>
</DIV> -->
<DIV id="id_2_2">
<P class="p2 ft11">Routine Directive</P>
<P class="p154 ft7">Function or subroutine calls within parallel loops can be problematic for compilers, since it’s not always possible for the compiler to see all of the loops at one time. OpenACC 1.0 compilers were forced to either inline all routines called within parallel regions or not parallelize loops containing routine calls at all. OpenACC 2.0 introduced the <SPAN class="ft9">routine </SPAN>directive to address this shortcoming. The <SPAN class="ft9">routine </SPAN>directive gives the compiler the necessary information about the function or subroutine and the loops it contains in order to parallelize the calling parallel region. The routine directive must be added to a function deﬁnition informing the compiler of the level of parallelism used within the routine. OpenACC’s <SPAN class="ft15">levels of parallelism </SPAN>will be discussed in a later section.</P>
<!-- <P class="p155 ft12">C++ Class Functions</P> -->
<P class="p18 ft8">When operating on C++ classes, it’s frequently necessary to call class functions from within parallel regions. The example below shows a C++ class <SPAN class="ft9">float3 </SPAN>that contains 3 ﬂoating point values and has a <SPAN class="ft9">set </SPAN>function that is used to set the values of its <SPAN class="ft9">x</SPAN>, <SPAN class="ft9">y</SPAN>, and <SPAN class="ft9">z </SPAN>members to that of another instance of <SPAN class="ft9">float3</SPAN>. In order for this to work from within a parallel region, the <SPAN class="ft9">set </SPAN>function is declared as an OpenACC routine using the <SPAN class="ft9">acc routine </SPAN>directive. Since we know that it will be called by each iteration of a parallel loop, it’s declared a <SPAN class="ft9">seq </SPAN>(or <SPAN class="ft15">sequential</SPAN>) routine.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_21">



<DIV id="id_2">

<DIV id="id_2_2">
<P class="p161 ft11">Case Study - Parallelize</P>
<P class="p162 ft8">In the last chapter we identiﬁed the two loop nests within the convergence loop as the most time consuming parts of our application. Additionally we looked at the loops and were able to determine that the outer, convergence loop is not parallel, but the two loops nested within are safe to parallelize. In this chapter we will accelerate those loop nests with OpenACC using the directives discussed earlier in this chapter. To further emphasize the similarities and diﬀerences between <SPAN class="ft9">parallel </SPAN>and <SPAN class="ft9">kernels </SPAN>directives, we will accelerate the loops using both and discuss the diﬀerences.</P>

<P class="p164 ft6">We previously identiﬁed the available parallelism in our code, now we will use the <SPAN class="ft9">parallel loop </SPAN>directive to accelerate the loops that we identiﬁed. Since we know that the two, <NOBR>doubly-nested</NOBR> sets of loops are parallel, simply add a <SPAN class="ft9">parallel loop </SPAN>directive above each of them. This will inform the compiler that the outer of the two loops is safely parallel. Some compilers will additionally analyze the inner loop and determine that it is also parallel, but to be certain we will also add a <SPAN class="ft9">loop </SPAN>directive around the inner loops.</P>
<P class="p165 ft8">There is one more subtlety to accelerating the loops in this example: we are attempting to calculate the maximum value for the variable <SPAN class="ft9">error</SPAN>. As discussed above, this is considered a <SPAN class="ft15">reduction </SPAN>since we are reducing from all possible values for <SPAN class="ft9">error </SPAN>down to just the single maximum. This means that it is necessary to indicate a reduction on the ﬁrst loop nest (the one that calculates <SPAN class="ft9">error</SPAN>).</P>
<P class="p166 ft14"><SPAN class="ft16">Best Practice: </SPAN>Some compilers will detect the reduction on <SPAN class="ft9">error </SPAN>and implicitly insert the <SPAN class="ft9">reduction </SPAN>clause, but for maximum portability the programmer should always indicate reductions in the code.</P>
<P class="p79 ft8">At this point the code looks like the examples below.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_22">



<DIV class="dclr"></DIV>
<DIV>
<DIV id="id_2">


<P class="p191 ft9"><SPAN class="ft16">Best Practice: </SPAN>Most OpenACC compilers will accept only the parallel loop directive on the j loops and detect for themselves that the i loop can also be parallelized without needing the loop directives on the i loops. By placing a loop directive on each loop that I know can be parallelized the programmer ensures that the compiler will understand that the loop is safe the parallelize. When used within a parallel region, the loop directive asserts that the loop iterations are independent of each other and are safe the parallelize and should be used to provide the compiler as much information about the loops as possible.</P>
<P class="p192 ft35">Building the above code using the PGI compiler (version 15.5) produces the following compiler feedback (shown for C, but the Fortran output is similar).</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_23">


<DIV>

<DIV id="id_2">

<P class="p203 ft9">Analyzing the compiler feedback gives the programmer the ability to ensure that the compiler is producing the expected results and ﬁx any problems if it’s not. In the output above we see that accelerator kernels were generated for the two loops that were identiﬁed (at lines 58 and 71, in the compiled source ﬁle) and that the compiler automatically generated data movement, which will be discussed in more detail in the next chapter.</P>
<P class="p204 ft35">Other clauses to the loop directive that may further beneﬁt the performance of the resulting code will be discussed in a later chapter.</P>
<!-- <P class="p14 ft12">Kernels</P> -->
<P class="p205 ft23">Using the kernels construct to accelerate the loops we’ve identiﬁed requires inserting just one directive in the code and allowing the compiler to perform the parallel analysis. Adding a kernels construct around the two computational loop nests results in the following code.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_24">


<DIV class="dclr"></DIV>
<DIV>

<DIV id="id_2">

<P class="p22 ft7">The above code demonstrates some of the power that the <SPAN class="ft9">kernels </SPAN>construct provides, since the compiler will analyze the code and identify both loop nests as parallel and it will automatically discover the reduction on the <SPAN class="ft9">error </SPAN>variable without programmer intervention. An OpenACC compiler will likely discover not only that the outer loops are parallel, but also the inner loops, resulting in more available parallelism with fewer directives than the <SPAN class="ft9">parallel loop </SPAN>approach. Had the programmer put the <SPAN class="ft9">kernels </SPAN>construct around the convergence loop, which we have already determined is not parallel, the compiler likely would not have found any available parallelism. Even with the <SPAN class="ft9">kernels </SPAN>directive it is necessary for the programmer to do some amount of analysis to determine where parallelism may be found.</P>
<P class="p216 ft8">Taking a look at the compiler output points to some more subtle diﬀerences between the two approaches.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_25">

<P class="p221 ft8">The ﬁrst thing to notice from the above output is that the compiler correctly identiﬁed all four loops as being parallelizable and generated kernels from those loops. Also notice that the compiler only generated implicit data movement directives at line 54 (the beginning of the <SPAN class="ft9">kernels </SPAN>region), rather than at the beginning of each <SPAN class="ft9">parallel loop</SPAN>. This means that the resulting code should perform fewer copies between host and device memory in this version than the version from the previous section. A more subtle diﬀerence between the output is that the compiler chose a diﬀerent loop decomposition scheme (as is evident by the implicit <SPAN class="ft9">acc loop </SPAN>directives in the compiler output) than the parallel loop because <SPAN class="ft9">kernels </SPAN>allowed it to do so. More details on how to interpret this decomposition feedback and how to change the behavior will be discussed in a later chapter.</P>
<P class="p222 ft8">At this point we have expressed all of the parallelism in the example code and the compiler has parallelized it for an accelerator device. Analyzing the performance of this code may yield surprising results on some accelerators, however. The results below demonstrate the performance of this code on 1 - 8 CPU threads on a modern CPU at the ime of publication and an NVIDIA Tesla K40 GPU using both implementations above. The <SPAN class="ft15">y axis </SPAN>for ﬁgure 3.1 is execution time in seconds, so smaller is better. For the two OpenACC versions, the bar is divided by time transferring data between the host and device, time executing on the device, and other time.</P>
<!-- <P class="p223 ft8">Figure 3.1: Jacobi Iteration Performance - Step 1</P> -->
<P class="p224 ft8">Notice that the performance of this code improves as CPU threads are added to the calcuation, but the OpenACC versions perform poorly compared to the CPU baseline. The OpenACC <SPAN class="ft9">kernels </SPAN>version performs slightly better than the serial version, but the <SPAN class="ft9">parallel loop </SPAN>case performs dramaticaly worse than even the slowest CPU version. Further performance analysis is necessary to identify the source of this slowdown. This analysis has already been applied to the graph above, which breaks down time spent computing the solution, copying data to and from the accelerator, and miscelaneous time, which includes various overheads involved in scheduling data transfers and computation.</P>
<P class="p225 ft8">A variety of tools are available for performing this analysis, but since this case study was compiled for an NVIDIA GPU, the NVIDIA Visual proﬁler will be used to understand the application peformance. The screenshot in ﬁgure 3.2 shows NVIDIA Visual Proﬁler for <SPAN class="ft16">2 </SPAN>iterations of the convergence loop in the <SPAN class="ft9">parallel loop </SPAN>version of the code.</P>
</DIV>
<DIV id="page_26">


</DIV>
<DIV id="page_27">


<DIV>

<DIV id="id_2">

<P class="p227 ft7">Since the test machine has two distinct memory spaces, one for the CPU and one for the GPU, it’s necessary to copy data between the two memories. In this screenshot, the tool represents data transfers using the tan colored boxes in the two <SPAN class="ft15">MemCpy </SPAN>rows and the computation time in the green and purple boxes in the rows below <SPAN class="ft15">Compute</SPAN>. It should be obvious from the timeline displayed that signiﬁcantly more time is being spent copying data to and from the accelerator before and after each compute kernel than actually computing on the device. In fact, the majority of the time is spent either in memory copies or in overhead incurred by the runtime scheduling memory copeis. In the next chapter we will ﬁx this ineﬃciency, but ﬁrst, why does the <SPAN class="ft9">kernels </SPAN>version outperform the <SPAN class="ft9">parallel loop </SPAN>version?</P>
<P class="p228 ft8">When an OpenACC compiler parallelizes a region of code it must analyze the data that is needed within that region and copy it to and from the accelerator if necessary. This analysis is done at a <NOBR>per-region</NOBR> level and will typically default to copying arrays used on the accelerator both to and from the device at the beginning and end of the region respectively. Since the <SPAN class="ft9">parallel loop </SPAN>version has two compute regions, as opposed to only one in the <SPAN class="ft9">kernels </SPAN>version, data is copied back and forth between the two regions. As a result, the copy and overhead times are roughly twice that of the <SPAN class="ft9">kernels </SPAN>region, although the compute kernel times are roughly the same.</P>
<P class="p21 ft11">Atomic Operations</P>
<P class="p229 ft8">When one or more loop iterations need to access an element in memory at the same time data races can occur. For instance, if one loop iteration is modifying the value contained in a variable and another is trying to read from the same variable in parallel, diﬀerent results may occur depending on which iteration occurs ﬁrst. In serial programs, the sequential loops ensure that the variable will be modiﬁed and read in a predictable order, but parallel programs don’t make guarantees that a particular loop iteration will happen before anoter. In simple cases, such as finding a sum, maximum, or minimum value, a reduction operation will ensure correctness. For more complex operations, the <SPAN class="ft9">atomic </SPAN>directive will ensure that no two threads can attempt to perfom the contained operation simultaneously. Use of atomics is sometimes a necessary part of parallelization to ensure correctness.</P>
<P class="p230 ft8">The <SPAN class="ft9">atomic </SPAN>directive accepts one of four clauses to declare the type of operation contained within the region. The <SPAN class="ft9">read </SPAN>operation ensures that no two loop iterations will read from the region at the same time. The <SPAN class="ft9">write </SPAN>operation will ensure that no two iterations with write to the region at the same time. An <SPAN class="ft9">update </SPAN>operation is a combined read and write. Finally a <SPAN class="ft9">capture </SPAN>operation performs an update, but saves the value calculated in that region to use in the code that follows. If no clause is given then an update operation will occur.</P>
<!-- <P class="p83 ft12">Atomic Example</P> -->
<P class="p20 ft8">A histogram is a common technique for counting up how many times values occur from an input set according to their value. Figure _ shows a histogram that counts the number of times numbers fall within particular ranges. The example code below loops through a series of integer numbers of a known range and counts the occurances of each number in that range. Since each number in the range can occur multiple times, we need to ensure that each element in the histogram array is updated atomically. The code below demonstrates using the <SPAN class="ft9">atomic </SPAN>directive to generate a histogram.</P>
<!-- <P class="p231 ft19">#pragma acc parallel loop <SPAN class="ft24">for</SPAN><SPAN class="ft9">(</SPAN><SPAN class="ft26">int </SPAN><SPAN class="ft9">i=</SPAN><SPAN class="ft25">0</SPAN><SPAN class="ft9">;i&lt;HN;i++)</SPAN></P>
<P class="p118 ft9">h[i]=<SPAN class="ft25">0</SPAN>;</P>
<P class="p232 ft37">#pragma acc parallel loop <SPAN class="ft30">for</SPAN><SPAN class="ft23">(</SPAN><SPAN class="ft40">int </SPAN><SPAN class="ft23">i=</SPAN><SPAN class="ft31">0</SPAN><SPAN class="ft23">;i&lt;N;i++) {</SPAN></P>
<P class="p119 ft19">#pragma acc atomic update</P> -->
</DIV>
</DIV>
</DIV>
<DIV id="page_28">


<DIV class="dclr"></DIV>
<!-- <DIV>
<DIV id="id_1">
<P class="p2 ft17">8</P>
<P class="p56 ft17">9</P>
<P class="p233 ft17">1</P>
<P class="p56 ft17">2</P>
<P class="p56 ft17">3</P>
<P class="p56 ft17">4</P>
<P class="p56 ft17">5</P>
<P class="p56 ft17">6</P>
<P class="p56 ft17">7</P>
<P class="p56 ft17">8</P>
<P class="p56 ft17">9</P>
</DIV> -->
<DIV id="id_2">
<!-- <TABLE cellpadding=0 cellspacing=0 class="t2">
<TR>
	<TD class="tr0 td13"><P class="p6 ft9">CHAPTER 3. PARALLELIZE LOOPS</P></TD>
	<TD class="tr0 td14"><P class="p8 ft9">27</P></TD>
</TR>
</TABLE> -->
<!-- <P class="p234 ft9">Figure 3.3: A histogram of number distribution.</P>
<P class="p235 ft9">h[a[i]]+=<SPAN class="ft25">1</SPAN>;</P>
<P class="p125 ft9">}</P>
<P class="p236 ft36">!$acc kernels <SPAN class="ft23">h(:) </SPAN><SPAN class="ft30">= </SPAN><SPAN class="ft31">0</SPAN></P>
<P class="p60 ft21">!$acc end kernels</P>
<P class="p237 ft21">!$acc parallel loop <SPAN class="ft24">do </SPAN><SPAN class="ft9">i</SPAN><SPAN class="ft24">=</SPAN><SPAN class="ft25">1</SPAN><SPAN class="ft9">,N</SPAN></P>
<P class="p119 ft42">!$acc atomic</P>
<P class="p119 ft9">h(a(i)) <SPAN class="ft24">= </SPAN>h(a(i)) <SPAN class="ft24">+ </SPAN><SPAN class="ft25">1</SPAN></P>
<P class="p60 ft24">enddo</P>
<P class="p60 ft21">!$acc end parallel loop</P> -->
<P class="p238 ft35">Notice that updates to the histogram array h are performed atomically. Because we are incrementing the value of the array element, an update operation is used to read the value, modify it, and then write it back.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_29">
<!-- <DIV id="p29dimg1">
<IMG src="data:image/jpg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAABATgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP/9k=" alt=""></DIV>
 -->

<DIV id="id_1">
<!-- <P class="p2 ft10">Chapter 4</P> -->
<P class="p9 ft3">Optimize Data Locality</P>
<P class="p239 ft8">At the end of the previous chapter we saw that although we’ve moved the most compute intensive parts of the application to the accelerator, sometimes the process of copying data from the host to the accelerator and back will be more costly than the computation itself. This is because it’s diﬃcult for a compler to determine when (or if) the data will be needed in the future, so it must be cautious and ensure that the data will be copied in case it’s needed. To improve upon this, we’ll exploit the <SPAN class="ft15">data locality </SPAN>of the application. Data locality means that data used in device or host memory should remain local to that memory for as long as it’s needed. This idea is sometimes referred to as optimizing data reuse or optimizing away unnecessary data copies between the host and device memories. However you think of it, providing the compiler with the information necessary to only relocate data when it needs to do so is frequently the key to success with OpenACC.</P>
<P class="p240 ft7">After expressing the parallelism of a program’s important regions it’s frequently necessary to provide the compiler with additional information about the locality of the data used by the parallel regions. As noted in the previous section, a compiler will take a cautious approach to data movement, always copying data that may be required, so that the program will still produce correct results. A programmer will have knowledge of what data is really needed and when it will be needed. The programmer will also have knowledge of how data may be shared between two functions, something that is diﬃcult for a compiler to determine. Even when does not immediately know how best to optimize data motion, profiling tools may help the programmer identify when excess data movement occurs, as will be shown in the case study at the end of this chapter.</P>
<P class="p142 ft8">The next step in the acceleration process is to provide the compiler with additional information about data locality to maximize reuse of data on the device and minimize data transfers. It is after this step that most applications will observe the beneﬁt of OpenACC acceleration. This step will be primarily beneﬁcial on machine where the host and device have separate memories.</P>
<P class="p155 ft11">Data Regions</P>
<P class="p229 ft8">The <SPAN class="ft9">data </SPAN>construct facilitates the sharing of data between multiple parallel regions. A data region may be added around one or more parallel regions in the same function or may be placed at a higher level in the program call tree to enable data to be shared between regions in multiple functions. The <SPAN class="ft9">data </SPAN>construct is a structured construct, meaning that it must begin and end in the same scope (such as the same function or subroutine). A later section will discuss how to handle cases where a structured construct is not useful. A <SPAN class="ft9">data </SPAN>region may be added to the earlier <SPAN class="ft9">parallel loop </SPAN>example to enable data to be shared between both loop nests as follows.</P>
</DIV>

</DIV>
<DIV id="page_30">



<DIV id="id_1">

<DIV id="id_1_2">

<P class="p252 ft23">The data region in the above examples enables the x and y arrays to be reused between the two parallel regions. This will remove any data copies that happen between the two regions, but it still does not guarantee optimal data movement. In order to provide the information necessary to perform optimal data movement, the programmer can add data clauses to the data region.</P>
<P class="p253 ft9"><SPAN class="ft15">Note: </SPAN>An implicit data region is created by each parallel and kernels region.</P>
</DIV>
</DIV>
<DIV id="id_2">
<P class="p2 ft11">Data Clauses</P>
<P class="p254 ft23">Data clauses give the programmer additional control over how and when data is created on and copied to or from the device. These clauses may be added to any data, parallel, or kernels construct to inform the compiler of the data needs of that region of code. The data directives, along with a brief description of their meanings, follow.</P>
<P class="p255 ft23"><SPAN class="ft9">•</SPAN><SPAN class="ft43">copy - Create space for the listed variables on the device, initialize the variable by copying data to the device at the beginning of the region, copy the results back to the host at the end of the region, and ﬁnally release the space on the device when done.</SPAN></P>
<P class="p256 ft23"><SPAN class="ft9">•</SPAN><SPAN class="ft43">copyin - Create space for the listed variables on the device, initialize the variable by copying data to the device at the beginning of the region, and release the space on the device when done without copying the data back the the host.</SPAN></P>
</DIV>
</DIV>


<DIV id="page_31">


<DIV>

<DIV id="id_2">

<P class="p257 ft22"><SPAN class="ft9">•</SPAN><SPAN class="ft44">copyout </SPAN>- Create space for the listed variables on the device but do not initialize them. At the end of the region, copy the results back to the host and release the space on the device.</P>
<P class="p258 ft8"><SPAN class="ft9">•</SPAN><SPAN class="ft44">create </SPAN>- Create space for the listed variables and release it at the end of the region, but do not copy to or from the device.</P>
<P class="p259 ft22"><SPAN class="ft9">•</SPAN><SPAN class="ft44">present </SPAN>- The listed variables are already present on the device, so no further action needs to be taken. This is most frequently used when a data region exists in a <NOBR>higher-level</NOBR> routine.</P>
<P class="p260 ft8"><SPAN class="ft9">•</SPAN><SPAN class="ft44">deviceptr </SPAN>- The listed variables use device memory that has been managed outside of OpenACC, therefore the variables should be used on the device without any address translation. This clause is generally used when OpenACC is mixed with another programming model, as will be discussed in the interoperability chapter.</P>
<P class="p139 ft8">In addition to these data clauses, OpenACC 1.0 and 2.0 provide <SPAN class="ft9">present_or_* </SPAN>clauses (<SPAN class="ft9">present_or_copy</SPAN>, for instance) that inform the compiler to check whether the variable is already present on the device; if it is present, use that existing copy of the data, if it is not, perform the action listed. These routines are frequently abbreviated, like <SPAN class="ft9">pcopyin </SPAN>instead of <SPAN class="ft9">present_or_copyin</SPAN>. In an upcoming OpenACC speciﬁcation the behavior of all data directives will be <SPAN class="ft15">present or</SPAN>, so programmers should begin writing their applications using these directives to ensure correctness with future OpenACC speciﬁcations. This change will simplify data reuse for the programmer.</P>
<!-- <P class="p52 ft12">Shaping Arrays</P> -->
<P class="p227 ft7">Sometimes a compiler will need some extra help determining the size and shape of arrays used in parallel or data regions. For the most part, Fortran programmers can rely on the <NOBR>self-describing</NOBR> nature of Fortran arrays, but C/C++ programmers will frequently need to give additional information to the compiler so that it will know how large an array to allocate on the device and how much data needs to be copied. To give this information the programmer adds a <SPAN class="ft15">shape </SPAN>speciﬁcation to the data clauses.</P>
<P class="p261 ft14">In C/C++ the shape of an array is described as <SPAN class="ft9">x[start:count] </SPAN>where <SPAN class="ft15">start </SPAN>is the ﬁrst element to be copied and <SPAN class="ft15">count </SPAN>is the number of elements to copy. If the ﬁrst element is 0, then it may be left oﬀ.</P>
<P class="p77 ft8">In Fortran the shape of an array is described as <SPAN class="ft9">x(start:end) </SPAN>where <SPAN class="ft15">start </SPAN>is the ﬁrst element to be copied and <SPAN class="ft15">end </SPAN>is the last element to be copied. If <SPAN class="ft15">start </SPAN>is the beginning of the array or <SPAN class="ft15">end </SPAN>is the end of the array, they may be left oﬀ.</P>
<P class="p262 ft8">Array shaping is frequently necessary in C/C++ codes when the OpenACC appears inside of function calls or the arrays are dynamically allocated, since the shape of the array will not be known at compile time. Shaping is also useful when only a part of the array needs to be stored on the device.</P>
<P class="p263 ft8">As an example of array shaping, the code below modiﬁes the previous example by adding shape information to each of the arrays.</P>
<!-- <P class="p264 ft19">#pragma acc data pcreate(x[0:N]) pcopyout(y[0:N])</P>
<P class="p60 ft9">{</P>
<P class="p265 ft19">#pragma acc parallel loop <SPAN class="ft24">for </SPAN><SPAN class="ft9">(i=</SPAN><SPAN class="ft25">0</SPAN><SPAN class="ft9">; i&lt;N; i++)</SPAN></P>
<P class="p63 ft9">{</P>
<P class="p73 ft9">y[i] = <SPAN class="ft25">0.</SPAN>0f;</P>
<P class="p73 ft9">x[i] = (<SPAN class="ft26">float</SPAN>)(i<SPAN class="ft25">+1</SPAN>);</P>
<P class="p61 ft9">}</P>
<P class="p266 ft37">#pragma acc parallel loop <SPAN class="ft30">for </SPAN><SPAN class="ft23">(i=</SPAN><SPAN class="ft31">0</SPAN><SPAN class="ft23">; i&lt;N; i++)</SPAN></P>
<P class="p63 ft9">{</P>
<P class="p73 ft9">y[i] = <SPAN class="ft25">2.</SPAN>0f * x[i] + y[i];</P> -->
</DIV>
</DIV>
</DIV>
<DIV id="page_32">

<!-- <DIV id="id_1">
<DIV id="id_1_1">
<P class="p121 ft17">14</P>
<P class="p122 ft17">15</P>
<P class="p123 ft17">1</P>
<P class="p124 ft17">2</P>
<P class="p124 ft17">3</P>
<P class="p124 ft17">4</P>
<P class="p124 ft17">5</P>
<P class="p124 ft17">6</P>
<P class="p124 ft17">7</P>
<P class="p124 ft17">8</P>
<P class="p124 ft17">9</P>
<P class="p122 ft17">10</P>
<P class="p122 ft17">11</P>
<P class="p122 ft17">12</P>
</DIV>
<DIV id="id_1_2">
<TABLE cellpadding=0 cellspacing=0 class="t2">
<TR>
	<TD class="tr0 td15"><P class="p6 ft9">CHAPTER 4. OPTIMIZE DATA LOCALITY</P></TD>
	<TD class="tr0 td16"><P class="p8 ft9">31</P></TD>
</TR>
</TABLE>
<P class="p267 ft9">}</P>
<P class="p125 ft9">}</P>
<P class="p268 ft21">!$acc data pcreate(x(1:N)) pcopyout(y(1:N))</P>
<P class="p60 ft21">!$acc parallel loop</P>
<P class="p269 ft30">do <SPAN class="ft23">i</SPAN>=<SPAN class="ft31">1</SPAN><SPAN class="ft23">,N y(i) </SPAN>= <SPAN class="ft31">0 </SPAN><SPAN class="ft23">x(i) </SPAN>= <SPAN class="ft23">i</SPAN></P>
<P class="p60 ft24">enddo</P>
<P class="p270 ft21">!$acc parallel loop</P>
<P class="p60 ft24">do <SPAN class="ft9">i</SPAN>=<SPAN class="ft25">1</SPAN><SPAN class="ft9">,N</SPAN></P>
<P class="p119 ft9">y(i) <SPAN class="ft24">= </SPAN><SPAN class="ft25">2.0 </SPAN><SPAN class="ft24">* </SPAN>x(i) <SPAN class="ft24">+ </SPAN>y(i)</P>
<P class="p60 ft24">enddo</P>
<P class="p125 ft21">!$acc end data</P>
</DIV>
</DIV>
<DIV id="id_2">
<DIV id="id_2_1">
<P class="p241 ft17">1</P>
<P class="p242 ft17">2</P>
<P class="p242 ft17">3</P>
<P class="p242 ft17">4</P>
<P class="p242 ft17">5</P>
<P class="p242 ft17">6</P>
<P class="p242 ft17">7</P>
<P class="p243 ft17">8</P>
<P class="p242 ft17">9</P>
<P class="p122 ft17">10</P>
<P class="p122 ft17">11</P>
<P class="p122 ft17">12</P>
<P class="p122 ft17">13</P>
<P class="p122 ft17">14</P>
<P class="p122 ft17">15</P>
<P class="p244 ft17">1</P>
<P class="p242 ft17">2</P>
<P class="p242 ft17">3</P>
<P class="p242 ft17">4</P>
<P class="p242 ft17">5</P>
<P class="p242 ft17">6</P>
<P class="p242 ft17">7</P>
</DIV> -->
<DIV id="id_2_2">
<P class="p271 ft9">With these data clauses it is possible to further improve the example shown above by informing the compiler how and when it should perform data transfers. In this simple example above, the programmer knows that both x and y will be populated with data on the device, so neither will need to be copied to the device, but the results of y are signiﬁcant, so it will need to be copied back to the host at the end of the calculation. The code below demonstrates using the pcreate and pcopyout directives to describe exactly this data locality to the compiler.</P>
<!-- <P class="p272 ft19">#pragma acc data pcreate(x) pcopyout(y)</P>
<P class="p60 ft9">{</P>
<P class="p273 ft19">#pragma acc parallel loop <SPAN class="ft24">for </SPAN><SPAN class="ft9">(i=</SPAN><SPAN class="ft25">0</SPAN><SPAN class="ft9">; i&lt;N; i++)</SPAN></P>
<P class="p63 ft9">{</P>
<P class="p73 ft9">y[i] = <SPAN class="ft25">0.</SPAN>0f;</P>
<P class="p73 ft9">x[i] = (<SPAN class="ft26">float</SPAN>)(i<SPAN class="ft25">+1</SPAN>);</P>
<P class="p63 ft9">}</P>
<P class="p274 ft37">#pragma acc parallel loop <SPAN class="ft30">for </SPAN><SPAN class="ft23">(i=</SPAN><SPAN class="ft31">0</SPAN><SPAN class="ft23">; i&lt;N; i++)</SPAN></P>
<P class="p63 ft9">{</P>
<P class="p73 ft9">y[i] = <SPAN class="ft25">2.</SPAN>0f * x[i] + y[i];</P>
<P class="p63 ft9">}</P>
<P class="p60 ft9">}</P>
<P class="p268 ft21">!$acc data pcreate(x) pcopyout(y)</P>
<P class="p60 ft21">!$acc parallel loop</P>
<P class="p269 ft30">do <SPAN class="ft23">i</SPAN>=<SPAN class="ft31">1</SPAN><SPAN class="ft23">,N y(i) </SPAN>= <SPAN class="ft31">0 </SPAN><SPAN class="ft23">x(i) </SPAN>= <SPAN class="ft23">i</SPAN></P>
<P class="p60 ft24">enddo</P> -->
</DIV>
</DIV>
</DIV>
<DIV id="page_33">


<DIV id="id_2">
<P class="p2 ft11">Unstructured Data Lifetimes</P>


<P class="p271 ft8">While structured data regions are generally suﬃcient for optimizing the data locality in a program, they are not suﬃcient for some programs, particularly those using Object Oriented coding practices. For example, in a C++ class data is frequently allocated in a class constructor, deallocated in the destructor, and cannot be accessed outside of the class. This makes using structured data regions impossible because there is no single, structured scope where the construct can be placed. For these situations OpenACC 2.0 introduced unstructured data lifetimes. The <SPAN class="ft9">enter data </SPAN>and <SPAN class="ft9">exit data </SPAN>directives can be used to identify precisely when data should be allocated and deallocated on the device.</P>
<P class="p225 ft8">The <SPAN class="ft9">enter data </SPAN>directive accepts the <SPAN class="ft9">create </SPAN>and <SPAN class="ft9">copyin </SPAN>data clauses and may be used to specify when data should be created on the device.</P>
<P class="p205 ft8">The <SPAN class="ft9">exit data </SPAN>directive accepts the <SPAN class="ft9">copyout </SPAN>and a special <SPAN class="ft9">delete </SPAN>data clause to specify when data should be removed from the device.</P>
<P class="p141 ft8">Please note that multiple <SPAN class="ft9">enter data </SPAN>directives may place an array on the device, but when any <SPAN class="ft9">exit data </SPAN>directive removes it from the device it will be immediately removed, regardless of how many <SPAN class="ft9">enter data </SPAN>regions reference it.</P>
<!-- <P class="p54 ft12">C++ Class Data</P> -->
<P class="p141 ft8">C++ class data is one of the primary reasons that unstructured data lifetimes were added to OpenACC. As described above, the encapsulation provided by classes makes it impossible to use a structured <SPAN class="ft9">data </SPAN>region to control the locality of the class data. Programmers may choose to use the unstructured data lifetime directives or the OpenACC API to control data locality within a C++ class. Use of the directives is preferable, since they will be safely ignored by <NOBR>non-OpenACC</NOBR> compilers, but the API is also available for times when the directives are not expressive enough to meet the needs of the programmer. The API will not be discussed in this guide, but is <NOBR>well-documented</NOBR> on the OpenACC website.</P>
<P class="p278 ft14">The example below shows a simple C++ class that has a constructor, a destructor, and a copy constructor. The data management of these routines has been handled using OpenACC directives.</P>

</DIV>
<DIV id="page_34">


<P class="p288 ft9">Notice that an enter data directive is added to the class constructor to handle creating space for the class data on the device. In addition to the data array itself the this pointer is copied to the device. Copying the this pointer ensures that the scalar member len, which denotes the length of the data array arr, and the pointer arr are available on the accelerator as well as the host. It is important to place the enter data directive after the class data has been initialized. Similarly exit data directives are added to the destructor to handle cleaning up the device memory. It is important to place this directive before array members are freed, because once the host copies are freed the underlying pointer may become invalid, making it impossible to then free the device memory as well. For the same reason the this pointer should not be removed from the device until after all other memory has been released.</P>
<P class="p28 ft9">The copy constructor is a special case that is worth looking at on its own. The copy constructor will be responsible for allocating space on the device for the class that it is creating, but it will also rely on data that is managed by the class being copied. Since OpenACC does not currently provide a portable way to copy from one array to another, like a memcpy on the host, a loop is used to copy each individual element to from one array to the other. Because we know that the Data object passed in will also have its members on the device, we use a present clause on the parallel loop to inform the compiler that no data movement is necessary.</P>
<P class="p289 ft9">The same technique used in the class constructor and destructor above can be used in other programming languages as well. For instance, it’s common practice in Fortran codes to have a subroutine that allocate and initialize all arrays contained within a module. Such a routine is a natural place to use an enter data region, as the allocation of both the host and device memory will appear within the same routine in the code. Placing enter data and exit data directives in close proximity to the usual allocation and deallocation of data within the code simpliﬁes code maintenance.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_35">


<DIV id="id_1">

<DIV id="id_1_2">
<P class="p24 ft11">Update Directive</P>
<P class="p154 ft7">Keeping data resident on the accelerator is often key to obtaining high performance, but sometimes it’s necessary to copy data between host and device memories. The <SPAN class="ft9">update </SPAN>directive provides a way to explicitly update the values of host or device memory with the values of the other. This can be thought of as syncrhonizing the contents of the two memories. The <SPAN class="ft9">update </SPAN>directive accepts a <SPAN class="ft9">device </SPAN>clause for copying data from the host to the device and a <SPAN class="ft9">self </SPAN>directive for updating from the device to local memory, which is the host memory, except in the case of nested OpenACC regions. OpenACC 1.0 had a <SPAN class="ft9">host </SPAN>clause, which is deprecated in OpenACC 2.0 and behaves the same as <SPAN class="ft9">self</SPAN>. The <SPAN class="ft9">update </SPAN>directive has other clauses and the more commonly used ones will be discussed in a later chapter.</P>
<P class="p143 ft14">As an example of the <SPAN class="ft9">update </SPAN>directive, below are two routines that may be added to the above <SPAN class="ft9">Data </SPAN>class to force a copy from host to device and device to host.</P>

</DIV>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<P class="p261 ft8">The update clauses accept an array shape, as already discussed in the data clauses section. Although the above example copies the entire <SPAN class="ft9">arr </SPAN>array to or from the device, a partial array may also be provided to reduce the data transfer cost when only part of an array needs to be updated, such as when exchanging boundary conditions.</P>
<P class="p271 ft7"><SPAN class="ft16">Best Practice: </SPAN>As noted earlier in the document, variables in an OpenACC code should always be thought of as a singular object, rather than a <SPAN class="ft15">host </SPAN>copy and a <SPAN class="ft15">device </SPAN>copy. Even when developing on a machine with a uniﬁed host and device memory it is important to include an <SPAN class="ft9">update </SPAN>directive whenever accessing data from the host or device that was previous written to by the other, it is important to use an <SPAN class="ft9">update </SPAN>directive to ensure correctness on all devices. For systems with distinct memories, the <SPAN class="ft9">update </SPAN>will synchronize the values of the aﬀected variable on the host and the device. On devices with a uniﬁed memory, the update will be ignored, incurring no performance penalty. In the example below, omiting the <SPAN class="ft9">update </SPAN>on line 17 will produce diﬀerent results on a uniﬁed and <NOBR>non-uniﬁed</NOBR> memory machine, making the code <NOBR>non-portable.</NOBR></P>

</DIV>
</DIV>
</DIV>
<DIV id="page_36">


<DIV id="id_3">

<DIV id="id_3_2">
<P class="p261 ft47">Best Practice: Offload Inefficient Operations to Maintain Data Locality</P>
<P class="p295 ft8">Due to the high cost of PCIe data transfers on systems with distinct host and device memories, it’s often beneﬁcial to move sections of the application to the accelerator device, even when the code lacks suﬃcient parallelism to see direct beneﬁt. The performance loss of running serial or code with a low degree of parallelism on a parallel accelerator is often less than the cost of transferring arrays back and forth between the two memories. A developer may use a <SPAN class="ft9">parallel </SPAN>region with just 1 gang as a way to oﬄoad a serial section of the code to the accelerator. For instance, in the code below the ﬁrst and last elements of the array are host elements that need to be set to zero. A <SPAN class="ft9">parallel </SPAN>region (without a <SPAN class="ft9">loop</SPAN>) is used to perform the parts that are serial.</P>

<P class="p76 ft8">In the above example, the second <SPAN class="ft9">parallel </SPAN>region will generate and launch a small kernel for setting the ﬁrst and last elements. Small kernels generally do not run long enough to overcome the cost of a kernel launch on some oﬄoading devices, such as GPUs. It’s important that the data transfer saved by employing this technique is large enough to overcome the high cost of a kernel launch on some devices. Both the <SPAN class="ft9">parallel loop </SPAN>and the second <SPAN class="ft9">parallel </SPAN>region could be made asynchronous (discussed in a later chapter) to reduce the cost of the second kernel launch.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_37">


<DIV id="id_1">
<!-- <TABLE cellpadding=0 cellspacing=0 class="t4">
<TR>
	<TD class="tr0 td15"><P class="p6 ft9">CHAPTER 4. OPTIMIZE DATA LOCALITY</P></TD>
	<TD class="tr0 td16"><P class="p8 ft9">36</P></TD>
</TR>
</TABLE> -->
<P class="p301 ft53">Note: Because the kernels directive instructs the compiler to search for parallelism, there is no similar technique for kernels, but the parallel approach above can be easily placed between kernels regions.</P>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<P class="p2 ft11">Case Study - Optimize Data Locality</P>
<P class="p116 ft9">By the end of the last chapter we had moved the main computational loops of our example code and, in doing so, introduced a signiﬁcant amount of implicit data transfers. The performance proﬁle for our code shows that for each iteration the A and Anew arrays are being copied back and forth between the <SPAN class="ft15">host </SPAN>and <SPAN class="ft15">device</SPAN>, four times for the parallel loop version and twice for the kernels version. Given that the values for these arrays are not needed until after the answer has converged, let’s add a data region around the convergence loop. Additionally, we’ll need to specify how the arrays should be managed by this data region. Both the initial and ﬁnal values for the A array are required, so that array will require a copy data clause. The results in the Anew array, however, are only used within this section of code, so a create clause will be used for it. The resulting code is shown below.</P>
<P class="p302 ft53">Note: The changes required during this step are the same for both versions of the code, so only the parallel loop version will be shown.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_38">


<DIV>

<DIV id="id_2">

<P class="p314 ft23">With this change, only the value computed for the maximum error, which is required by the convergence loop, is copied from the device every iteration. The A and Anew arrays will remain local to the device through the extent of this calculation. Using the Nvidia Visual Proﬁler again, we see that each data transfers now only occur at the beginning and end of the data region and that the time between each iterations is much less.</P>
<P class="p315 ft23">Looking at the ﬁnal performance of this code, we see that the time for the OpenACC code on a GPU is now much faster than even the best threaded CPU code. Although only the parallel loop version is shown in the performance graph, the kernels version performs equally well once the data region has been added.</P>
<P class="p316 ft9">This ends the Jacobi Iteration case study. The simplicity of this implementation generally shows very good <NOBR>speed-ups</NOBR> with OpenACC, often leaving little potential for further improvement. The reader should feel encouraged, however, to revisit this code to see if further improvements are possible on the device of interest to them.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_39">


</DIV> 
<DIV id="page_40">


<DIV id="id_1">
<!-- <P class="p2 ft10">Chapter 5</P> -->
<P class="p9 ft3">Optimize Loops</P>
<P class="p81 ft8">Once data locality has been expressed developers may wish to further tune the code for the hardware of interest. It’s important to understand that the more loops are tuned for a particular type of hardware the less performance portable the code becomes to other architecures. If you’re generally running on one particular accelerator, however, there may be some gains to be had by tuning how the loops are mapped to the underlying hardware.</P>
<P class="p140 ft6">It’s tempting to begin tuning the loops before all of the data locality has been expressed in the code. Because PCIe copies are frequently the limiter to application performance on the current generation of accelerators the performance impact of tuning a particular loop may be too diﬃcult to measure until data locality has been optimized. For this reason the best practice is to wait to optimize particular loops until after all of the data locality has been expressed in the code, reducing the PCIe transfer time to a minimum.</P>
<P class="p26 ft38">Eﬃcient Loop Ordering</P>
<P class="p227 ft7">Before changing the way OpenACC maps loops onto the hardware of interest, the developer should examine the important loops to ensure that data arrays are being accessed in an eﬃcient manner. Most modern hardware, be it a CPU with large caches and SIMD operations or a GPU with coalesced memory accesses and SIMT operations, favor accessing arrays in a <SPAN class="ft15">stride 1 </SPAN>manner. That is to say that each loop iteration accesses consecutive memory addresses. This is achieved by ensuring that the innermost loop of a loop nest iterates on the fastest varying array dimension and each successive loop outward accesses the next fastest varying dimension. Arranging loops in this increasing manner will frequently improve cache eﬃciency and improve vectorization on most architectures.</P>
<P class="p11 ft38">OpenACC’s 3 Levels of Parallelism</P>
<P class="p263 ft14">OpenACC deﬁnes three levels of parallelism: <SPAN class="ft15">gang</SPAN>, <SPAN class="ft15">worker</SPAN>, and <SPAN class="ft15">vector</SPAN>. Additionally execution may be marked as being sequential (<SPAN class="ft15">seq</SPAN>). Vector parallelism has the ﬁnest granularity, with an individual instruction operating on multiple pieces of data (much like <SPAN class="ft15">SIMD </SPAN>parallelism on a modern CPU or <SPAN class="ft15">SIMT </SPAN>parallelism on a modern GPU). Vector operations are performed with a particular <SPAN class="ft15">vector length</SPAN>, indicating how many data elements may be operated on with the same instruction. Gang parallelism is <NOBR>coarse-grained</NOBR> parallelism, where gangs work independently of each other and may not synchronize. Worker parallelism sits between vector and gang levels. A gang consists of 1 or more workers, each of which operates on a vector of some length. Within a gang the OpenACC model exposes a <SPAN class="ft15">cache </SPAN>memory, which can be used by all workers and vectors within the gang, and it is legal to synchronize within a gang, although OpenACC does not expose synchronization to the user. Using these three levels of parallelism, plus sequential, a programmer can map the parallelism in the</P>
</DIV>

</DIV>
<DIV id="page_41">

<P class="p82 ft9">code to any device. OpenACC does not require the programmer to do this mapping explicitly, however. If the programmer chooses not to explicitly map loops to the device of interest the compiler will implicitly perform this mapping using what it knows about the target device. This makes OpenACC highly portable, since the same code may be mapped to any number of target devices. The more explicit mapping of parallelism the programmer adds to the code, however, the less portable they make the code to other architectures.</P>
<P class="p318 ft9">Figure 5.1: OpenACC’s Three Levels of Parallelism</P>
<P class="p319 ft11">Mapping Parallelism to the Hardware</P>
<P class="p229 ft23">With some understanding of how the underlying accelerator hardware works it’s possible to inform that compiler how it should map the loop iterations into parallelism on the hardware. It’s worth restating that the more detail the compiler is given about how to map the parallelism onto a particular accelerator the less performance portable the code will be.</P>
<P class="p13 ft23">As discussed earlier in this guide the loop directive is intended to give the compiler additional information about the next loop in the code. In addition to the clauses shown before, which were intended to ensure correctness, the clauses below inform the compiler which level of parallelism should be used to for the given loop.</P>
<P class="p320 ft9"><SPAN class="ft9">•</SPAN><SPAN class="ft44">Gang clause - partition the loop across gangs.</SPAN></P>
<P class="p321 ft9"><SPAN class="ft9">•</SPAN><SPAN class="ft44">Worker clause - partition the loop across workers.</SPAN></P>
<P class="p321 ft9"><SPAN class="ft9">•</SPAN><SPAN class="ft44">Vector clause - vectorize the loop.</SPAN></P>
</DIV>
<DIV id="page_42">


<DIV id="id_1">

<P class="p320 ft8"><SPAN class="ft9">•</SPAN><SPAN class="ft33">Seq clause - do not partition this loop, run it sequentially instead.</SPAN></P>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<P class="p322 ft8">These directives may also be combined on a particular loop. For example, a <SPAN class="ft9">gang vector </SPAN>loop would be partitioned across gangs, each of which with 1 worker implicitly, and then vectorized. The OpenACC speciﬁcation enforces that the outermost loop must be a gang loop, the innermost parallel loop must be a vector loop, and a worker loop may appear in between. A sequential loop may appear at any level.</P>

</DIV>
</DIV>

<DIV id="id_5">

<DIV id="id_5_2">
<P class="p271 ft14">Informing the compiler where to partition the loops is just one part of optimizing the loops. The programmer may additionally tell the compiler the speciﬁc number of gangs, workers, or the vector length to use for the loops. This speciﬁc mapping is achieved slightly diﬀerently when using the <SPAN class="ft9">kernels </SPAN>directive or the <SPAN class="ft9">parallel </SPAN>directive. In the case of the <SPAN class="ft9">kernels </SPAN>directive, the <SPAN class="ft9">gang</SPAN>, <SPAN class="ft9">worker</SPAN>, and <SPAN class="ft9">vector </SPAN>clauses accept an integer parameter that will optionally inform the compiler how to partition that level of parallelism. For example, <SPAN class="ft9">vector(128) </SPAN>informs the compiler to use a vector length of 128 for the loop.</P>

</DIV>
</DIV>

</DIV>
<DIV id="page_43">


<DIV id="id_2">

<DIV id="id_2_2">
<P class="p335 ft7">Since these mappings will vary between diﬀerent accelerator, the <SPAN class="ft9">loop </SPAN>directive accepts a <SPAN class="ft9">device_type </SPAN>clause, which will inform the compiler that these clauses only apply to a particular device time. Clauses after a <SPAN class="ft9">device_type </SPAN>clause up until either the next <SPAN class="ft9">device_type </SPAN>or the end of the directive will apply only to the speciﬁed device. Clauses that appear before all <SPAN class="ft9">device_type </SPAN>clauses are considered default values, which will be used if they are not overridden by a later clause. For example, the code below speciﬁes that a vector length of 128 should be used on devices of type <SPAN class="ft9">acc_device_nvidia </SPAN>or a vector length of 256 should be used on devices of type <SPAN class="ft9">acc_device_radeon</SPAN>. The compiler will choose a default vector length for all other device types.</P>

</DIV>
</DIV>
<DIV id="id_3">

<DIV id="id_3_2">
<P class="p2 ft11">Collapse Clause</P>
<P class="p12 ft8">When a code contains tightly nested loops it is frequently beneﬁcial to <SPAN class="ft15">collapse </SPAN>these loops into a single loop. Collapsing loops means that two loops of trip counts N and M respectively will be automatically turned into a single loop with a trip count of N times M. By collapsing two or more parallel loops into a single loop the compiler has an increased amount of parallelism to use when mapping the code to the device. On highly parallel architectures, such as GPUs, this can result in improved performance. Additionally, if a loop lacked suﬃcient parallelism for the hardware by itself, collapsing it with another loop multiplies the available parallelism. This is especially beneﬁcial on vector loops, since some hardware types will require longer vector lengths to achieve high performance than others. The code below demonstrates how to use the collapse directive.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_44">


<DIV id="id_1">

<DIV id="id_1_2">
<P class="p344 ft8">The above code is an excerpt from a real application where collapsing loops extended the parallelism available to be exploited. On line 1, the two outermost loops are collapsed together to make it possible to generate <SPAN class="ft15">gangs </SPAN>across the iterations of both loops, thus making the total number of gangs <SPAN class="ft9">nelemd </SPAN>x <SPAN class="ft9">qsize </SPAN>rather than just <SPAN class="ft9">nelemd</SPAN>. The collapse at line 4 collapses together 3 small loops to increase the possible <SPAN class="ft15">vector length</SPAN>, as none of the loops iterate for enough trips to create a reasonable vector length on the target accelerator. How much this optimization will <NOBR>speed-up</NOBR> the code will vary according to the application and the target accelerator, but it’s not uncommon to see large <NOBR>speed-ups</NOBR> by using collapse on loop nests.</P>
</DIV>
</DIV>
<DIV id="id_2">
<P class="p161 ft11">Routine Parallelism</P>
<P class="p12 ft8">A previous chapter introduced the <SPAN class="ft9">routine </SPAN>directive for calling functions and subroutines from OpenACC parallel regions. In that chapter it was assumed that the routine would be called from each loop iteration, therefore requiring a <SPAN class="ft9">routine seq </SPAN>directive. In some cases, the routine itself may contain parallelism that must be mapped to the device. In these cases, the <SPAN class="ft9">routine </SPAN>directive may have a <SPAN class="ft9">gang</SPAN>, <SPAN class="ft9">worker</SPAN>, or <SPAN class="ft9">vector </SPAN>clause instead of <SPAN class="ft9">seq </SPAN>to inform the compiler that the routine will contain the speciﬁed level of parallelism. When the compiler then encounters the call site of the aﬀected routine, it will then know how it can parallelize the code to use the routine.</P>
</DIV>
<DIV id="id_3">

<DIV id="id_3_2">
<P class="p345 ft11">Case Study - Optimize Loops</P>
<P class="p346 ft8">This case study will focus on a diﬀerent algorithm than the previous chapters. When a compiler has suﬃcient information about loops to make informed decisions, it’s frequently diﬃcult to improve the performance of a given parallel loop by more than a few percent. In some cases, the code lacks the information necessary for the compiler to make informed optimization decisions. In these cases, it’s often possible for a developer to optimize the parallel loops signiﬁcantly by informing the compiler how to decompose and distribute the loops to the hardware.</P>
<P class="p347 ft8">The code used in this section implements a sparse, <NOBR>matrix-vector</NOBR> product (SpMV) operation. This means that a matrix and a vector will be multiplied together, but the matrix has very few elements that are not zero (it is <SPAN class="ft15">sparse</SPAN>), meaning that calculating these values is unnecessary. The matrix is stored in a Compress Sparse Row (CSR) format. In CSR the sparse array, which may contain a signiﬁcant number of cells whose value is zero, thus wasting a signiﬁcant amount of memory, is stored using three, smaller arrays: one containing the <NOBR>non-zero</NOBR> values from the matrix, a second that describes where in a given row these <NOBR>non-zero</NOBR> elements would reside, and a third describing the columns in which the data would reside. The code for this exercise is below.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_45">



<DIV id="id_2">

<P class="p129 ft8">One important thing to note about this code is that the compiler is unable to determine how many <NOBR>non-zeros</NOBR> each row will contain and use that information in order to schedule the loops. The developer knows, however, that the number of <NOBR>non-zero</NOBR> elements per row is very small and this detail will be key to achieving high performance.</P>
<P class="p358 ft16">NOTE: Because this case study features optimization techniques, it is necessary to perform optimizations that may be beneﬁcial on one hardware, but not on others. This case study was performed using the PGI 2015 compiler on an NVIDIA Tesla K40 GPU. These same techniques may apply on other architectures, particularly those similar to NVIDIA GPUs, but it will be necessary to make certain optimization decisions based on the particular accelerator in use.</P>
<P class="p359 ft8">In examining the compiler feedback from the code shown above, I know that the compiler has chosen to use a vector length of 256 on the innermost loop. I could have also obtained this information from a runtime proﬁle of the application.</P>

<P class="p227 ft13">Based on my knowledge of the matrix, I know that this is signiﬁcantly larger than the typical number of <NOBR>non-zeros</NOBR> per row, so many of the <SPAN class="ft15">vector lanes </SPAN>on the accelerator will be wasted because there’s not suﬃcient work for them. The ﬁrst thing to try in order to improve performance is to adjust the vector length used on</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_46">



<DIV id="id_2">

<P class="p367 ft13">the innermost loop. I happen to know that the compiler I’m using will restrict me to using multiples of the <SPAN class="ft15">warp size </SPAN>(the minimum SIMT execution size on NVIDIA GPUs) of this processor, which is 32. This detail will vary according to the accelerator of choice. Below is the modiﬁed code using a vector length of 32.</P>

<P class="p376 ft8">Notice that I have now explicitly informed the compiler that the innermost loop should be a vector loop, to ensure that the compiler will map the parallelism exactly how I wish. I can try diﬀerent vector lengths to find the optimal value for my accelerator by modifying the <SPAN class="ft9">vector_length </SPAN>clause. Below is a graph showing the relative <NOBR>speed-up</NOBR> of varying the vector length compared to the <NOBR>compiler-selected</NOBR> value.</P>
<P class="p377 ft14">Notice that the best performance comes from the smallest vector length. Again, this is because the number of <NOBR>non-zeros</NOBR> per row is very small, so a small vector length results in fewer wasted compute resources. On the particular chip I’m using, the smallest possible vector length, 32, achieves the best possible performance. On this particular accelerator, I also know that the hardware will not perform eﬃciently at this vector length unless we can identify further parallelism another way. In this case, we can use the <SPAN class="ft15">worker </SPAN>level of parallelism to ﬁll each <SPAN class="ft15">gang </SPAN>with more of these short vectors. Below is the modiﬁed code.</P>

</DIV>
</DIV>
</DIV>


<DIV id="page_47">

<P class="p387 ft35">In this version of the code, I’ve explicitly mapped the outermost look to both gang and worker parallelism and will vary the number of workers using the num_workers clause. The results follow.</P>
<P class="p388 ft35">On this particular hardware, the best performance comes from a vector length of 32 and 32 workers. This turns out to be the maximum amount of parallelism that the particular accelerator being used supports</P>

</DIV>


<DIV id="page_48">
<P class="p390 ft8">within a gang. In this case, we observed a 1.3X <NOBR>speed-up</NOBR> from decreasing the vector length and another 2.1X <NOBR>speed-up</NOBR> from varying the number of workers within each gang, resulting in an overall 2.9X performance improvement from the untuned OpenACC code.</P>
<P class="p263 ft8"><SPAN class="ft16">Best Practice: </SPAN>Although not shown in order to save space, it’s generally best to use the <SPAN class="ft9">device_type </SPAN>clause whenever specifying the sorts of optimizations demonstrated in this section, because these clauses will likely diﬀer from accelerator to accelerator. By using the <SPAN class="ft9">device_type </SPAN>clause it’s possible to provide this information only on accelerators where the optimizations apply and allow the compiler to make its own decisions on other architectures. The OpenACC speciﬁcation speciﬁcally suggests <SPAN class="ft9">nvidia</SPAN>, <SPAN class="ft9">radeon</SPAN>, and <SPAN class="ft9">xeonphi </SPAN>as three common device type strings.</P>
</DIV>


<DIV id="page_49">

<P class="p9 ft3">OpenACC Interoperability</P>
<P class="p391 ft9">The authors of OpenACC recognized that it may sometimes be beneﬁcial to mix OpenACC code with code accelerated using other parallel programming languages, such as CUDA or OpenCL, or accelerated math libraries. This interoperability means that a developer can choose the programming paradigm that makes the most sense in the particular situation and leverage code and libraries that may already be available. Developers don’t need to decide at the begining of a project between OpenACC <SPAN class="ft15">or </SPAN>something else, they can choose to use OpenACC <SPAN class="ft15">and </SPAN>other technologies.</P>
<P class="p19 ft11">The Host Data Region</P>
<P class="p92 ft9">The ﬁrst method for interoperating between OpenACC and some other code is by managing all data using OpenACC, but calling into a function that requires device data. For the purpose of example the cublasSaxpy routine will be used in place of writing a <SPAN class="ft15">saxpy </SPAN>routine, as was shown in an earlier chapter. This routine is freely provided by Nvidia for their hardware in the CUBLAS library. Most other vendors provide their own, tuned library.</P>
<P class="p28 ft9">The host_data region gives the programmer a way to expose the device address of a given array to the host for passing into a function. This data must have already been moved to the device previously. The host_data region accepts only the use_device clause, which speciﬁes which device variables should be exposed to the host. In the example below, the arrays x and y are placed on the device via a data region and then initialized in an OpenACC loop. These arrays are then passed to the cublasSaxpy function as device pointers using the host_data region.</P>

</DIV>


<DIV id="page_50">

<P class="p326 ft9">The call to cublasSaxpy can be changed to any function that expects device memory as parameter.</P>

<P class="p2 ft11">Using Device Pointers</P>
<P class="p229 ft9">Because there is already a large ecosystem of accelerated applications using languages such as CUDA or OpenCL it may also be necessary to add an OpenACC region to an existing accelerated application. In this case the arrays may be managed outside of OpenACC and already exist on the device. In this case OpenACC provides the deviceptr data clause, which may be used where any data clause may appear. This clause informs the compiler that the variables speciﬁed are already device on the device and no other action needs to be taken on them. The example below uses the acc_malloc function, which allocates device memory and returns a pointer, to allocate an array only on the device and then uses that array within an OpenACC region.</P>

</DIV>


<DIV id="page_51">


<DIV id="id_1">
<DIV id="id_1_2">

<P class="p400 ft9">Notice that in the set and saxpy routines, where the OpenACC compute regions are found, each compute region is informed that the pointers being passed in are already device pointers by using the deviceptr keyword. This example also uses the acc_malloc, acc_free, and acc_memcpy_from_device routines for memory management. Although the above example uses acc_malloc and acc_memcpy_from_device, which are provided by the OpenACC speciﬁcation for portable memory management, a <NOBR>device-speciﬁc</NOBR> API may have also been used, such as cudaMalloc and cudaMemcpy.</P>
</DIV>
</DIV>
<DIV id="id_2">
<P class="p2 ft11">Obtaining Device and Host Pointer Addresses</P>
<P class="p229 ft23">OpenACC provides the acc_deviceptr and acc_hostptr function calls for obtaining the device and host addresses of pointers based on the host and device addresses, respectively. These routines require that the addresses actually have corresponding addresses, otherwise they will return NULL.</P>


<P class="p155 ft11">Additional Vendor-Specific Interoperability Features</P>
<P class="p401 ft9">The OpenACC speciﬁcation suggests several features that are speciﬁc to individual vendors. While imple- mentations are not required to provide the functionality, it’s useful to know that these features exist in some implementations. The purpose of these features are to provide interoperability with the native runtime of each platform. Developers should refer to the OpenACC speciﬁcation and their compiler’s documentation for a full list of supported features.</P>
<!-- <P class="p155 ft12">Asynchronous Queues and CUDA Streams (NVIDIA)</P> -->
<P class="p20 ft9">As demonstrated in the next chapter, asynchronous work queues are frequently an important way to deal with the cost of PCIe data transfers on devices with distinct host and device memory. In the NVIDIA CUDA programming model asynchronous operations are programmed using CUDA streams. Since developers may need to interoperate between CUDA streams and OpenACC queues, the speciﬁcation suggests two routines for mapping CUDA streams and OpenACC asynchronous queues.</P>
<P class="p138 ft35">The acc_get_cuda_stream function accepts an integer async id and returns a CUDA stream object (as a void*) for use as a CUDA stream.</P>
<P class="p402 ft35">The acc_set_cuda_stream function accepts an integer async handle and a CUDA stream object (as a void*) and maps the CUDA stream used by the async handle to the stream provided.</P>
<P class="p403 ft35">With these two functions it’s possible to place both OpenACC operations and CUDA operations into the same underlying CUDA stream so that they will execute in the appropriate order.</P>
<!-- <P class="p404 ft12">CUDA Managed Memory (NVIDIA)</P> -->
<P class="p139 ft9">NVIDIA added support for <SPAN class="ft15">CUDA Managed Memory</SPAN>, which provides a single pointer to memory regardless of whether it is accessed from the host or device, in CUDA 6.0. In many ways managed memory is similar to OpenACC memory management, in that only a single reference to the memory is necessary and the runtime will handle the complexities of data movement. The advantage that managed memory sometimes has it that it is better able to handle complex data structures, such as C++ classes or structures containing pointers, since pointer references are valid on both the host and the device. More information about CUDA Managed Memory can be obtained from NVIDIA. To use managed memory within an OpenACC program the developer can simply declare pointers to managed memory as device pointers using the deviceptr clause so that the OpenACC runtime will not attempt to create a separate device allocation for the pointers.</P>
</DIV>
</DIV>
<DIV id="page_52">


<DIV>

<DIV id="id_2">

<!-- <P class="p405 ft12">Using CUDA Device Kernels (NVIDIA)</P> -->
<P class="p141 ft9">The host_data directive is useful for passing device memory to <NOBR>host-callable</NOBR> CUDA kernels. In cases where it’s necessary to call a device kernel (CUDA __device__ function) from within an OpenACC parallel region it’s possible to use the acc routine directive to inform the compiler that the function being called is available on the device. The function declaration must be decorated with the acc routine directive and the level of parallelism at which the function may be called. In the example below the function f1dev is a sequential function that will be called from each CUDA thread, so it is declared acc routine seq.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_53">


<DIV id="id_1">
<!-- <P class="p2 ft10">Chapter 7</P> -->
<P class="p9 ft3">Advanced OpenACC Features</P>
<P class="p81 ft23">This chapter will discuss OpenACC features and techniques that do not ﬁt neatly into other sections of the guide. These techniques are considered advanced, so readers should feel comfortable with the features discussed in previous chapters before proceeding to this chapter.</P>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<P class="p161 ft11">Asynchronous Operation</P>
<P class="p411 ft9">In a previous chapter we discussed the necessity to optimize for data locality to reduce the cost of data transfers on systems where the host and accelerator have physically distinct memories. There will always be some amount of data transfers that simply cannot be optimized away and still produce correct results. After minimizing data transfers, it may be possible to further reduce the performance penalty associated with those transfers by overlapping the copies with other operations on the host, device, or both. This can be achieved with OpenACC using the async clause. The async clause can be added to parallel, kernels, and update directives to specify that once the associated operation has been sent to the accelerator or runtime for execution the CPU may continue doing other things, rather than waiting for the accelerator operation to complete. This may include enqueing additional accelerator operations or computing other work that is unrelated to the work being performed by the accelerator. The code below demonstrates adding the async clause to a parallel loop and an update directive that follows.</P>

<P class="p414 ft35">In the case above, the host thread will enqueue the parallel region into the <SPAN class="ft53">default asynchronous queue</SPAN>, then execution will return to the host thread so that it can also enqueue the update, and ﬁnally the CPU</P>
</DIV>
</DIV>

</DIV>
<DIV id="page_54">


<DIV id="id_1">

<DIV id="id_1_2">

<P class="p416 ft7">thread will continue execution. Eventually, however, the host thread will need the results computed on the accelerator and copied back to the host using the <SPAN class="ft9">update</SPAN>, so it must synchronize with the accelerator to ensure that these operations have ﬁnished before attempting to use the data. The <SPAN class="ft9">wait </SPAN>directive instructs the runtime to wait for past asynchronous operations to complete before proceeding. So, the above examples can be extended to include a synchronization before the data being copied by the <SPAN class="ft9">update </SPAN>directive proceeds.</P>

</DIV>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<P class="p271 ft7">While this is useful, it would be even more useful to expose dependencies into these asynchronous operations and the associated waits such that independent operations could potentially be executed concurrently. Both <SPAN class="ft9">async </SPAN>and <SPAN class="ft9">wait </SPAN>have an optional argument for a <NOBR>non-negative,</NOBR> integer number that speciﬁes a queue number for that operation. All operations placed in the same queue will operate <NOBR>in-order,</NOBR> but operations place in diﬀerent queues may operate in any order. These work queues are unique <NOBR>per-device,</NOBR> so two devices will have distinct queues with the same number. If a <SPAN class="ft9">wait </SPAN>is encountered without an argument, it will wait on all previously enqueued work on that device. The case study below will demonstrate how to use diﬀerent work queues to achieve overlapping of computation and data transfers.</P>
<P class="p143 ft13">In addition to being able to place operations in separate queues, it’d be useful to be able to join these queues together at a point where results from both are needed before proceeding. This can be achieved by adding an <SPAN class="ft9">async </SPAN>clause to an <SPAN class="ft9">wait</SPAN>. This may seem unintuitive, so the code below demonstrates how this is done.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_55">


<DIV id="id_2">
<P class="p331 ft8">The above code initializes the values contained in <SPAN class="ft9">a </SPAN>and <SPAN class="ft9">b </SPAN>using separate work queues so that they may potentially be done independently. The <SPAN class="ft9">wait(1) async(2) </SPAN>ensures that work queue 2 does not proceed until queue 1 has completed. The vector addition is then able to be enqueued to the device because the previous kernels will have completed prior to this point. Lastly the code waits for all previous operations to complete. Using this technique we’ve expressed the dependencies of our loops to maximize concurrency between regions but still give correct results.</P>
<P class="p425 ft6"><SPAN class="ft16">Best Practice: </SPAN>The cost of sending an operation to the accelerator for execution is frequently quite high on oﬄoading accelerators, such as GPUs connected over a PCIe bus to a host CPU. Once the loops and data transfers within a routine have been tested, it is frequently beneﬁcial to make each parallel region and update asynchrounous and then place a <SPAN class="ft9">wait </SPAN>directive after the last accelerator directive. This allows the runtime to enqueue all of the work immediately, which will reduce how often the accelerator and host must synchronize and reduce the cost of launching work onto the accelerator. It is criticial when implementing this optimization that the developer not leave oﬀ the <SPAN class="ft9">wait </SPAN>after the last accelerator directive, otherwise the code will be likely to produce incorrect results. This is such a beneﬁcial optimization that some compilers provide a <NOBR>build-time</NOBR> option to enable this for all accelerator directives automatically.</P>
</DIV>
<DIV id="id_3">

<DIV id="id_3_2">
<!-- <P class="p161 ft12">Case Study: Asynchronous Pipelining of a Mandelbrot Set</P> -->
<P class="p426 ft15"><SPAN class="ft14">For this example we will be modifying a simple application that generates a mandelbrot set, such as the picture shown above. Since each pixel of the image can be independently calculated, the code is trivial to parallelize, but because of the large size of the image itself, the data transfer to copy the results back to the host before writing to an image ﬁle is costly. Since this data transfer must occur, it’d be nice to overlap it with the computation, but as the code is written below, the entire computation must occur before the copy can occur, therefore there is noting to overlap. </SPAN>(Note: The mandelbrot function is a sequential function</P>
<P class="p261 ft53">used to calculate the value of each pixel. It is left out of this chapter to save space, but is included in the full examples.)</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_56">

<DIV class="dclr"></DIV>
<DIV>

<DIV id="id_2">

<P class="p58 ft9">Since each pixel is independent of each other, it’s possible to use a technique known as pipelining to break up the generation of the image into smaller parts, which allows the output from each part to be copied while the next part is being computed. The ﬁgure below demonstrates an idealized pipeline where the computation and copies are equally sized, but this rarely occurs in real applications. By breaking the operation into two parts, the same amount of data is transferred, but all but the ﬁrst and last transfers can be overlapped with computation. The number and size of these smaller chunks of work can be adjusted to find the value that provides the best performance.</P>
<P class="p204 ft9">The mandelbrot code can use this same technique by chunking up the image generation and data transfers into smaller, independent pieces. This will be done in multiple steps to reduce the likelihood of introducing an error. The ﬁrst step is to introduce a blocking loop to the calculation, but keep the data transfers the same. This will ensure that the work itself is properly divided to give correct results. After each step the developer should build and run the code to ensure the resulting image is still correct.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_57">

<DIV id="id_1">

<!-- <P class="p433 ft9">Figure 7.2: Idealized Pipeline Showing Overlapping of 2 Independent Operations</P> -->
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<!-- <P class="p2 ft4">Step 1: Blocking Computation</P> -->
<P class="p205 ft9">The ﬁrst step in pipelining the image generation is to introduce a loop that will break the computation up into chunks of work that can be generated independently. To do this, we will need decide how many blocks of work is desired and use that to determine the starting and ending bounds for each block. Next we introduce an additional loop around the existing two and modify the y loop to only operate within the current block of work by updating its loop bounds with what we’ve calculated as the starting and ending values for the current block. The modiﬁed loop nests are shown below.</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_58">


<DIV id="id_1">

<DIV id="id_1_2">


<P class="p238 ft35">At this point we have only conﬁrmed that we can successfully generate each block of work independently. The performance of this step should not be noticably better than the original code and may be worse.</P>
<!-- <P class="p52 ft4">Step 2: Blocking Data Transfers</P> -->
<P class="p20 ft9">The next step in the process is to break up the data transfers to and from the device in the same way the computation has already been broken up. To do this we will ﬁrst need to introduce a data region around the blocking loop. This will ensure that the device memory used to hold the image will remain on the device for all blocks of work. Since the initial value of the image array isn’t important, we use a create data clause to allocate an uninitialized array on the device. Next we use the update directive to copy each block of the image from the device to the host after it has been calculated. In order to do this, we need to determine the size of each block to ensure that we update only the part of the image that coincides with the current block of work. The resulting code at the end of this step is below.</P>
</DIV>
</DIV>
<DIV id="id_2">
<DIV id="id_2_2">

</DIV>
</DIV>
<DIV id="id_3">
<!-- <P class="p2 ft21"><SPAN class="ft17">16</SPAN><SPAN class="ft20">!$acc end data</SPAN></P> -->
</DIV>
</DIV>
<DIV id="page_59">


<DIV id="id_1">

<P class="p458 ft8">By the end of this step we are calculating and copying each block of the image independently, but this is still being done sequentially, each block after the previous. The performance at the end of this step is generally comparable to the original version.</P>
</DIV>
<DIV id="id_2">

<DIV id="id_2_2">
<!-- <P class="p2 ft4">Step 3: Overlapping Computation and Transfers</P> -->
<P class="p205 ft8">The last step of this case study is to make the device operations asynchronous so that the independent copies and computation can happen simultaneously. To do this we will use asynchronous work queues to ensure that the computation and data transfer within a single block are in the same queue, but separate blocks land in diﬀerent queues. The block number is a convenient asynchronous handle to use for this change. Of course, since we’re now operating completely asynchronously, it’s critical that we add a <SPAN class="ft9">wait </SPAN>directive after the block loop to ensure that all work completes before we attempt to use the image data from the host. The modiﬁed code is found below.</P>
<P class="p140 ft13">With this modiﬁcation it’s now possible for the computational part of one block to operate simultaneously as the data transfer of another. The developer should now experiment with varying block sizes to determine what the optimal value is on the architecture of interest. It’s important to note, however, that on some</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_60">


<DIV id="id_1">

<P class="p230 ft8">architectures the cost of creating an asynchronous queue the ﬁrst time its used can be quite expensive. In <NOBR>long-running</NOBR> applications, where the queues may be created once at the beginning of a <NOBR>many-hour</NOBR> run and reused throughout, this cost is amortized. In <NOBR>short-running</NOBR> codes, such as the demonstration code used in this chapter, this cost may outweigh the beneﬁt of the pipelining. Two solutions to this are to introduce a simple block loop at the beginning of the code that <NOBR>pre-creates</NOBR> the asynchronous queues before the timed section, or to use a modulus operation to reuse the same smaller number of queues among all of the blocks. For instance, by using the block number modulus 2 as the asynchronous handle, only two queues will be used and the cost of creating those queues will be amortized by their reuse. Two queues is generally suﬃcient to see a gain in performance, since it still allows computation and updates to overlap, but the developer should experiment to find the best value on a given machine.</P>
<P class="p228 ft8">Below we see a screenshot showing before and after proﬁles from applying these changes to the code on an NVIDIA GPU platform. Similar results should be possible on any acclerated platform. Using 64 blocks and two asynchronous queues, as shown below, roughly a 2X performance improvement was observed on the test machine over the performance without pipelining.</P>

<P class="p466 ft11">Multi-device Programming</P>
<P class="p229 ft8">For systems containing more than accelerator, OpenACC provides and API to make operations happen on a particular device. In case a system contains accelerators of diﬀerent types, the speciﬁcation also allows for querying and selecting devices of a speciﬁc architecture.</P>
<!-- <P class="p26 ft12">acc_get_num_devices()</P> -->
</DIV>
<DIV id="id_2">
<P class="p2 ft14">The <SPAN class="ft9">acc_get_num_devices() </SPAN>routine may be used to query how many devices of a given architecture are available on the system. It accepts one parameter of type <SPAN class="ft9">acc_device_t </SPAN>and returns a integer number of</P>
</DIV>
</DIV>
<DIV id="page_61">


<DIV id="id_1">

<P class="p467 ft8">devices.</P>
</DIV>
<DIV id="id_2">
<DIV id="id_2_1">

</DIV>
<DIV id="id_2_2">
<!-- <P class="p2 ft12">acc_get_device_num() and acc_set_device_num()</P> -->
<P class="p141 ft8">The <SPAN class="ft9">acc_get_device_num() </SPAN>routines query the current device that will be used of a given type and returns the integer identiﬁer of that device. The <SPAN class="ft9">acc_set_device_num() </SPAN>accepts two parameters, the desired device number and device type. Once a device number has been set, all operations will be sent to the speciﬁed device until a diﬀerent device is speciﬁed by a later call to <SPAN class="ft9">acc_set_device_num()</SPAN>.</P>
<!-- <P class="p14 ft12">acc_get_device_type() and acc_set_device_type()</P> -->
<P class="p468 ft13">The <SPAN class="ft9">acc_get_device_type() </SPAN>routine takes no parameters and returns the device type of the current default device. The <SPAN class="ft9">acc_set_device_type() </SPAN>speciﬁes to the runtime the type of device that the runtime should use for accelerator operations, but allows the runtime to choose which device of that type to use.</P>
<!-- <P class="p415 ft12"><NOBR>Multi-device</NOBR> Programming Example</P> -->
<P class="p400 ft7">As a example of <NOBR>multi-device</NOBR> programming, it’s possible to further extend the mandelbrot example used previously to send diﬀerent blocks of work to diﬀerent accelerators. In order to make this work, it’s necessary to ensure that device copies of the data are created on each device. We will do this by replacing the structured <SPAN class="ft9">data </SPAN>region in the code with an unstructured <SPAN class="ft9">enter data </SPAN>directive for eac device, using the <SPAN class="ft9">acc_set_device_num() </SPAN>function to specify the device for each <SPAN class="ft9">enter data</SPAN>. For simplicity, we will allocate the full image array on each device, although only a part of the array is actually needed. When the memory requirements of the application is large, it will be necessary to allocate just the pertinent parts of the data on each accelerator.</P>
<P class="p142 ft8">Once the data has been created on each device, a call to <SPAN class="ft9">acc_get_device_type() </SPAN>in the blocking loop, using a simple modulus operation to select which device should receive each block, will sent blocks to diﬀerent devices.</P>
<P class="p77 ft13">Lastly it’s necessary to introduce a loop over devices to wait on each device to complete. Since the <SPAN class="ft9">wait </SPAN>directive is <NOBR>per-device,</NOBR> the loop will once again use <SPAN class="ft9">acc_get_device_type() </SPAN>to select a device to wait on, and then use an <SPAN class="ft9">exit data </SPAN>directive to deallocate the device memory. The ﬁnal code is below.
</P>

</DIV>
</DIV>
</DIV>
<DIV id="page_62">

<DIV>

<DIV id="id_2">

<P class="p390 ft9">Although this example <NOBR>over-allocates</NOBR> device memory by placing the entire image array on the device, it does serve as a simple example of how the acc_set_device_num() routine can be used to operate on a machine with multiple devices. In production codes the developer will likely want to partition the work such that only the parts of the array needed by a speciﬁc device are available there. Additionally, by using CPU threads it may be possible to issue work to the devices more quickly and improve overall performance. Figure 7.3 shows a screenshot of the NVIDIA Visual Proﬁler showing the mandelbrot computation divided across two NVIDIA GPUs.</P>
</DIV>
</DIV>
</DIV>
<DIV id="page_63">

<DIV id="id_1">

</DIV>

</DIV>
<DIV id="page_64">


<DIV id="id_1">
<P class="p2 ft10">Appendix A</P>
<P class="p9 ft3">References</P>
<!-- <P class="p477 ft66"><SPAN class="ft9">•</SPAN><A href="http://openacc.org"><SPAN class="ft65">OpenACC.org</SPAN></A></P>
<P class="p321 ft66"><SPAN class="ft9">•</SPAN><A href="http://devblogs.nvidia.com/parallelforall/tag/openacc/"><SPAN class="ft65">OpenACC on the NVIDIA Parallel Forall Blog</SPAN></A></P>
<P class="p478 ft66"><SPAN class="ft9">•</SPAN><A href="https://www.pgroup.com/resources/articles.htm"><SPAN class="ft65">PGI Insider Newsletter</SPAN></A></P>
<P class="p479 ft66"><SPAN class="ft9">•</SPAN><A href="http://bit.ly/gtc-openacc"><SPAN class="ft65">OpenACC at the NVIDIA GPU Technology Conference</SPAN></A></P>
<P class="p480 ft66"><SPAN class="ft9">•</SPAN><A href="http://stackoverflow.com/questions/tagged/openacc"><SPAN class="ft65">OpenACC on Stack Exchange</SPAN></A></P>
</DIV> -->
<DIV id="id_2">
<P class="p2 ft9">63</P>
</DIV>
</DIV>
</BODY>
</HTML>
