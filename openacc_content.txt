titles length:  39
[{'body': u'The kernels construct identifies a region of code that may contain parallelism, but relies on the automatic parallelization capabilities of the compiler to analyze the region, identify which loops are safe to parallelize, and then accelerate those loops. Developers will little or no parallel programming experience, or those working on functions containing many loop nests that might be parallelized will find the kernels directive a good starting place for OpenACC acceleration. The code below demonstrates the use of kernels in both C/C++ and Fortran. 15 10 11 12 13 1 2 3 4 5 6 7 8 9 10 In this example the code is initializing two arrays and then performing a simple calculation on them. Notice that we have identified a block of code, using curly braces in C and starting and ending directives in Fortran, that contains two candidate loops for acceleration. The compiler will analyze these loops for data independence and parallelize both loops by generating an accelerator kernel for each. The compiler is given complete freedom to determine how best to map the parallelism available in these loops to the hardware, meaning that we will be able to use this same code regardless of the accelerator we are building for. The compiler will use its own knowledge of the target accelerator to choose the best path for acceleration. One caution about the kernels directive, however, is that if the compiler cannot be certain that a loop is data independent, it will not parallelize the loop. Common reasons for why a compiler may misidentify a loop as non-parallel will be discussed in a later section.',
  'href': '#page_16',
  'state': 2,
  'title': u'The Kernels Construct'},
 {'body': u"The parallel construct identifies a region of code that will be parallelized across OpenACC gangs. By itself a parallel region is of limited use, but when paired with the loop directive (discussed in more detail later) the compiler will generate a parallel version of the loop for the accelerator. These two directives can, and most often are, combined into a single parallel loop directive. By placing this directive on a loop the programmer asserts that the affected loop is safe to parallelize and allows the compiler to select how to schedule the loop iterations on the target accelerator. The code below demonstrates the use of the parallel loop combined directive in both C/C++ and Fortran. Notice that, unlike the kernels directive, each loop needs to be explicitly decorated with parallel loop directives. This is because the parallel construct relies on the programmer to identify the parallelism in the code rather than performing its own compiler analysis of the loops. In this case, the programmer is only identifying the availability of parallelism, but still leaving the decision of how to map that parallelism to the accelerator to the compiler's knowledge about the device. This is a key feature that differentiates OpenACC from other, similar programming models. The programmer identifies the parallelism without dictating to the compiler how to exploit that parallelism. This means that OpenACC code will be portable to devices other than the device on which the code is being developed, because details about how to parallelize the code are left to compiler knowledge rather than being hard-coded into the source.",
  'href': '#page_17',
  'state': 2,
  'title': u'The Parallel Construct'},
 {'body': u"One of the biggest points of confusion for new OpenACC programmers is why the specification has both the parallel and kernels directives, which appear to do the same thing. While they are very closely related there are subtle differences between them. The kernels construct gives the compiler maximum leeway to parallelize and optimize the code how it sees fit for the target accelerator, but also relies most heavily on the compiler's ability to automatically parallelize the code. As a result, the programmer may see differences in what different compilers are able to parallelize and how they do so. The parallel loop directive is an assertion by the programmer that it is both safe and desirable to parallelize the affected loop. This relies on the programmer to have correctly identified parallelism in the code and remove anything in the code that may be unsafe to parallelize. If the programmer asserts incorrectly that the loop may be parallelized then the resulting application may produce incorrect results. To put things another way: the kernels construct may be thought of as a hint to the compiler of where it should look for parallelism while the parallel directive is an assertion to the compiler of where there is parallelism. An important thing to note about the kernels construct is that the compiler will analyze the code and only parallelize when it is certain that it is safe to do so. In some cases the compiler may not have enough information at compile time to determine whether a loop is safe the parallelize, in which case it will not parallelize the loop, even if the programmer can clearly see that the loop is safely parallel. For example, in the case of C/C++ code, where arrays are passed into functions as pointers, the compiler may not always be able to determine that two arrays do not share the same memory, otherwise known as pointer aliasing. If the compiler cannot know that two pointers are not aliased it will not be able to parallelize a loop that accesses those arrays. Best Practice: C programmers should use the restrict keyword (or the __restrict decorator in C++) whenever possible to inform the compiler that the pointers are not aliased, which will frequently give the compiler enough information to then parallelize loops that it would not have otherwise. In addition to the restrict keyword, declaring constant variables using the const keyword may allow the compiler to use a read-only memory for that variable if such a memory exists on the accelerator. Use of const and restrict is a good programming practice in general, as it gives the compiler additional information that can be used when optimizing the code. Fortran programmers should also note that an OpenACC compiler will parallelize Fortran array syntax that is contained in a kernels construct. When using parallel instead, it will be necessary to explicitly introduce loops over the elements of the arrays. One more notable benefit that the kernels construct provides is that if data is moved to the device for use in loops contained in the region, that data will remain on the device for the full extent of the region, or until it is needed again on the host within that region. This means that if multiple loops access the same data it will only be copied to the accelerator once. When parallel loop is used on two subsequent loops that access the same data a compiler may or may not copy the data back and forth between the host and the device between the two loops. In the examples shown in the previous section the compiler generates implicit data movement for both parallel loops, but only generates data movement once for the kernels approach, which may result in less data motion by default. This difference will be revisited in the case study later in this chapter. For more information on the differences between the kernels and parallel directives, please see [http://www.pgroup.com/lit/articles/insider/v4n2a1.htm]. At this point many programmers will be left wondering which directive they should use in their code. More experienced parallel programmers, who may have already identified parallel loops within their code, will likely find the parallel loop approach more desirable. Programmers with less parallel programming experience or whose code contains a large number of loops that need to be analyzed may find the kernels approach much simpler, as it puts more of the burden on the compiler. Both approaches have advantages, so new OpenACC programmers should determine for themselves which approach is a better fit for them. A programmer may even choose to use kernels in one part of the code, but parallel in another if it makes sense to do so. Note: For the remainder of the document the phrase parallel region will be used to describe either a parallel or kernels region. When refering to the parallel construct, a terminal font will be used, as shown in this sentence.",
  'href': '#page_18',
  'state': 2,
  'title': u'Di\ufb00erences Between Parallel and Kernels'},
 {'body': u'The loop construct gives the compiler additional information about the very next loop in the source code. The loop directive was shown above in connection with the parallel directive, although it is also valid with kernels. Loop clauses come in two forms: clauses for correctness and clauses for optimization. This chapter will only discuss the two correctness clauses and a later chapter will discuss optimization clauses. private The private clause specifies that each loop iteration requires its own copy of the listed variables. For example, if each loop contains a small, temporary array named tmp that it uses during its calculation, then this variable must be made private to each loop iteration in order to ensure correct results. If tmp is not declared private, then threads executing different loop iterations may access this shared tmp variable in unpredictable ways, resulting in a race condition and potentially incorrect results. Below is the synax for the private clause. private(variable) There are a few special cases that must be understood about scalar variables within loops. First, loop iterators will be privatized by default, so they do not need to be listed as private. Second, unless otherwise specified, any scalar accessed within a parallel loop will be made first private by default, meaning a private copy will be made of the variable for each loop iteration and it will be initialized with the value of that scalar upon entering the region. Finally, any variables (scalar or not) that are declared within a loop in C or C++ will be made private to the iterations of that loop by default. Note: The parallel construct also has a private clause which will privatize the listed variables for each gang in the parallel region. reduction The reduction clause works similarly to the private clause in that a private copy of the affected variable is generated for each loop iteration, but reduction goes a step further to reduce all of those private copies into one final result, which is returned from the region. For example, the maximum of all private copies of the variable may be required or perhaps the sum. A reduction may only be specified on a scalar variable and only common, specified operations can be performed, such as +, *, min, max, and various bitwise operations (see the OpenACC specification for a complete list). The format of the reduction clause is as follows, where operator should be replaced with the operation of interest and variable should be replaced with the variable being reduced: reduction(operator:variable) An example of using the reduction clause will come in the case study below.',
  'href': '#page_19',
  'state': 2,
  'title': u'The Loop Construct'},
 {'body': u"Function or subroutine calls within parallel loops can be problematic for compilers, since it is not always possible for the compiler to see all of the loops at one time. OpenACC 1.0 compilers were forced to either inline all routines called within parallel regions or not parallelize loops containing routine calls at all. OpenACC 2.0 introduced the routine directive to address this shortcoming. The routine directive gives the compiler the necessary information about the function or subroutine and the loops it contains in order to parallelize the calling parallel region. The routine directive must be added to a function definition informing the compiler of the level of parallelism used within the routine. OpenACC's levels of parallelism will be discussed in a later section. C++ Class Functions When operating on C++ classes, it is frequently necessary to call class functions from within parallel regions. The example below shows a C++ class float3 that contains 3 \ufb02oating point values and has a set function that is used to set the values of its x, y, and z members to that of another instance of float3. In order for this to work from within a parallel region, the set function is declared as an OpenACC routine using the acc routine directive. Since we know that it will be called by each iteration of a parallel loop, it is declared a seq (or sequential) routine.",
  'href': '#page_20',
  'state': 2,
  'title': u'Routine Directive'},
 {'body': u"In the last chapter we identified the two loop nests within the convergence loop as the most time consuming parts of our application. Additionally we looked at the loops and were able to determine that the outer, convergence loop is not parallel, but the two loops nested within are safe to parallelize. In this chapter we will accelerate those loop nests with OpenACC using the directives discussed earlier in this chapter. To further emphasize the similarities and differences between parallel and kernels directives, we will accelerate the loops using both and discuss the differences. We previously identified the available parallelism in our code, now we will use the parallel loop directive to accelerate the loops that we identified. Since we know that the two, doubly-nested sets of loops are parallel, simply add a parallel loop directive above each of them. This will inform the compiler that the outer of the two loops is safely parallel. Some compilers will additionally analyze the inner loop and determine that it is also parallel, but to be certain we will also add a loop directive around the inner loops. There is one more subtlety to accelerating the loops in this example: we are attempting to calculate the maximum value for the variable error. As discussed above, this is considered a reduction since we are reducing from all possible values for error down to just the single maximum. This means that it is necessary to indicate a reduction on the first loop nest (the one that calculates error). Best Practice: Some compilers will detect the reduction on error and implicitly insert the reduction clause, but for maximum portability the programmer should always indicate reductions in the code. At this point the code looks like the examples below. Best Practice: Most OpenACC compilers will accept only the parallel loop directive on the j loops and detect for themselves that the i loop can also be parallelized without needing the loop directives on the i loops. By placing a loop directive on each loop that I know can be parallelized the programmer ensures that the compiler will understand that the loop is safe the parallelize. When used within a parallel region, the loop directive asserts that the loop iterations are independent of each other and are safe the parallelize and should be used to provide the compiler as much information about the loops as possible. Building the above code using the PGI compiler (version 15.5) produces the following compiler feedback (shown for C, but the Fortran output is similar). Analyzing the compiler feedback gives the programmer the ability to ensure that the compiler is producing the expected results and fix any problems if it is not. In the output above we see that accelerator kernels were generated for the two loops that were identified (at lines 58 and 71, in the compiled source file) and that the compiler automatically generated data movement, which will be discussed in more detail in the next chapter. Other clauses to the loop directive that may further benefit the performance of the resulting code will be discussed in a later chapter. Kernels Using the kernels construct to accelerate the loops we've identified requires inserting just one directive in the code and allowing the compiler to perform the parallel analysis. Adding a kernels construct around the two computational loop nests results in the following code. The above code demonstrates some of the power that the kernels construct provides, since the compiler will analyze the code and identify both loop nests as parallel and it will automatically discover the reduction on the error variable without programmer intervention. An OpenACC compiler will likely discover not only that the outer loops are parallel, but also the inner loops, resulting in more available parallelism with fewer directives than the parallel loop approach. Had the programmer put the kernels construct around the convergence loop, which we have already determined is not parallel, the compiler likely would not have found any available parallelism. Even with the kernels directive it is necessary for the programmer to do some amount of analysis to determine where parallelism may be found. Taking a look at the compiler output points to some more subtle differences between the two approaches. The first thing to notice from the above output is that the compiler correctly identified all four loops as being parallelizable and generated kernels from those loops. Also notice that the compiler only generated implicit data movement directives at line 54 (the beginning of the kernels region), rather than at the beginning of each parallel loop. This means that the resulting code should perform fewer copies between host and device memory in this version than the version from the previous section. A more subtle difference between the output is that the compiler chose a different loop decomposition scheme (as is evident by the implicit acc loop directives in the compiler output) than the parallel loop because kernels allowed it to do so. More details on how to interpret this decomposition feedback and how to change the behavior will be discussed in a later chapter. At this point we have expressed all of the parallelism in the example code and the compiler has parallelized it for an accelerator device. Analyzing the performance of this code may yield surprising results on some accelerators, however. The results below demonstrate the performance of this code on 1 - 8 CPU threads on a modern CPU at the ime of publication and an NVIDIA Tesla K40 GPU using both implementations above. The y axis for figure 3.1 is execution time in seconds, so smaller is better. For the two OpenACC versions, the bar is divided by time transferring data between the host and device, time executing on the device, and other time. Notice that the performance of this code improves as CPU threads are added to the calcuation, but the OpenACC versions perform poorly compared to the CPU baseline. The OpenACC kernels version performs slightly better than the serial version, but the parallel loop case performs dramaticaly worse than even the slowest CPU version. Further performance analysis is necessary to identify the source of this slowdown. This analysis has already been applied to the graph above, which breaks down time spent computing the solution, copying data to and from the accelerator, and miscelaneous time, which includes various overheads involved in scheduling data transfers and computation. A variety of tools are available for performing this analysis, but since this case study was compiled for an NVIDIA GPU, the NVIDIA Visual profiler will be used to understand the application peformance. The screenshot in figure 3.2 shows NVIDIA Visual Profiler for 2 iterations of the convergence loop in the parallel loop version of the code. Since the test machine has two distinct memory spaces, one for the CPU and one for the GPU, it is necessary to copy data between the two memories. In this screenshot, the tool represents data transfers using the tan colored boxes in the two MemCpy rows and the computation time in the green and purple boxes in the rows below Compute. It should be obvious from the timeline displayed that significantly more time is being spent copying data to and from the accelerator before and after each compute kernel than actually computing on the device. In fact, the majority of the time is spent either in memory copies or in overhead incurred by the runtime scheduling memory copeis. In the next chapter we will fix this inefficiency, but first, why does the kernels version outperform the parallel loop version? When an OpenACC compiler parallelizes a region of code it must analyze the data that is needed within that region and copy it to and from the accelerator if necessary. This analysis is done at a per-region level and will typically default to copying arrays used on the accelerator both to and from the device at the beginning and end of the region respectively. Since the parallel loop version has two compute regions, as opposed to only one in the kernels version, data is copied back and forth between the two regions. As a result, the copy and overhead times are roughly twice that of the kernels region, although the compute kernel times are roughly the same.",
  'href': '#page_21',
  'state': 2,
  'title': u'Case Study - Parallelize'},
 {'body': u'', 'href': '#page_27', 'state': 2, 'title': u'Atomic Operations'},
 {'body': u"Because many accelerated architectures, such as CPU + GPU architectures, use distinct memory spaces for the host and device it is necessary for the compiler to manage data in both memories and move the data between the two memories to ensure correct results. Compilers rarely have full knowledge of the application, so they must be cautious in order to ensure correctness, which often involves copying data to and from the accelerator more often than is actually necessary. The programmer can give the compiler additional information about how to manage the memory so that it remains local to the accelerator as long as possible and is only moved between the two memories when absolutely necessary. Programmers will often realize the largest performance gains after optimizing data movement during this step. Optimize Loops Compilers will make decisions about how to map the parallelism in the code to the target accelerator based on internal heuristics and the limited knowledge it has about the application. Sometimes additional performance can be gained by providing the compiler with more information so that it can make better decisions on how to map the parallelism to the accelerator. When coming from a traditional CPU architecture to a more parallel architecture, such as a GPU, it may also be necessary to restructure loops to expose additional parallelism for the accelerator or to reduce the frequency of data movement. Frequently code refactoring that was motivated by improving performance on parallel accelerators is beneficial to traditional CPUs as well. This process is by no means the only way to accelerate using OpenACC, but it has been proven successful in numerous applications. Doing the same steps in different orders may cause both frustration and difficulty debugging, so it is advisable to perform each step of the process in the order shown above. Heterogenous Computing Best Practices Many applications have been written with little or even no parallelism exposed in the code. The applications that do expose parallelism frequently do so in a coarse-grained manner, where a small number of threads or processes execute for a long time and compute a significant amount work each. Modern GPUs and many-core processors, however, are designed to execute fine-grained threads, which are short-lived and execute a minimal amount of work each. These parallel architectures achieve high throughput by trading single-threaded performance in favor of several orders in magnitude more parallelism. This means that when accelerating an application with OpenACC, which was designed in light of increased hardware parallelism, it may be necessary to refactor the code to favor tightly-nested loops with a significant amount of data reuse. In many cases these same code changes also benefit more traditional CPU architectures as well by improving cache use and vectorization. OpenACC may be used to accelerate applications on devices that have a discrete memory or that have a memory space that's shared with the host. Even on devices that utilize a shared memory there is frequently still a hierarchy of a fast, close memory for the accelerator and a larger, slower memory used by the host. For this reason it is important to structure the application code to maximize reuse of arrays regardless of whether the underlying architecture uses discrete or unified memories. When refactoring the code for use with OpenACC it is frequently beneficial to assume a discrete memory, even if the device you are developing on has a unified memory. This forces data locality to be a primary consideration in the refactoring and will ensure that the resulting code exploits hierarchical memories and is portable to a wide range of devices. Case Study - Jacobi Iteration Throughout this guide we will use simple applications to demonstrate each step of the acceleration process. The first such application will solve the 2D-Laplace equation with the iterative Jacobi solver. Iterative methods are a common technique to approximate the solution of elliptic PDEs, like the 2D-Laplace equation, within some allowable tolerance. In the case of our example we will perform a simple stencil calculation where each point calculates it value as the mean of its neighbors' values. The calculation will continue to iterate until either the maximum change in value between two iterations drops below some tolerance level or a maximum number of iterations is reached. For the sake of consistent comparison through the document the examples will always iterate 1000 times. The main iteration loop for both C/C++ and Fortran appears below. 71 72 end do The outermost loop in each example will be referred to as the convergence loop, since it loops until the answer has converged by reaching some maximum error tolerance or number of iterations. Notice that whether or not a loop iteration occurs depends on the error value of the previous iteration. Also, the values for each element of A is calculated based on the values of the previous iteration, known as a data dependency. These two facts mean that this loop cannot be run in parallel. The first loop nest within the convergence loop calculates the new value for each element based on the current values of its neighbors. Notice that it is necessary to store this new value into a different array. If each iteration stored the new value back into itself then a data dependency would exist between the data elements, as the order each element is calculated would affect the final answer. By storing into a temporary array we ensure that all values are calculated using the current state of A before A is updated. As a result, each loop iteration is completely independent of each other iteration. These loop iterations may safely be run in any order or in parallel and the final result would be the same. This loop also calculates a maximum error value. The error value is the difference between the new value and the old. If the maximum amount of change between two iterations is within some tolerance, the problem is considered converged and the outer loop will exit. The second loop nest simply updates the value of A with the values calculated into Anew. If this is the last iteration of the convergence loop, A will be the final, converged value. If the problem has not yet converged, then A will serve as the input for the next iteration. As with the above loop nest, each iteration of this loop nest is independent of each other and is safe to parallelize. In the coming sections we will accelerate this simple application using the method described in this document. Chapter 2 Assess Application Performance A variety of tools can be used to evaluate application performance and which are available will depend on your development environment. From simple application timers to graphical performance analyzers, the choice of performance analysis tool is outside of the scope of this document. The purpose of this section is to provide guidance on choosing important sections of code for acceleration, which is independent of the profiling tools available. Because this document is focused on OpenACC, the PGProf tool, which is provided with the PGI OpenACC compiler will be used for CPU profiling. When accelerator profiling is needed, the application will be run on an Nvidia GPU and the Nvidia Visual Profiler will be used. Baseline Profiling Before parallelizing an application with OpenACC the programmer must first understand where time is currently being spent in the code. Routines and loops that take up a significant percentage of the runtime are frequently referred to as hot spots and will be the starting point for accelerating the application. A variety of tools exist for generating application profiles, such as gprof, pgprof, Vampir, and TAU. Selecting the specific tool that works best for a given application is outside of the scope of this document, but regardless of which tool or tools are used below are some important pieces of information that will help guide the next steps in parallelizing the application. Application performance - How much time does the application take to run? How efficiently does the program use the computing resources? Program hotspots - In which routines is the program spending most of its time? What is being done within these important routines? Focusing on the most time consuming parts of the application will yield the greatest results. Performance limiters - Within the identified hotspots, what's currently limiting the application perfor- mance? Some common limiters may be I/O, memory bandwidth, cache reuse, \ufb02oating point performance, communication, etc. One way to evaluate the performance limiters of a given loop nest is to evaluate its computational intensity, which is a measure of how many operations are performed on a data element per load or store from memory. Available parallelism - Examine the loops within the hotspots to understand how much work each loop nest performs. Do the loops iterate 10's, 100's, 1000's of times (or more)? Do the loop iterations operate independently of each other? Look not only at the individual loops, but look a nest of loops to understand the bigger picture of the entire nest. Gathering baseline data like the above both helps inform the developer where to focus efforts for the best results and provides a basis for comparing performance throughout the rest of the process. It's important to 11 choose input that will realistically re\ufb02ect how the application will be used once it has been accelerated. It's tempting to use a known benchmark problem for profiling, but frequently these benchmark problems use a reduced problem size or reduced I/O, which may lead to incorrect assumptions about program performance. Many developers also use the baseline profile to gather the expected output of the application to use for verifying the correctness of the application as it is accelerated. Additional Profiling Through the process of porting and optimizing an application with OpenACC it is necessary to gather additional profile data to guide the next steps in the process. Some profiling tools, such as pgprof and Vampir, support profiling on CPUs and GPUs, while other tools, such as gprof and NVIDIA Visual Profiler, may only support profiling on a particular platform. Additionally, some compilers build their own profiling into the application, such is the case with the PGI compiler, which supports setting the PGI_ACC_TIME environment variable for gathering runtime information about the application. When developing on offloading platforms, such as CPU + GPU platforms, it is generally important to use a profiling tool throughout the development process that can evaluate both time spent in computation and time spent performing PCIe data transfers. This document will use NVIDIA Visual Profiler for performing this analysis, although it is only available on NVIDIA platforms. Case Study - Analysis To get a better understanding of the case study program we will use the PGProf utility that comes as a part of the PGI Workstation package. First, it is necessary to build the executable to embed the compiler feedback into the executable using the common compiler feedback framework (CCFF) feature of the PGI compiler. This feature is enabled with the -Mprof=ccff compiler \ufb02ag and embeds additional information into the executable that can then be used by the PGProf utility to display additional information about how the compiler optimized the code. The executable is built with the following command: target process has terminated, writing profile data Once the data has been collected, it can be visualized using the pgprof command, which will open a PGProf window. $ pgprof -exe ./a.out When PGPROG opens we see that the vast majority of the time is spent in two routines: main and __c_mcopy8. A screenshot of the initial screen for PGProf is show in figure 2.1. Since the code for this case study is completely within the main function of the program, it is not surprising that nearly all of the time is spent in main, but in larger applications it is likely that the time will be spent in several other routines. Clicking into the main function we can see that nearly all of the runtime within main comes from the loop that calculates the next value for A. This is shown in figure 2.2. What is not obvious from the profiler output, however, is that the time spent in the memory copy routine shown in the initial screen is actually the second loop nest, which performs the array swap at the end of each iteration. The compiler output shows above (and is reiterated in PGProf) that the loop at line 68 was replaced by a memory copy, because doing so is more efficient than copying each element individually. So what the profiler is really showing us is that the major hotspots for our application are the loop nest that calculate Anew from A and the loop nest that copies from Anew to A for the next iteration, so we'll concentrate our efforts on these two loop nests. In the chapters that follow, we will optimize the loops identified in this chapter as the hotspots within our example application. Chapter 3 Parallelize Loops Now that the important hotspots in the application have been identified, the programmer should incrementally accelerate these hotspots by adding OpenACC directives to the important loops within those routines. There is no reason to think about the movement of data at this point in the process, the OpenACC compiler will analyze the data needed in the identified region and automatically ensure that the data is available on the accelerator. By focusing solely on the parallelism during this step, the programmer can move as much computation to the device as possible and ensure that the program is still giving correct results before optimizing away data motion in the next step. During this step in the process it is common for the overall runtime of the application to increase, even if the execution of the individual loops is faster using the accelerator. This is because the compiler must take a cautious approach to data movement, frequently copying more data to and from the accelerator than is actually necessary. Even if overall execution time increases during this step, the developer should focus on expressing a significant amount of parallelism in the code before moving on to the next step and realizing a benefit from the directives. OpenACC provides two different approaches for exposing parallelism in the code: parallel and kernels regions. Each of these directives will be detailed in the sections that follow. The Kernels Construct The kernels construct identifies a region of code that may contain parallelism, but relies on the automatic parallelization capabilities of the compiler to analyze the region, identify which loops are safe to parallelize, and then accelerate those loops. Developers will little or no parallel programming experience, or those working on functions containing many loop nests that might be parallelized will find the kernels directive a good starting place for OpenACC acceleration. The code below demonstrates the use of kernels in both C/C++ and Fortran. 15 10 11 12 13 1 2 3 4 5 6 7 8 9 10 In this example the code is initializing two arrays and then performing a simple calculation on them. Notice that we have identified a block of code, using curly braces in C and starting and ending directives in Fortran, that contains two candidate loops for acceleration. The compiler will analyze these loops for data independence and parallelize both loops by generating an accelerator kernel for each. The compiler is given complete freedom to determine how best to map the parallelism available in these loops to the hardware, meaning that we will be able to use this same code regardless of the accelerator we are building for. The compiler will use its own knowledge of the target accelerator to choose the best path for acceleration. One caution about the kernels directive, however, is that if the compiler cannot be certain that a loop is data independent, it will not parallelize the loop. Common reasons for why a compiler may misidentify a loop as non-parallel will be discussed in a later section. The Parallel Construct The parallel construct identifies a region of code that will be parallelized across OpenACC gangs. By itself a parallel region is of limited use, but when paired with the loop directive (discussed in more detail later) the compiler will generate a parallel version of the loop for the accelerator. These two directives can, and most often are, combined into a single parallel loop directive. By placing this directive on a loop the programmer asserts that the affected loop is safe to parallelize and allows the compiler to select how to schedule the loop iterations on the target accelerator. The code below demonstrates the use of the parallel loop combined directive in both C/C++ and Fortran. Notice that, unlike the kernels directive, each loop needs to be explicitly decorated with parallel loop directives. This is because the parallel construct relies on the programmer to identify the parallelism in the code rather than performing its own compiler analysis of the loops. In this case, the programmer is only identifying the availability of parallelism, but still leaving the decision of how to map that parallelism to the accelerator to the compiler's knowledge about the device. This is a key feature that differentiates OpenACC from other, similar programming models. The programmer identifies the parallelism without dictating to the compiler how to exploit that parallelism. This means that OpenACC code will be portable to devices other than the device on which the code is being developed, because details about how to parallelize the code are left to compiler knowledge rather than being hard-coded into the source. Differences Between Parallel and Kernels One of the biggest points of confusion for new OpenACC programmers is why the specification has both the parallel and kernels directives, which appear to do the same thing. While they are very closely related there are subtle differences between them. The kernels construct gives the compiler maximum leeway to parallelize and optimize the code how it sees fit for the target accelerator, but also relies most heavily on the compiler's ability to automatically parallelize the code. As a result, the programmer may see differences in what different compilers are able to parallelize and how they do so. The parallel loop directive is an assertion by the programmer that it is both safe and desirable to parallelize the affected loop. This relies on the programmer to have correctly identified parallelism in the code and remove anything in the code that may be unsafe to parallelize. If the programmer asserts incorrectly that the loop may be parallelized then the resulting application may produce incorrect results. To put things another way: the kernels construct may be thought of as a hint to the compiler of where it should look for parallelism while the parallel directive is an assertion to the compiler of where there is parallelism. An important thing to note about the kernels construct is that the compiler will analyze the code and only parallelize when it is certain that it is safe to do so. In some cases the compiler may not have enough information at compile time to determine whether a loop is safe the parallelize, in which case it will not parallelize the loop, even if the programmer can clearly see that the loop is safely parallel. For example, in the case of C/C++ code, where arrays are passed into functions as pointers, the compiler may not always be able to determine that two arrays do not share the same memory, otherwise known as pointer aliasing. If the compiler cannot know that two pointers are not aliased it will not be able to parallelize a loop that accesses those arrays. Best Practice: C programmers should use the restrict keyword (or the __restrict decorator in C++) whenever possible to inform the compiler that the pointers are not aliased, which will frequently give the compiler enough information to then parallelize loops that it would not have otherwise. In addition to the restrict keyword, declaring constant variables using the const keyword may allow the compiler to use a read-only memory for that variable if such a memory exists on the accelerator. Use of const and restrict is a good programming practice in general, as it gives the compiler additional information that can be used when optimizing the code. Fortran programmers should also note that an OpenACC compiler will parallelize Fortran array syntax that is contained in a kernels construct. When using parallel instead, it will be necessary to explicitly introduce loops over the elements of the arrays. One more notable benefit that the kernels construct provides is that if data is moved to the device for use in loops contained in the region, that data will remain on the device for the full extent of the region, or until it is needed again on the host within that region. This means that if multiple loops access the same data it will only be copied to the accelerator once. When parallel loop is used on two subsequent loops that access the same data a compiler may or may not copy the data back and forth between the host and the device between the two loops. In the examples shown in the previous section the compiler generates implicit data movement for both parallel loops, but only generates data movement once for the kernels approach, which may result in less data motion by default. This difference will be revisited in the case study later in this chapter. For more information on the differences between the kernels and parallel directives, please see [http://www.pgroup.com/lit/articles/insider/v4n2a1.htm]. At this point many programmers will be left wondering which directive they should use in their code. More experienced parallel programmers, who may have already identified parallel loops within their code, will likely find the parallel loop approach more desirable. Programmers with less parallel programming experience or whose code contains a large number of loops that need to be analyzed may find the kernels approach much simpler, as it puts more of the burden on the compiler. Both approaches have advantages, so new OpenACC programmers should determine for themselves which approach is a better fit for them. A programmer may even choose to use kernels in one part of the code, but parallel in another if it makes sense to do so. Note: For the remainder of the document the phrase parallel region will be used to describe either a parallel or kernels region. When refering to the parallel construct, a terminal font will be used, as shown in this sentence. The Loop Construct The loop construct gives the compiler additional information about the very next loop in the source code. The loop directive was shown above in connection with the parallel directive, although it is also valid with kernels. Loop clauses come in two forms: clauses for correctness and clauses for optimization. This chapter will only discuss the two correctness clauses and a later chapter will discuss optimization clauses. private The private clause specifies that each loop iteration requires its own copy of the listed variables. For example, if each loop contains a small, temporary array named tmp that it uses during its calculation, then this variable must be made private to each loop iteration in order to ensure correct results. If tmp is not declared private, then threads executing different loop iterations may access this shared tmp variable in unpredictable ways, resulting in a race condition and potentially incorrect results. Below is the synax for the private clause. private(variable) There are a few special cases that must be understood about scalar variables within loops. First, loop iterators will be privatized by default, so they do not need to be listed as private. Second, unless otherwise specified, any scalar accessed within a parallel loop will be made first private by default, meaning a private copy will be made of the variable for each loop iteration and it will be initialized with the value of that scalar upon entering the region. Finally, any variables (scalar or not) that are declared within a loop in C or C++ will be made private to the iterations of that loop by default. Note: The parallel construct also has a private clause which will privatize the listed variables for each gang in the parallel region. reduction The reduction clause works similarly to the private clause in that a private copy of the affected variable is generated for each loop iteration, but reduction goes a step further to reduce all of those private copies into one final result, which is returned from the region. For example, the maximum of all private copies of the variable may be required or perhaps the sum. A reduction may only be specified on a scalar variable and only common, specified operations can be performed, such as +, *, min, max, and various bitwise operations (see the OpenACC specification for a complete list). The format of the reduction clause is as follows, where operator should be replaced with the operation of interest and variable should be replaced with the variable being reduced: reduction(operator:variable) An example of using the reduction clause will come in the case study below. Routine Directive Function or subroutine calls within parallel loops can be problematic for compilers, since it is not always possible for the compiler to see all of the loops at one time. OpenACC 1.0 compilers were forced to either inline all routines called within parallel regions or not parallelize loops containing routine calls at all. OpenACC 2.0 introduced the routine directive to address this shortcoming. The routine directive gives the compiler the necessary information about the function or subroutine and the loops it contains in order to parallelize the calling parallel region. The routine directive must be added to a function definition informing the compiler of the level of parallelism used within the routine. OpenACC's levels of parallelism will be discussed in a later section. C++ Class Functions When operating on C++ classes, it is frequently necessary to call class functions from within parallel regions. The example below shows a C++ class float3 that contains 3 \ufb02oating point values and has a set function that is used to set the values of its x, y, and z members to that of another instance of float3. In order for this to work from within a parallel region, the set function is declared as an OpenACC routine using the acc routine directive. Since we know that it will be called by each iteration of a parallel loop, it is declared a seq (or sequential) routine. Case Study - Parallelize In the last chapter we identified the two loop nests within the convergence loop as the most time consuming parts of our application. Additionally we looked at the loops and were able to determine that the outer, convergence loop is not parallel, but the two loops nested within are safe to parallelize. In this chapter we will accelerate those loop nests with OpenACC using the directives discussed earlier in this chapter. To further emphasize the similarities and differences between parallel and kernels directives, we will accelerate the loops using both and discuss the differences. We previously identified the available parallelism in our code, now we will use the parallel loop directive to accelerate the loops that we identified. Since we know that the two, doubly-nested sets of loops are parallel, simply add a parallel loop directive above each of them. This will inform the compiler that the outer of the two loops is safely parallel. Some compilers will additionally analyze the inner loop and determine that it is also parallel, but to be certain we will also add a loop directive around the inner loops. There is one more subtlety to accelerating the loops in this example: we are attempting to calculate the maximum value for the variable error. As discussed above, this is considered a reduction since we are reducing from all possible values for error down to just the single maximum. This means that it is necessary to indicate a reduction on the first loop nest (the one that calculates error). Best Practice: Some compilers will detect the reduction on error and implicitly insert the reduction clause, but for maximum portability the programmer should always indicate reductions in the code. At this point the code looks like the examples below. Best Practice: Most OpenACC compilers will accept only the parallel loop directive on the j loops and detect for themselves that the i loop can also be parallelized without needing the loop directives on the i loops. By placing a loop directive on each loop that I know can be parallelized the programmer ensures that the compiler will understand that the loop is safe the parallelize. When used within a parallel region, the loop directive asserts that the loop iterations are independent of each other and are safe the parallelize and should be used to provide the compiler as much information about the loops as possible. Building the above code using the PGI compiler (version 15.5) produces the following compiler feedback (shown for C, but the Fortran output is similar). Analyzing the compiler feedback gives the programmer the ability to ensure that the compiler is producing the expected results and fix any problems if it is not. In the output above we see that accelerator kernels were generated for the two loops that were identified (at lines 58 and 71, in the compiled source file) and that the compiler automatically generated data movement, which will be discussed in more detail in the next chapter. Other clauses to the loop directive that may further benefit the performance of the resulting code will be discussed in a later chapter. Kernels Using the kernels construct to accelerate the loops we've identified requires inserting just one directive in the code and allowing the compiler to perform the parallel analysis. Adding a kernels construct around the two computational loop nests results in the following code. The above code demonstrates some of the power that the kernels construct provides, since the compiler will analyze the code and identify both loop nests as parallel and it will automatically discover the reduction on the error variable without programmer intervention. An OpenACC compiler will likely discover not only that the outer loops are parallel, but also the inner loops, resulting in more available parallelism with fewer directives than the parallel loop approach. Had the programmer put the kernels construct around the convergence loop, which we have already determined is not parallel, the compiler likely would not have found any available parallelism. Even with the kernels directive it is necessary for the programmer to do some amount of analysis to determine where parallelism may be found. Taking a look at the compiler output points to some more subtle differences between the two approaches. The first thing to notice from the above output is that the compiler correctly identified all four loops as being parallelizable and generated kernels from those loops. Also notice that the compiler only generated implicit data movement directives at line 54 (the beginning of the kernels region), rather than at the beginning of each parallel loop. This means that the resulting code should perform fewer copies between host and device memory in this version than the version from the previous section. A more subtle difference between the output is that the compiler chose a different loop decomposition scheme (as is evident by the implicit acc loop directives in the compiler output) than the parallel loop because kernels allowed it to do so. More details on how to interpret this decomposition feedback and how to change the behavior will be discussed in a later chapter. At this point we have expressed all of the parallelism in the example code and the compiler has parallelized it for an accelerator device. Analyzing the performance of this code may yield surprising results on some accelerators, however. The results below demonstrate the performance of this code on 1 - 8 CPU threads on a modern CPU at the ime of publication and an NVIDIA Tesla K40 GPU using both implementations above. The y axis for figure 3.1 is execution time in seconds, so smaller is better. For the two OpenACC versions, the bar is divided by time transferring data between the host and device, time executing on the device, and other time. Notice that the performance of this code improves as CPU threads are added to the calcuation, but the OpenACC versions perform poorly compared to the CPU baseline. The OpenACC kernels version performs slightly better than the serial version, but the parallel loop case performs dramaticaly worse than even the slowest CPU version. Further performance analysis is necessary to identify the source of this slowdown. This analysis has already been applied to the graph above, which breaks down time spent computing the solution, copying data to and from the accelerator, and miscelaneous time, which includes various overheads involved in scheduling data transfers and computation. A variety of tools are available for performing this analysis, but since this case study was compiled for an NVIDIA GPU, the NVIDIA Visual profiler will be used to understand the application peformance. The screenshot in figure 3.2 shows NVIDIA Visual Profiler for 2 iterations of the convergence loop in the parallel loop version of the code. Since the test machine has two distinct memory spaces, one for the CPU and one for the GPU, it is necessary to copy data between the two memories. In this screenshot, the tool represents data transfers using the tan colored boxes in the two MemCpy rows and the computation time in the green and purple boxes in the rows below Compute. It should be obvious from the timeline displayed that significantly more time is being spent copying data to and from the accelerator before and after each compute kernel than actually computing on the device. In fact, the majority of the time is spent either in memory copies or in overhead incurred by the runtime scheduling memory copeis. In the next chapter we will fix this inefficiency, but first, why does the kernels version outperform the parallel loop version? When an OpenACC compiler parallelizes a region of code it must analyze the data that is needed within that region and copy it to and from the accelerator if necessary. This analysis is done at a per-region level and will typically default to copying arrays used on the accelerator both to and from the device at the beginning and end of the region respectively. Since the parallel loop version has two compute regions, as opposed to only one in the kernels version, data is copied back and forth between the two regions. As a result, the copy and overhead times are roughly twice that of the kernels region, although the compute kernel times are roughly the same. Atomic Operations When one or more loop iterations need to access an element in memory at the same time data races can occur. For instance, if one loop iteration is modifying the value contained in a variable and another is trying to read from the same variable in parallel, different results may occur depending on which iteration occurs first. In serial programs, the sequential loops ensure that the variable will be modified and read in a predictable order, but parallel programs don't make guarantees that a particular loop iteration will happen before anoter. In simple cases, such as finding a sum, maximum, or minimum value, a reduction operation will ensure correctness. For more complex operations, the atomic directive will ensure that no two threads can attempt to perfom the contained operation simultaneously. Use of atomics is sometimes a necessary part of parallelization to ensure correctness. The atomic directive accepts one of four clauses to declare the type of operation contained within the region. The read operation ensures that no two loop iterations will read from the region at the same time. The write operation will ensure that no two iterations with write to the region at the same time. An update operation is a combined read and write. Finally a capture operation performs an update, but saves the value calculated in that region to use in the code that follows. If no clause is given then an update operation will occur. Atomic Example A histogram is a common technique for counting up how many times values occur from an input set according to their value. Figure _ shows a histogram that counts the number of times numbers fall within particular ranges. The example code below loops through a series of integer numbers of a known range and counts the occurances of each number in that range. Since each number in the range can occur multiple times, we need to ensure that each element in the histogram array is updated atomically. The code below demonstrates using the atomic directive to generate a histogram. Notice that updates to the histogram array h are performed atomically. Because we are incrementing the value of the array element, an update operation is used to read the value, modify it, and then write it back. Chapter 4 Optimize Data Locality At the end of the previous chapter we saw that although we've moved the most compute intensive parts of the application to the accelerator, sometimes the process of copying data from the host to the accelerator and back will be more costly than the computation itself. This is because it is difficult for a compler to determine when (or if) the data will be needed in the future, so it must be cautious and ensure that the data will be copied in case it is needed. To improve upon this, we'll exploit the data locality of the application. Data locality means that data used in device or host memory should remain local to that memory for as long as it is needed. This idea is sometimes referred to as optimizing data reuse or optimizing away unnecessary data copies between the host and device memories. However you think of it, providing the compiler with the information necessary to only relocate data when it needs to do so is frequently the key to success with OpenACC. After expressing the parallelism of a program's important regions it is frequently necessary to provide the compiler with additional information about the locality of the data used by the parallel regions. As noted in the previous section, a compiler will take a cautious approach to data movement, always copying data that may be required, so that the program will still produce correct results. A programmer will have knowledge of what data is really needed and when it will be needed. The programmer will also have knowledge of how data may be shared between two functions, something that is difficult for a compiler to determine. Even when does not immediately know how best to optimize data motion, profiling tools may help the programmer identify when excess data movement occurs, as will be shown in the case study at the end of this chapter. The next step in the acceleration process is to provide the compiler with additional information about data locality to maximize reuse of data on the device and minimize data transfers. It is after this step that most applications will observe the benefit of OpenACC acceleration. This step will be primarily beneficial on machine where the host and device have separate memories.",
  'href': '#page_29',
  'state': 1,
  'title': u'4 Optimize Data Locality'},
 {'body': u'The data construct facilitates the sharing of data between multiple parallel regions. A data region may be added around one or more parallel regions in the same function or may be placed at a higher level in the program call tree to enable data to be shared between regions in multiple functions. The data construct is a structured construct, meaning that it must begin and end in the same scope (such as the same function or subroutine). A later section will discuss how to handle cases where a structured construct is not useful. A data region may be added to the earlier parallel loop example to enable data to be shared between both loop nests as follows. 28 The data region in the above examples enables the x and y arrays to be reused between the two parallel regions. This will remove any data copies that happen between the two regions, but it still does not guarantee optimal data movement. In order to provide the information necessary to perform optimal data movement, the programmer can add data clauses to the data region. Note: An implicit data region is created by each parallel and kernels region.',
  'href': '#page_29',
  'state': 2,
  'title': u'Data Regions'},
 {'body': u'Data clauses give the programmer additional control over how and when data is created on and copied to or from the device. These clauses may be added to any data, parallel, or kernels construct to inform the compiler of the data needs of that region of code. The data directives, along with a brief description of their meanings, follow. copy - Create space for the listed variables on the device, initialize the variable by copying data to the device at the beginning of the region, copy the results back to the host at the end of the region, and finally release the space on the device when done. copyin - Create space for the listed variables on the device, initialize the variable by copying data to the device at the beginning of the region, and release the space on the device when done without copying the data back the the host. copyout - Create space for the listed variables on the device but do not initialize them. At the end of the region, copy the results back to the host and release the space on the device. create - Create space for the listed variables and release it at the end of the region, but do not copy to or from the device. present - The listed variables are already present on the device, so no further action needs to be taken. This is most frequently used when a data region exists in a higher-level routine. deviceptr - The listed variables use device memory that has been managed outside of OpenACC, therefore the variables should be used on the device without any address translation. This clause is generally used when OpenACC is mixed with another programming model, as will be discussed in the interoperability chapter. In addition to these data clauses, OpenACC 1.0 and 2.0 provide present_or_* clauses (present_or_copy, for instance) that inform the compiler to check whether the variable is already present on the device; if it is present, use that existing copy of the data, if it is not, perform the action listed. These routines are frequently abbreviated, like pcopyin instead of present_or_copyin. In an upcoming OpenACC specification the behavior of all data directives will be present or, so programmers should begin writing their applications using these directives to ensure correctness with future OpenACC specifications. This change will simplify data reuse for the programmer. Shaping Arrays Sometimes a compiler will need some extra help determining the size and shape of arrays used in parallel or data regions. For the most part, Fortran programmers can rely on the self-describing nature of Fortran arrays, but C/C++ programmers will frequently need to give additional information to the compiler so that it will know how large an array to allocate on the device and how much data needs to be copied. To give this information the programmer adds a shape specification to the data clauses. In C/C++ the shape of an array is described as x[start:count] where start is the first element to be copied and count is the number of elements to copy. If the first element is 0, then it may be left off. In Fortran the shape of an array is described as x(start:end) where start is the first element to be copied and end is the last element to be copied. If start is the beginning of the array or end is the end of the array, they may be left off. Array shaping is frequently necessary in C/C++ codes when the OpenACC appears inside of function calls or the arrays are dynamically allocated, since the shape of the array will not be known at compile time. Shaping is also useful when only a part of the array needs to be stored on the device. As an example of array shaping, the code below modifies the previous example by adding shape information to each of the arrays. With these data clauses it is possible to further improve the example shown above by informing the compiler how and when it should perform data transfers. In this simple example above, the programmer knows that both x and y will be populated with data on the device, so neither will need to be copied to the device, but the results of y are significant, so it will need to be copied back to the host at the end of the calculation. The code below demonstrates using the pcreate and pcopyout directives to describe exactly this data locality to the compiler. !$acc parallel loop do i=1,N y(i) = 2.0 * x(i) + y(i) enddo !$acc end data',
  'href': '#page_30',
  'state': 2,
  'title': u'Data Clauses'}]
